2024-10-17 01:35:28,924 - log.py[38] - DEBUG: entry file content: ---------------------------------
2024-10-17 01:35:28,924 - log.py[39] - DEBUG: 
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

from legodnn.utils.dl.common.env import set_random_seed
set_random_seed(0)

import sys
sys.setrecursionlimit(100000)
import torch

from legodnn import BlockExtractor, BlockTrainer, ServerBlockProfiler, EdgeBlockProfiler, OptimalRuntime
from legodnn.gen_series_legodnn_models import gen_series_legodnn_models
from legodnn.block_detection.model_topology_extraction import topology_extraction
from legodnn.presets.auto_block_manager import AutoBlockManager
from legodnn.presets.common_detection_manager_1204_new import CommonDetectionManager
from legodnn.model_manager.common_model_manager import CommonModelManager
from legodnn.utils.common.file import experiments_model_file_path
from legodnn.utils.dl.common.model import get_module, set_module, get_model_size

from cv_task.datasets.image_classification.cifar_dataloader import CIFAR10Dataloader, CIFAR100Dataloader
from cv_task.image_classification.cifar.models import resnet18
from cv_task.image_classification.cifar.legodnn_configs import get_cifar100_train_config_200e
from cv_task.image_classification.cifar.models import mobilenetv2_w2

if __name__ == '__main__':
    cv_task = 'image_classification'
    dataset_name = 'cifar100'
    model_name = 'mobilenetv2_w2'
    method = 'legodnn'
    device = 'cuda'
    compress_layer_max_ratio = 0.125
    model_input_size = (1, 3, 32, 32)
    
    block_sparsity = [0.0, 0.2, 0.4, 0.6, 0.8]
    root_path = os.path.join('results/legodnn', cv_task, model_name+'_'+dataset_name + '_' + str(compress_layer_max_ratio).replace('.', '-'))

    compressed_blocks_dir_path = root_path + '/compressed'
    trained_blocks_dir_path = root_path + '/trained'
    descendant_models_dir_path = root_path + '/descendant'
    block_training_max_epoch = 65
    test_sample_num = 100
    
    checkpoint = '/data/gxy/legodnn-auto-on-cv-models/cv_task_model/image_classification/cifar100/mobilenetv2_w2/2024-10-15/22-20-26/mobilenetv2_w2.pth'
    teacher_model = mobilenetv2_w2(num_classes=100).to(device)
    teacher_model.load_state_dict(torch.load(checkpoint)['net'])

    print('\033[1;36m-------------------------------->    BUILD LEGODNN GRAPH\033[0m')
    model_graph = topology_extraction(teacher_model, model_input_size, device=device, mode='unpack')
    model_graph.print_ordered_node()
    
    print('\033[1;36m-------------------------------->    START BLOCK DETECTION\033[0m')
    detection_manager = CommonDetectionManager(model_graph, max_ratio=compress_layer_max_ratio)
    detection_manager.detection_all_blocks()
    detection_manager.print_all_blocks()

    model_manager = CommonModelManager()
    block_manager = AutoBlockManager(block_sparsity, detection_manager, model_manager)
    
    print('\033[1;36m-------------------------------->    START BLOCK EXTRACTION\033[0m')
    block_extractor = BlockExtractor(teacher_model, block_manager, compressed_blocks_dir_path, model_input_size, device)
    block_extractor.extract_all_blocks()

    print('\033[1;36m-------------------------------->    START BLOCK TRAIN\033[0m')
    train_loader, test_loader = CIFAR100Dataloader()
    block_trainer = BlockTrainer(teacher_model, block_manager, model_manager, compressed_blocks_dir_path,
                                 trained_blocks_dir_path, block_training_max_epoch, train_loader, device=device)
    block_trainer.train_all_blocks()

    server_block_profiler = ServerBlockProfiler(teacher_model, block_manager, model_manager,
                                                trained_blocks_dir_path, test_loader, model_input_size, device)
    server_block_profiler.profile_all_blocks()

    edge_block_profiler = EdgeBlockProfiler(block_manager, model_manager, trained_blocks_dir_path, 
                                            test_sample_num, model_input_size, device)
    edge_block_profiler.profile_all_blocks()

    optimal_runtime = OptimalRuntime(trained_blocks_dir_path, model_input_size,
                                     block_manager, model_manager, device)
    model_size_min = get_model_size(torch.load(os.path.join(compressed_blocks_dir_path, 'model_frame.pt')))/1024**2
    model_size_max = get_model_size(teacher_model)/1024**2 + 1
    gen_series_legodnn_models(deadline=100, model_size_search_range=[model_size_min, model_size_max], target_model_num=100, optimal_runtime=optimal_runtime, descendant_models_save_path=descendant_models_dir_path, device=device)
2024-10-17 01:35:28,924 - log.py[40] - DEBUG: entry file content: ---------------------------------
2024-10-17 01:35:36,438 - block_extractor.py[28] - INFO: save pruned block block-0 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-0-0.pt
2024-10-17 01:35:36,438 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (0): ModuleDict(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6()
    )
    (1): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:37,034 - block_extractor.py[28] - INFO: save pruned block block-0 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-0-2.pt
2024-10-17 01:35:37,034 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (0): ModuleDict(
      (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6()
    )
    (1): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(48, 26, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(26, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:37,653 - block_extractor.py[28] - INFO: save pruned block block-0 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-0-4.pt
2024-10-17 01:35:37,653 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (0): ModuleDict(
      (0): Conv2d(3, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6()
    )
    (1): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(36, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(20, 100, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)
        (4): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(100, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:38,168 - block_extractor.py[28] - INFO: save pruned block block-0 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-0-6.pt
2024-10-17 01:35:38,168 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (0): ModuleDict(
      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6()
    )
    (1): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(24, 13, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(13, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=58, bias=False)
        (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(58, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:38,688 - block_extractor.py[28] - INFO: save pruned block block-0 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-0-8.pt
2024-10-17 01:35:38,688 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (0): ModuleDict(
      (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6()
    )
    (1): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)
        (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(10, 7, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(7, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=27, bias=False)
        (4): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(27, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:38,752 - block_extractor.py[28] - INFO: save pruned block block-1 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-1-0.pt
2024-10-17 01:35:38,752 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (2): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
        (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:40,060 - block_extractor.py[28] - INFO: save pruned block block-1 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-1-2.pt
2024-10-17 01:35:40,060 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (2): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 218, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(218, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(218, 218, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=218, bias=False)
        (4): BatchNorm2d(218, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(218, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 205, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(205, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(205, 205, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=205, bias=False)
        (4): BatchNorm2d(205, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(205, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:40,678 - block_extractor.py[28] - INFO: save pruned block block-1 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-1-4.pt
2024-10-17 01:35:40,678 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (2): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 156, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(156, 156, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=156, bias=False)
        (4): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 125, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(125, 125, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=125, bias=False)
        (4): BatchNorm2d(125, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(125, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:41,277 - block_extractor.py[28] - INFO: save pruned block block-1 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-1-6.pt
2024-10-17 01:35:41,277 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (2): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 98, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(98, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=98, bias=False)
        (4): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(98, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 61, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(61, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=61, bias=False)
        (4): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(61, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:41,843 - block_extractor.py[28] - INFO: save pruned block block-1 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-1-8.pt
2024-10-17 01:35:41,843 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (2): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=44, bias=False)
        (4): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(44, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(48, 25, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(25, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=25, bias=False)
        (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(25, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:41,974 - block_extractor.py[28] - INFO: save pruned block block-2 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-2-0.pt
2024-10-17 01:35:41,974 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (4): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:42,670 - block_extractor.py[28] - INFO: save pruned block block-2 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-2-2.pt
2024-10-17 01:35:42,670 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (4): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 289, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289, bias=False)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(289, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:43,330 - block_extractor.py[28] - INFO: save pruned block block-2 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-2-4.pt
2024-10-17 01:35:43,330 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (4): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 202, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(202, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(202, 202, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=202, bias=False)
        (4): BatchNorm2d(202, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(202, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 197, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(197, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(197, 197, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=197, bias=False)
        (4): BatchNorm2d(197, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(197, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:43,968 - block_extractor.py[28] - INFO: save pruned block block-2 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-2-6.pt
2024-10-17 01:35:43,969 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (4): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 133, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(133, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=133, bias=False)
        (4): BatchNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(133, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 122, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122, bias=False)
        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(122, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:44,571 - block_extractor.py[28] - INFO: save pruned block block-2 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-2-8.pt
2024-10-17 01:35:44,571 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (4): ModuleDict(
      (conv): ModuleDict(
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 54, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(54, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=54, bias=False)
        (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(54, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:44,639 - block_extractor.py[28] - INFO: save pruned block block-3 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-3-0.pt
2024-10-17 01:35:44,639 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (7): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:45,416 - block_extractor.py[28] - INFO: save pruned block block-3 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-3-2.pt
2024-10-17 01:35:45,416 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (7): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 278, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(278, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(278, 278, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=278, bias=False)
        (4): BatchNorm2d(278, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(278, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 572, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(572, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(572, 572, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=572, bias=False)
        (4): BatchNorm2d(572, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(572, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:46,196 - block_extractor.py[28] - INFO: save pruned block block-3 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-3-4.pt
2024-10-17 01:35:46,196 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (7): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 185, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(185, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(185, 185, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=185, bias=False)
        (4): BatchNorm2d(185, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(185, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 396, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(396, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(396, 396, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=396, bias=False)
        (4): BatchNorm2d(396, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(396, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:46,858 - block_extractor.py[28] - INFO: save pruned block block-3 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-3-6.pt
2024-10-17 01:35:46,858 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (7): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 103, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(103, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(103, 103, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=103, bias=False)
        (4): BatchNorm2d(103, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(103, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 242, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(242, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(242, 242, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=242, bias=False)
        (4): BatchNorm2d(242, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(242, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:47,520 - block_extractor.py[28] - INFO: save pruned block block-3 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-3-8.pt
2024-10-17 01:35:47,521 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (7): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(64, 49, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(49, 49, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=49, bias=False)
        (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(49, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 107, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(107, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=107, bias=False)
        (4): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(107, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:47,587 - block_extractor.py[28] - INFO: save pruned block block-4 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-4-0.pt
2024-10-17 01:35:47,587 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (9): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:48,232 - block_extractor.py[28] - INFO: save pruned block block-4 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-4-2.pt
2024-10-17 01:35:48,232 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (9): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 570, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(570, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(570, 570, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=570, bias=False)
        (4): BatchNorm2d(570, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(570, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:48,684 - block_extractor.py[28] - INFO: save pruned block block-4 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-4-4.pt
2024-10-17 01:35:48,685 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (9): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 399, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(399, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(399, 399, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=399, bias=False)
        (4): BatchNorm2d(399, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(399, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:49,087 - block_extractor.py[28] - INFO: save pruned block block-4 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-4-6.pt
2024-10-17 01:35:49,087 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (9): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 233, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(233, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(233, 233, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=233, bias=False)
        (4): BatchNorm2d(233, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(233, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:49,458 - block_extractor.py[28] - INFO: save pruned block block-4 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-4-8.pt
2024-10-17 01:35:49,458 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (9): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 94, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(94, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=94, bias=False)
        (4): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(94, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:49,603 - block_extractor.py[28] - INFO: save pruned block block-5 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-5-0.pt
2024-10-17 01:35:49,603 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (10): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:50,110 - block_extractor.py[28] - INFO: save pruned block block-5 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-5-2.pt
2024-10-17 01:35:50,110 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (10): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 565, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(565, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(565, 565, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=565, bias=False)
        (4): BatchNorm2d(565, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(565, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:50,553 - block_extractor.py[28] - INFO: save pruned block block-5 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-5-4.pt
2024-10-17 01:35:50,553 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (10): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 394, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(394, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(394, 394, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=394, bias=False)
        (4): BatchNorm2d(394, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(394, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:50,948 - block_extractor.py[28] - INFO: save pruned block block-5 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-5-6.pt
2024-10-17 01:35:50,948 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (10): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 227, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(227, 227, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=227, bias=False)
        (4): BatchNorm2d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(227, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:51,308 - block_extractor.py[28] - INFO: save pruned block block-5 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-5-8.pt
2024-10-17 01:35:51,308 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (10): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(88, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:51,379 - block_extractor.py[28] - INFO: save pruned block block-6 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-6-0.pt
2024-10-17 01:35:51,379 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (11): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
        (4): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:52,439 - block_extractor.py[28] - INFO: save pruned block block-6 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-6-2.pt
2024-10-17 01:35:52,439 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (11): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 538, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(538, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(538, 538, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=538, bias=False)
        (4): BatchNorm2d(538, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(538, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 844, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(844, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(844, 844, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=844, bias=False)
        (4): BatchNorm2d(844, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(844, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:53,346 - block_extractor.py[28] - INFO: save pruned block block-6 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-6-4.pt
2024-10-17 01:35:53,346 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (11): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(352, 352, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=352, bias=False)
        (4): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(352, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 592, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(592, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(592, 592, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=592, bias=False)
        (4): BatchNorm2d(592, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(592, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:54,152 - block_extractor.py[28] - INFO: save pruned block block-6 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-6-6.pt
2024-10-17 01:35:54,152 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (11): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 202, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(202, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(202, 202, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=202, bias=False)
        (4): BatchNorm2d(202, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(202, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 364, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(364, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(364, 364, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=364, bias=False)
        (4): BatchNorm2d(364, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(364, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:54,910 - block_extractor.py[28] - INFO: save pruned block block-6 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-6-8.pt
2024-10-17 01:35:54,910 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (11): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(128, 79, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(79, 79, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=79, bias=False)
        (4): BatchNorm2d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(79, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 142, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(142, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(142, 142, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=142, bias=False)
        (4): BatchNorm2d(142, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(142, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:54,976 - block_extractor.py[28] - INFO: save pruned block block-7 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-7-0.pt
2024-10-17 01:35:54,976 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (13): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
        (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:55,658 - block_extractor.py[28] - INFO: save pruned block block-7 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-7-2.pt
2024-10-17 01:35:55,658 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (13): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 854, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(854, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(854, 854, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=854, bias=False)
        (4): BatchNorm2d(854, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(854, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:56,238 - block_extractor.py[28] - INFO: save pruned block block-7 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-7-4.pt
2024-10-17 01:35:56,239 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (13): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 586, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(586, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(586, 586, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=586, bias=False)
        (4): BatchNorm2d(586, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(586, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:56,728 - block_extractor.py[28] - INFO: save pruned block block-7 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-7-6.pt
2024-10-17 01:35:56,729 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (13): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 359, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(359, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(359, 359, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=359, bias=False)
        (4): BatchNorm2d(359, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(359, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:57,145 - block_extractor.py[28] - INFO: save pruned block block-7 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-7-8.pt
2024-10-17 01:35:57,145 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (13): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 155, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(155, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(155, 155, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=155, bias=False)
        (4): BatchNorm2d(155, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(155, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:35:57,227 - block_extractor.py[28] - INFO: save pruned block block-8 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-8-0.pt
2024-10-17 01:35:57,227 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (14): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1152, bias=False)
        (4): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)
        (4): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:35:59,033 - block_extractor.py[28] - INFO: save pruned block block-8 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-8-2.pt
2024-10-17 01:35:59,033 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (14): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 904, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(904, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(904, 904, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=904, bias=False)
        (4): BatchNorm2d(904, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(904, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 1398, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1398, 1398, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1398, bias=False)
        (4): BatchNorm2d(1398, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1398, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:36:00,475 - block_extractor.py[28] - INFO: save pruned block block-8 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-8-4.pt
2024-10-17 01:36:00,475 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (14): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 657, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(657, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(657, 657, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=657, bias=False)
        (4): BatchNorm2d(657, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(657, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 937, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(937, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(937, 937, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=937, bias=False)
        (4): BatchNorm2d(937, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(937, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:36:01,710 - block_extractor.py[28] - INFO: save pruned block block-8 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-8-6.pt
2024-10-17 01:36:01,710 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (14): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 422, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(422, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(422, 422, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=422, bias=False)
        (4): BatchNorm2d(422, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(422, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:36:02,546 - block_extractor.py[28] - INFO: save pruned block block-8 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-8-8.pt
2024-10-17 01:36:02,546 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (14): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(192, 219, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(219, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(219, 219, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=219, bias=False)
        (4): BatchNorm2d(219, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(219, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 227, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(227, 227, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=227, bias=False)
        (4): BatchNorm2d(227, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(227, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
2024-10-17 01:36:02,619 - block_extractor.py[28] - INFO: save pruned block block-9 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-9-0.pt
2024-10-17 01:36:02,619 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (16): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)
        (4): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:36:03,843 - block_extractor.py[28] - INFO: save pruned block block-9 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-9-2.pt
2024-10-17 01:36:03,843 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (16): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 1457, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1457, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1457, 1457, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1457, bias=False)
        (4): BatchNorm2d(1457, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1457, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:36:04,769 - block_extractor.py[28] - INFO: save pruned block block-9 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-9-4.pt
2024-10-17 01:36:04,769 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (16): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 942, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(942, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(942, 942, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=942, bias=False)
        (4): BatchNorm2d(942, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(942, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:36:05,478 - block_extractor.py[28] - INFO: save pruned block block-9 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-9-6.pt
2024-10-17 01:36:05,478 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (16): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 559, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(559, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(559, 559, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=559, bias=False)
        (4): BatchNorm2d(559, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(559, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:36:05,984 - block_extractor.py[28] - INFO: save pruned block block-9 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-9-8.pt
2024-10-17 01:36:05,984 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (16): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 214, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(214, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(214, 214, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=214, bias=False)
        (4): BatchNorm2d(214, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(214, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
    )
  )
)
2024-10-17 01:36:06,076 - block_extractor.py[28] - INFO: save pruned block block-10 (sparsity 0.0) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-10-0.pt
2024-10-17 01:36:06,076 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (17): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)
        (4): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): ModuleDict(
    (0): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)
2024-10-17 01:36:07,411 - block_extractor.py[28] - INFO: save pruned block block-10 (sparsity 0.2) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-10-2.pt
2024-10-17 01:36:07,411 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (17): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 1474, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1474, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1474, 1474, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1474, bias=False)
        (4): BatchNorm2d(1474, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1474, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): ModuleDict(
    (0): Conv2d(512, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)
2024-10-17 01:36:08,595 - block_extractor.py[28] - INFO: save pruned block block-10 (sparsity 0.4) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-10-4.pt
2024-10-17 01:36:08,595 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (17): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 1050, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1050, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(1050, 1050, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1050, bias=False)
        (4): BatchNorm2d(1050, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(1050, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): ModuleDict(
    (0): Conv2d(384, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)
2024-10-17 01:36:09,416 - block_extractor.py[28] - INFO: save pruned block block-10 (sparsity 0.6) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-10-6.pt
2024-10-17 01:36:09,417 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (17): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 597, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(597, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(597, 597, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=597, bias=False)
        (4): BatchNorm2d(597, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(597, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): ModuleDict(
    (0): Conv2d(256, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)
2024-10-17 01:36:10,013 - block_extractor.py[28] - INFO: save pruned block block-10 (sparsity 0.8) in results/legodnn/image_classification/mobilenetv2_w2_cifar100_0-125/compressed/block-10-8.pt
2024-10-17 01:36:10,013 - block_extractor.py[29] - DEBUG: LegoDNNBlock(
  (features): ModuleDict(
    (17): ModuleDict(
      (conv): ModuleDict(
        (0): Conv2d(320, 221, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(221, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6()
        (3): Conv2d(221, 221, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=221, bias=False)
        (4): BatchNorm2d(221, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6()
        (6): Conv2d(221, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): ModuleDict(
    (0): Conv2d(128, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)
2024-10-17 01:36:12,468 - block_trainer.py[183] - INFO: start block training...
2024-10-17 01:39:17,957 - block_trainer.py[357] - INFO: epoch 0 (185.489609s, 55 blocks still need training), blocks loss: 
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-0 | 0.00163885 | 0.00332008 | 0.02066330 | 0.09949450 | 0.32589795 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-1 | 0.00237146 | 0.01150890 | 0.09291111 | 0.36699614 | 0.78341493 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-2 | 0.00106107 | 0.00171066 | 0.00666115 | 0.01768570 | 0.05623746 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-3 | 0.00168116 | 0.00449649 | 0.01629078 | 0.05249620 | 0.13436754 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-4 | 0.00011375 | 0.00031000 | 0.00114455 | 0.00382847 | 0.01047193 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-5 | 0.00008669 | 0.00021299 | 0.00089658 | 0.00281656 | 0.00709163 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-6 | 0.00094664 | 0.00518572 | 0.01893952 | 0.04939237 | 0.14247394 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-7 | 0.00008352 | 0.00015885 | 0.00073412 | 0.00261115 | 0.00764759 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-8 | 0.00260653 | 0.00351633 | 0.00899195 | 0.02574546 | 0.08514238 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+---------+------------+------------+------------+------------+------------+
|         |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+---------+------------+------------+------------+------------+------------+
| block-9 | 0.00059283 | 0.00075810 | 0.00325855 | 0.00996312 | 0.02553505 |
|         |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+---------+------------+------------+------------+------------+------------+
+----------+------------+------------+------------+------------+------------+
|          |    0.0     |    0.2     |    0.4     |    0.6     |    0.8     |
+----------+------------+------------+------------+------------+------------+
| block-10 | 0.00389360 | 0.00424321 | 0.01006132 | 0.03665438 | 0.12241199 |
|          |    (-)     |    (-)     |    (-)     |    (-)     |    (-)     |
+----------+------------+------------+------------+------------+------------+

2024-10-17 01:42:23,623 - block_trainer.py[357] - INFO: epoch 1 (185.664794s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00161370   |   0.00201071   |   0.00746570   |   0.03520263   |   0.16429062   |
|         | ( 0.00002516) | ( 0.00130937) | ( 0.01319760) | ( 0.06429187) | ( 0.16160733) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00236213   |   0.00610993   |   0.02610137   |   0.10024450   |   0.36571012   |
|         | ( 0.00000934) | ( 0.00539897) | ( 0.06680973) | ( 0.26675164) | ( 0.41770482) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00105972   |   0.00155572   |   0.00461219   |   0.01112715   |   0.03295759   |
|         | ( 0.00000135) | ( 0.00015494) | ( 0.00204896) | ( 0.00655855) | ( 0.02327987) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-3 |    0.00169629   |   0.00375260   |   0.01026550   |   0.02587617   |   0.06074308   |
|         | ( -0.00001513) | ( 0.00074389) | ( 0.00602528) | ( 0.02662003) | ( 0.07362446) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00011307   |   0.00026921   |   0.00086644   |   0.00254044   |   0.00631927   |
|         | ( 0.00000068) | ( 0.00004078) | ( 0.00027810) | ( 0.00128803) | ( 0.00415266) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008543   |   0.00019065   |   0.00070716   |   0.00205284   |   0.00485630   |
|         | ( 0.00000126) | ( 0.00002234) | ( 0.00018941) | ( 0.00076372) | ( 0.00223532) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00095243   |   0.00430654   |   0.01309584   |   0.02983413   |   0.07257498   |
|         | ( -0.00000579) | ( 0.00087918) | ( 0.00584368) | ( 0.01955823) | ( 0.06989895) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00008182   |   0.00014694   |   0.00057576   |   0.00186134   |   0.00548935   |
|         | ( 0.00000170) | ( 0.00001191) | ( 0.00015836) | ( 0.00074981) | ( 0.00215823) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00259502   |   0.00339656   |   0.00782667   |   0.01990707   |   0.05505911   |
|         | ( 0.00001150) | ( 0.00011978) | ( 0.00116528) | ( 0.00583839) | ( 0.03008328) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00058193   |   0.00073150   |   0.00263999   |   0.00752918   |   0.01857674   |
|         | ( 0.00001090) | ( 0.00002660) | ( 0.00061856) | ( 0.00243394) | ( 0.00695831) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00382419   |   0.00390798   |   0.00727136   |   0.02270205   |   0.07685739   |
|          | ( 0.00006941) | ( 0.00033523) | ( 0.00278996) | ( 0.01395233) | ( 0.04555459) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 01:45:29,276 - block_trainer.py[357] - INFO: epoch 2 (185.652785s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00154727   |   0.00186335   |   0.00613725   |   0.02682303   |   0.12326672   |
|         | ( 0.00006643) | ( 0.00014736) | ( 0.00132845) | ( 0.00837960) | ( 0.04102390) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00221796   |   0.00553119   |   0.02065232   |   0.06423378   |   0.21574979   |
|         | ( 0.00014417) | ( 0.00057874) | ( 0.00544905) | ( 0.03601072) | ( 0.14996033) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00099895   |   0.00147967   |   0.00421338   |   0.01017116   |   0.02968786   |
|         | ( 0.00006077) | ( 0.00007606) | ( 0.00039882) | ( 0.00095600) | ( 0.00326974) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00160586   |   0.00357131   |   0.00946732   |   0.02233852   |   0.04922344   |
|         | ( 0.00009043) | ( 0.00018128) | ( 0.00079818) | ( 0.00353766) | ( 0.01151964) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010489   |   0.00025748   |   0.00079929   |   0.00238481   |   0.00571047   |
|         | ( 0.00000818) | ( 0.00001173) | ( 0.00006715) | ( 0.00015563) | ( 0.00060881) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008528   |   0.00018757   |   0.00066190   |   0.00192042   |   0.00452307   |
|         | ( 0.00000015) | ( 0.00000308) | ( 0.00004527) | ( 0.00013242) | ( 0.00033323) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091948   |   0.00416354   |   0.01232244   |   0.02729379   |   0.06078984   |
|         | ( 0.00003295) | ( 0.00014300) | ( 0.00077341) | ( 0.00254035) | ( 0.01178514) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00008001   |   0.00014462   |   0.00055442   |   0.00170824   |   0.00510090   |
|         | ( 0.00000182) | ( 0.00000232) | ( 0.00002134) | ( 0.00015310) | ( 0.00038846) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00255217   |   0.00332255   |   0.00765833   |   0.01916021   |   0.05041064   |
|         | ( 0.00004285) | ( 0.00007401) | ( 0.00016835) | ( 0.00074686) | ( 0.00464847) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00057842   |   0.00072680   |   0.00257557   |   0.00719618   |   0.01750697   |
|         | ( 0.00000351) | ( 0.00000470) | ( 0.00006442) | ( 0.00033300) | ( 0.00106976) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00381679   |   0.00390102   |   0.00717695   |   0.02211169   |   0.06877110   |
|          | ( 0.00000741) | ( 0.00000695) | ( 0.00009442) | ( 0.00059036) | ( 0.00808630) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 01:48:34,237 - block_trainer.py[357] - INFO: epoch 3 (184.960520s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00153486   |   0.00181447   |   0.00547674   |   0.02329229   |   0.10385155   |
|         | ( 0.00001240) | ( 0.00004888) | ( 0.00066051) | ( 0.00353074) | ( 0.01941517) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-1 |    0.00229106   |   0.00541433   |   0.01851148   |   0.05274663   |   0.15510099   |
|         | ( -0.00007309) | ( 0.00011686) | ( 0.00214084) | ( 0.01148715) | ( 0.06064880) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-2 |    0.00101857   |    0.00148721   |   0.00408364   |   0.00967171   |   0.02826572   |
|         | ( -0.00001962) | ( -0.00000755) | ( 0.00012974) | ( 0.00049945) | ( 0.00142214) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-3 |    0.00164209   |   0.00356528   |   0.00917086   |   0.02080967   |   0.04421952   |
|         | ( -0.00003623) | ( 0.00000603) | ( 0.00029646) | ( 0.00152885) | ( 0.00500392) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-4 |    0.00011184   |    0.00026182   |   0.00077661   |   0.00230827   |   0.00543790   |
|         | ( -0.00000695) | ( -0.00000434) | ( 0.00002269) | ( 0.00007654) | ( 0.00027257) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008259   |   0.00018151   |   0.00063591   |   0.00183983   |   0.00435287   |
|         | ( 0.00000269) | ( 0.00000607) | ( 0.00002599) | ( 0.00008059) | ( 0.00017020) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00092667   |   0.00407816   |   0.01191600   |   0.02606800   |   0.05593025   |
|         | ( -0.00000719) | ( 0.00008538) | ( 0.00040643) | ( 0.00122578) | ( 0.00485959) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-7 |    0.00008118   |    0.00014565   |   0.00054569   |   0.00166491   |   0.00488127   |
|         | ( -0.00000117) | ( -0.00000103) | ( 0.00000873) | ( 0.00004332) | ( 0.00021963) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-8 |    0.00255942   |   0.00332107   |   0.00756256   |   0.01880715   |   0.04823563   |
|         | ( -0.00000724) | ( 0.00000148) | ( 0.00009577) | ( 0.00035306) | ( 0.00217500) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00057431   |   0.00072222   |   0.00252290   |   0.00698278   |   0.01696734   |
|         | ( 0.00000411) | ( 0.00000458) | ( 0.00005267) | ( 0.00021340) | ( 0.00053964) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+----------------+----------------+----------------+
|          |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+----------------+----------------+----------------+
| block-10 |    0.00384216   |    0.00392654   |   0.00712574   |   0.02184284   |   0.06450628   |
|          | ( -0.00002537) | ( -0.00002552) | ( 0.00005121) | ( 0.00026885) | ( 0.00426481) |
+----------+-----------------+-----------------+----------------+----------------+----------------+

2024-10-17 01:51:39,908 - block_trainer.py[357] - INFO: epoch 4 (185.670479s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00156024   |    0.00182102   |   0.00512048   |   0.02138562   |   0.09335408   |
|         | ( -0.00002538) | ( -0.00000655) | ( 0.00035626) | ( 0.00190666) | ( 0.01049746) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-1 |    0.00229783   |   0.00529998   |   0.01723670   |   0.04663145   |   0.12690592   |
|         | ( -0.00000678) | ( 0.00011434) | ( 0.00127478) | ( 0.00611518) | ( 0.02819507) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-2 |    0.00103310   |    0.00149609   |   0.00404667   |   0.00934241   |   0.02734580   |
|         | ( -0.00001453) | ( -0.00000888) | ( 0.00003696) | ( 0.00032930) | ( 0.00091992) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-3 |    0.00165061   |   0.00355023   |   0.00896029   |   0.01994950   |   0.04157366   |
|         | ( -0.00000852) | ( 0.00001505) | ( 0.00021056) | ( 0.00086017) | ( 0.00264586) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010894   |   0.00025812   |   0.00076508   |   0.00225153   |   0.00528108   |
|         | ( 0.00000290) | ( 0.00000370) | ( 0.00001153) | ( 0.00005674) | ( 0.00015682) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-5 |    0.00008462   |   0.00018136   |   0.00062063   |   0.00179873   |   0.00425131   |
|         | ( -0.00000203) | ( 0.00000015) | ( 0.00001528) | ( 0.00004110) | ( 0.00010156) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092222   |   0.00401584   |   0.01165849   |   0.02525618   |   0.05342504   |
|         | ( 0.00000445) | ( 0.00006233) | ( 0.00025751) | ( 0.00081183) | ( 0.00250521) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007955   |   0.00014406   |   0.00054095   |   0.00164068   |   0.00475019   |
|         | ( 0.00000162) | ( 0.00000159) | ( 0.00000474) | ( 0.00002424) | ( 0.00013108) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00254514   |   0.00330089   |   0.00749867   |   0.01853690   |   0.04696020   |
|         | ( 0.00001428) | ( 0.00002018) | ( 0.00006389) | ( 0.00027025) | ( 0.00127544) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00056129   |   0.00071018   |   0.00249443   |   0.00677728   |   0.01661060   |
|         | ( 0.00001302) | ( 0.00001204) | ( 0.00002847) | ( 0.00020550) | ( 0.00035674) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00381956   |   0.00390402   |   0.00704502   |   0.02159444   |   0.06176754   |
|          | ( 0.00002260) | ( 0.00002252) | ( 0.00008072) | ( 0.00024840) | ( 0.00273874) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 01:54:46,164 - block_trainer.py[357] - INFO: epoch 5 (186.256257s, 55 blocks still need training), blocks loss: 
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-0 |    0.00156581   |   0.00180978   |   0.00487882   |   0.02009399   |   0.08682038   |
|         | ( -0.00000557) | ( 0.00001124) | ( 0.00024166) | ( 0.00129163) | ( 0.00653371) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-1 |    0.00229901   |   0.00521534   |   0.01637923   |   0.04263599   |   0.11135182   |
|         | ( -0.00000117) | ( 0.00008464) | ( 0.00085747) | ( 0.00399546) | ( 0.01555410) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102810   |   0.00148602   |   0.00401045   |   0.00916791   |   0.02669884   |
|         | ( 0.00000500) | ( 0.00001007) | ( 0.00003622) | ( 0.00017450) | ( 0.00064696) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00164253   |   0.00352219   |   0.00879379   |   0.01932333   |   0.03990815   |
|         | ( 0.00000808) | ( 0.00002804) | ( 0.00016651) | ( 0.00062617) | ( 0.00166551) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-4 |    0.00011625   |    0.00026022   |   0.00076446   |   0.00221840   |   0.00517564   |
|         | ( -0.00000731) | ( -0.00000210) | ( 0.00000062) | ( 0.00003313) | ( 0.00010544) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008771   |    0.00018380   |   0.00061519   |   0.00177263   |   0.00418287   |
|         | ( -0.00000309) | ( -0.00000244) | ( 0.00000545) | ( 0.00002610) | ( 0.00006844) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00093847   |   0.00397590   |   0.01147932   |   0.02465936   |   0.05184576   |
|         | ( -0.00001625) | ( 0.00003994) | ( 0.00017917) | ( 0.00059682) | ( 0.00157927) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-7 |    0.00008001   |    0.00014440   |   0.00053907   |   0.00162230   |   0.00466109   |
|         | ( -0.00000045) | ( -0.00000034) | ( 0.00000189) | ( 0.00001838) | ( 0.00008910) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00255523   |    0.00330398   |   0.00746760   |   0.01833650   |   0.04609617   |
|         | ( -0.00001009) | ( -0.00000309) | ( 0.00003107) | ( 0.00020041) | ( 0.00086402) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00057765   |    0.00072581   |    0.00249753   |   0.00663981   |   0.01634861   |
|         | ( -0.00001636) | ( -0.00001563) | ( -0.00000310) | ( 0.00013747) | ( 0.00026199) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00378531   |   0.00386788   |   0.00697302   |   0.02137010   |   0.06001768   |
|          | ( 0.00003424) | ( 0.00003614) | ( 0.00007199) | ( 0.00022434) | ( 0.00174986) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 01:57:52,185 - block_trainer.py[357] - INFO: epoch 6 (186.020575s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00158235   |    0.00182085   |   0.00474477   |   0.01917816   |   0.08247500   |
|         | ( -0.00001654) | ( -0.00001106) | ( 0.00013405) | ( 0.00091583) | ( 0.00434538) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00225829   |   0.00513042   |   0.01576754   |   0.03985062   |   0.10151827   |
|         | ( 0.00004071) | ( 0.00008492) | ( 0.00061169) | ( 0.00278537) | ( 0.00983356) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-2 |    0.00103748   |    0.00149348   |   0.00398496   |   0.00908525   |   0.02625717   |
|         | ( -0.00000938) | ( -0.00000746) | ( 0.00002549) | ( 0.00008266) | ( 0.00044167) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-3 |    0.00165771   |    0.00352538   |   0.00869578   |   0.01888129   |   0.03886887   |
|         | ( -0.00001519) | ( -0.00000320) | ( 0.00009801) | ( 0.00044203) | ( 0.00103928) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010693   |   0.00025085   |   0.00075495   |   0.00219551   |   0.00510428   |
|         | ( 0.00000932) | ( 0.00000937) | ( 0.00000951) | ( 0.00002288) | ( 0.00007136) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008223   |   0.00017757   |   0.00060642   |   0.00175336   |   0.00413783   |
|         | ( 0.00000548) | ( 0.00000623) | ( 0.00000877) | ( 0.00001927) | ( 0.00004504) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00093013   |   0.00392292   |   0.01132311   |   0.02422716   |   0.05079904   |
|         | ( 0.00000833) | ( 0.00005298) | ( 0.00015620) | ( 0.00043220) | ( 0.00104672) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-7 |    0.00008135   |    0.00014559   |   0.00053857   |   0.00161026   |   0.00459651   |
|         | ( -0.00000134) | ( -0.00000119) | ( 0.00000050) | ( 0.00001204) | ( 0.00006458) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00254380   |   0.00328983   |   0.00743033   |   0.01816765   |   0.04549486   |
|         | ( 0.00001143) | ( 0.00001415) | ( 0.00003728) | ( 0.00016885) | ( 0.00060131) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00056284   |   0.00071122   |   0.00247660   |   0.00655279   |   0.01615632   |
|         | ( 0.00001480) | ( 0.00001459) | ( 0.00002093) | ( 0.00008702) | ( 0.00019228) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00380999   |    0.00389390   |    0.00697995   |   0.02129641   |   0.05892435   |
|          | ( -0.00002468) | ( -0.00002602) | ( -0.00000693) | ( 0.00007369) | ( 0.00109332) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 02:00:58,282 - block_trainer.py[357] - INFO: epoch 7 (186.096676s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00162275   |    0.00185350   |   0.00466906   |   0.01842941   |   0.07918126   |
|         | ( -0.00004040) | ( -0.00003266) | ( 0.00007571) | ( 0.00074876) | ( 0.00329374) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-1 |    0.00233804   |    0.00516773   |   0.01538793   |   0.03784931   |   0.09472506   |
|         | ( -0.00007974) | ( -0.00003731) | ( 0.00037961) | ( 0.00200131) | ( 0.00679321) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-2 |    0.00103798   |   0.00149163   |   0.00393965   |   0.00900519   |   0.02591281   |
|         | ( -0.00000049) | ( 0.00000185) | ( 0.00004531) | ( 0.00008006) | ( 0.00034437) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00165125   |   0.00350839   |   0.00859448   |   0.01850481   |   0.03804453   |
|         | ( 0.00000646) | ( 0.00001699) | ( 0.00010130) | ( 0.00037648) | ( 0.00082434) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00010833   |    0.00025179   |    0.00075514   |   0.00218415   |   0.00505340   |
|         | ( -0.00000140) | ( -0.00000094) | ( -0.00000019) | ( 0.00001136) | ( 0.00005089) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-5 |    0.00008225   |   0.00017668   |   0.00060359   |   0.00173568   |   0.00410476   |
|         | ( -0.00000002) | ( 0.00000089) | ( 0.00000284) | ( 0.00001768) | ( 0.00003307) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092384   |   0.00387773   |   0.01118838   |   0.02386836   |   0.05003080   |
|         | ( 0.00000630) | ( 0.00004519) | ( 0.00013473) | ( 0.00035880) | ( 0.00076824) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007819   |   0.00014263   |   0.00053436   |   0.00159507   |   0.00454807   |
|         | ( 0.00000316) | ( 0.00000296) | ( 0.00000421) | ( 0.00001519) | ( 0.00004844) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00253760   |   0.00328176   |   0.00739965   |   0.01800605   |   0.04500441   |
|         | ( 0.00000620) | ( 0.00000807) | ( 0.00003068) | ( 0.00016160) | ( 0.00049045) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-9 |    0.00057150   |    0.00071846   |   0.00247504   |   0.00649160   |   0.01600177   |
|         | ( -0.00000866) | ( -0.00000724) | ( 0.00000156) | ( 0.00006119) | ( 0.00015455) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00377543   |   0.00386088   |   0.00694226   |   0.02117207   |   0.05809616   |
|          | ( 0.00003456) | ( 0.00003302) | ( 0.00003770) | ( 0.00012433) | ( 0.00082820) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:04:04,106 - block_trainer.py[357] - INFO: epoch 8 (185.823186s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00164212   |    0.00186580   |   0.00458811   |   0.01780001   |   0.07659720   |
|         | ( -0.00001937) | ( -0.00001229) | ( 0.00008095) | ( 0.00062940) | ( 0.00258406) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-1 |    0.00241771   |    0.00519533   |   0.01508243   |   0.03627475   |   0.08954906   |
|         | ( -0.00007967) | ( -0.00002761) | ( 0.00030550) | ( 0.00157456) | ( 0.00517600) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-2 |    0.00105849   |    0.00150980   |   0.00393314   |   0.00896144   |   0.02568672   |
|         | ( -0.00002051) | ( -0.00001817) | ( 0.00000651) | ( 0.00004374) | ( 0.00022609) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-3 |    0.00168336   |    0.00353086   |   0.00854080   |   0.01823169   |   0.03742927   |
|         | ( -0.00003211) | ( -0.00002247) | ( 0.00005368) | ( 0.00027313) | ( 0.00061526) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-4 |    0.00011072   |    0.00025217   |   0.00075470   |   0.00217444   |   0.00501315   |
|         | ( -0.00000240) | ( -0.00000038) | ( 0.00000044) | ( 0.00000971) | ( 0.00004024) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008325   |    0.00017681   |   0.00059948   |   0.00171702   |   0.00408016   |
|         | ( -0.00000100) | ( -0.00000012) | ( 0.00000410) | ( 0.00001867) | ( 0.00002460) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00094262   |   0.00386133   |   0.01109661   |   0.02358060   |   0.04940994   |
|         | ( -0.00001878) | ( 0.00001640) | ( 0.00009177) | ( 0.00028776) | ( 0.00062087) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-7 |    0.00007931   |    0.00014349   |   0.00053256   |   0.00158212   |   0.00451247   |
|         | ( -0.00000112) | ( -0.00000086) | ( 0.00000180) | ( 0.00001295) | ( 0.00003560) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00257210   |    0.00331664   |   0.00738763   |   0.01791268   |   0.04464000   |
|         | ( -0.00003451) | ( -0.00003488) | ( 0.00001202) | ( 0.00009337) | ( 0.00036440) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00056299   |   0.00070920   |   0.00246312   |   0.00642192   |   0.01586004   |
|         | ( 0.00000852) | ( 0.00000926) | ( 0.00001193) | ( 0.00006968) | ( 0.00014174) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00379966   |    0.00388447   |    0.00695271   |   0.02109168   |   0.05745031   |
|          | ( -0.00002423) | ( -0.00002359) | ( -0.00001045) | ( 0.00008039) | ( 0.00064585) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 02:07:08,689 - block_trainer.py[357] - INFO: epoch 9 (184.582072s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00153922   |   0.00176204   |   0.00442029   |   0.01717935   |   0.07447643   |
|         | ( 0.00010290) | ( 0.00010376) | ( 0.00016782) | ( 0.00062066) | ( 0.00212077) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00225771   |   0.00501536   |   0.01465901   |   0.03487656   |   0.08528605   |
|         | ( 0.00015999) | ( 0.00017997) | ( 0.00042342) | ( 0.00139819) | ( 0.00426301) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102536   |   0.00146647   |   0.00387427   |   0.00887632   |   0.02545208   |
|         | ( 0.00003313) | ( 0.00004333) | ( 0.00005887) | ( 0.00008512) | ( 0.00023464) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00164983   |   0.00349003   |   0.00843270   |   0.01796239   |   0.03689017   |
|         | ( 0.00003354) | ( 0.00004083) | ( 0.00010810) | ( 0.00026929) | ( 0.00053910) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010464   |   0.00024495   |   0.00074700   |   0.00215957   |   0.00497580   |
|         | ( 0.00000609) | ( 0.00000722) | ( 0.00000770) | ( 0.00001487) | ( 0.00003735) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008198   |   0.00017575   |   0.00059457   |   0.00170866   |   0.00406044   |
|         | ( 0.00000127) | ( 0.00000106) | ( 0.00000492) | ( 0.00000835) | ( 0.00001972) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092776   |   0.00382723   |   0.01099472   |   0.02331439   |   0.04888500   |
|         | ( 0.00001486) | ( 0.00003410) | ( 0.00010188) | ( 0.00026621) | ( 0.00052494) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007612   |   0.00014071   |   0.00052794   |   0.00157099   |   0.00448209   |
|         | ( 0.00000318) | ( 0.00000278) | ( 0.00000462) | ( 0.00001113) | ( 0.00003039) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00255317   |   0.00329800   |   0.00734829   |   0.01779823   |   0.04430354   |
|         | ( 0.00001894) | ( 0.00001864) | ( 0.00003935) | ( 0.00011445) | ( 0.00033646) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00056064   |   0.00070515   |   0.00245490   |   0.00637133   |   0.01576780   |
|         | ( 0.00000234) | ( 0.00000405) | ( 0.00000821) | ( 0.00005059) | ( 0.00009224) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00379397   |   0.00387987   |   0.00694373   |   0.02103035   |   0.05702989   |
|          | ( 0.00000569) | ( 0.00000460) | ( 0.00000898) | ( 0.00006133) | ( 0.00042042) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:10:13,384 - block_trainer.py[357] - INFO: epoch 10 (184.694583s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00156867   |    0.00178644   |   0.00438443   |   0.01677879   |   0.07286090   |
|         | ( -0.00002945) | ( -0.00002440) | ( 0.00003585) | ( 0.00040056) | ( 0.00161553) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-1 |    0.00227416   |   0.00499590   |   0.01441276   |   0.03384875   |   0.08184560   |
|         | ( -0.00001645) | ( 0.00001946) | ( 0.00024625) | ( 0.00102781) | ( 0.00344045) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-2 |    0.00103061   |    0.00146790   |   0.00383977   |   0.00883241   |   0.02526530   |
|         | ( -0.00000525) | ( -0.00000143) | ( 0.00003449) | ( 0.00004391) | ( 0.00018678) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00164637   |   0.00347864   |   0.00835457   |   0.01773804   |   0.03643033   |
|         | ( 0.00000346) | ( 0.00001139) | ( 0.00007813) | ( 0.00022435) | ( 0.00045984) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00011118   |    0.00024946   |    0.00074892   |   0.00215291   |   0.00494914   |
|         | ( -0.00000654) | ( -0.00000450) | ( -0.00000192) | ( 0.00000666) | ( 0.00002666) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008532   |    0.00017872   |   0.00059345   |   0.00170441   |   0.00404489   |
|         | ( -0.00000334) | ( -0.00000296) | ( 0.00000112) | ( 0.00000426) | ( 0.00001555) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00094115   |   0.00382578   |   0.01094179   |   0.02312530   |   0.04850304   |
|         | ( -0.00001339) | ( 0.00000145) | ( 0.00005293) | ( 0.00018909) | ( 0.00038195) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-7 |    0.00008155   |    0.00014591   |    0.00053094   |   0.00156343   |   0.00445962   |
|         | ( -0.00000542) | ( -0.00000521) | ( -0.00000300) | ( 0.00000756) | ( 0.00002246) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00257422   |    0.00331792   |   0.00734773   |   0.01774859   |   0.04406858   |
|         | ( -0.00002106) | ( -0.00001992) | ( 0.00000055) | ( 0.00004963) | ( 0.00023496) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00057027   |    0.00071411   |    0.00245864   |   0.00635093   |   0.01569115   |
|         | ( -0.00000963) | ( -0.00000897) | ( -0.00000374) | ( 0.00002040) | ( 0.00007666) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00377540   |   0.00386294   |   0.00692468   |   0.02097744   |   0.05665080   |
|          | ( 0.00001857) | ( 0.00001692) | ( 0.00001905) | ( 0.00005290) | ( 0.00037909) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:13:18,441 - block_trainer.py[357] - INFO: epoch 11 (185.056930s, 55 blocks still need training), blocks loss: 
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-0 |    0.00157348   |   0.00178496   |   0.00433448   |   0.01645655   |   0.07159929   |
|         | ( -0.00000481) | ( 0.00000147) | ( 0.00004995) | ( 0.00032224) | ( 0.00126161) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-1 |    0.00229675   |    0.00499954   |   0.01424691   |   0.03305685   |   0.07904774   |
|         | ( -0.00002259) | ( -0.00000364) | ( 0.00016585) | ( 0.00079190) | ( 0.00279786) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102819   |   0.00146488   |   0.00382162   |   0.00880007   |   0.02511772   |
|         | ( 0.00000242) | ( 0.00000302) | ( 0.00001815) | ( 0.00003234) | ( 0.00014758) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00163115   |   0.00345844   |   0.00827901   |   0.01755123   |   0.03604689   |
|         | ( 0.00001522) | ( 0.00002020) | ( 0.00007556) | ( 0.00018681) | ( 0.00038344) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010778   |   0.00024642   |   0.00074625   |   0.00214616   |   0.00492656   |
|         | ( 0.00000340) | ( 0.00000304) | ( 0.00000268) | ( 0.00000675) | ( 0.00002258) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008306   |   0.00017638   |   0.00058880   |   0.00169880   |   0.00403385   |
|         | ( 0.00000225) | ( 0.00000234) | ( 0.00000465) | ( 0.00000561) | ( 0.00001105) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092236   |   0.00379391   |   0.01086217   |   0.02293533   |   0.04814898   |
|         | ( 0.00001879) | ( 0.00003187) | ( 0.00007962) | ( 0.00018997) | ( 0.00035407) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00008058   |   0.00014469   |   0.00052879   |   0.00155136   |   0.00443713   |
|         | ( 0.00000097) | ( 0.00000122) | ( 0.00000215) | ( 0.00001206) | ( 0.00002249) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00254661   |   0.00328972   |   0.00730136   |   0.01765042   |   0.04379060   |
|         | ( 0.00002761) | ( 0.00002820) | ( 0.00004637) | ( 0.00009817) | ( 0.00027798) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00055722   |   0.00070089   |   0.00244264   |   0.00631675   |   0.01560389   |
|         | ( 0.00001305) | ( 0.00001323) | ( 0.00001600) | ( 0.00003417) | ( 0.00008725) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00377085   |   0.00385820   |   0.00691282   |   0.02090169   |   0.05628094   |
|          | ( 0.00000455) | ( 0.00000475) | ( 0.00001186) | ( 0.00007576) | ( 0.00036986) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:16:22,538 - block_trainer.py[357] - INFO: epoch 12 (184.096952s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00157932   |    0.00179130   |   0.00430098   |   0.01616781   |   0.07051794   |
|         | ( -0.00000584) | ( -0.00000634) | ( 0.00003350) | ( 0.00028874) | ( 0.00108135) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-1 |    0.00239623   |    0.00506570   |   0.01414500   |   0.03241557   |   0.07672831   |
|         | ( -0.00009948) | ( -0.00006616) | ( 0.00010191) | ( 0.00064128) | ( 0.00231943) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-2 |    0.00105767   |    0.00149321   |    0.00383401   |   0.00879812   |   0.02496108   |
|         | ( -0.00002948) | ( -0.00002833) | ( -0.00001239) | ( 0.00000196) | ( 0.00015664) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-3 |    0.00167575   |    0.00349746   |   0.00826507   |   0.01744356   |   0.03573556   |
|         | ( -0.00004460) | ( -0.00003902) | ( 0.00001394) | ( 0.00010768) | ( 0.00031133) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00011569   |    0.00025282   |    0.00075035   |   0.00214419   |   0.00491255   |
|         | ( -0.00000791) | ( -0.00000641) | ( -0.00000410) | ( 0.00000197) | ( 0.00001402) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008420   |    0.00017730   |   0.00058609   |   0.00169477   |   0.00402398   |
|         | ( -0.00000114) | ( -0.00000092) | ( 0.00000271) | ( 0.00000403) | ( 0.00000987) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00094393   |    0.00379970   |   0.01082102   |   0.02280556   |   0.04786136   |
|         | ( -0.00002158) | ( -0.00000579) | ( 0.00004115) | ( 0.00012977) | ( 0.00028762) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-7 |    0.00008284   |    0.00014678   |    0.00052966   |   0.00154148   |   0.00441915   |
|         | ( -0.00000227) | ( -0.00000209) | ( -0.00000087) | ( 0.00000988) | ( 0.00001798) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00256036   |    0.00330329   |   0.00729681   |   0.01760138   |   0.04361123   |
|         | ( -0.00001375) | ( -0.00001357) | ( 0.00000454) | ( 0.00004904) | ( 0.00017937) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00056388   |    0.00070740   |    0.00244385   |   0.00630294   |   0.01554410   |
|         | ( -0.00000666) | ( -0.00000652) | ( -0.00000121) | ( 0.00001381) | ( 0.00005980) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00377761   |    0.00386608   |    0.00692002   |   0.02087305   |   0.05603390   |
|          | ( -0.00000676) | ( -0.00000788) | ( -0.00000719) | ( 0.00002863) | ( 0.00024704) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 02:19:26,873 - block_trainer.py[357] - INFO: epoch 13 (184.334529s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00159268   |    0.00180290   |   0.00427561   |   0.01590855   |   0.06957639   |
|         | ( -0.00001335) | ( -0.00001160) | ( 0.00002537) | ( 0.00025927) | ( 0.00094154) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00235925   |   0.00500317   |   0.01394472   |   0.03180268   |   0.07474513   |
|         | ( 0.00003698) | ( 0.00006253) | ( 0.00020028) | ( 0.00061289) | ( 0.00198318) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-2 |    0.00106000   |    0.00149523   |   0.00382825   |   0.00877984   |   0.02481414   |
|         | ( -0.00000233) | ( -0.00000202) | ( 0.00000576) | ( 0.00001827) | ( 0.00014694) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00166861   |   0.00348573   |   0.00820998   |   0.01731376   |   0.03545192   |
|         | ( 0.00000714) | ( 0.00001173) | ( 0.00005509) | ( 0.00012979) | ( 0.00028364) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00011033   |   0.00024836   |   0.00074560   |   0.00213624   |   0.00489211   |
|         | ( 0.00000536) | ( 0.00000447) | ( 0.00000475) | ( 0.00000795) | ( 0.00002044) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+-----------------+----------------+----------------+----------------+
|         |      0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+-----------------+----------------+----------------+----------------+
| block-5 |   0.00008389   |    0.00017739   |   0.00058286   |   0.00169179   |   0.00401868   |
|         | ( 0.00000031) | ( -0.00000009) | ( 0.00000323) | ( 0.00000298) | ( 0.00000529) |
+---------+----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00093721   |   0.00378025   |   0.01075099   |   0.02267263   |   0.04759722   |
|         | ( 0.00000672) | ( 0.00001945) | ( 0.00007003) | ( 0.00013293) | ( 0.00026415) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00008210   |   0.00014612   |   0.00052937   |   0.00153417   |   0.00440390   |
|         | ( 0.00000075) | ( 0.00000066) | ( 0.00000029) | ( 0.00000731) | ( 0.00001525) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00257596   |    0.00331809   |    0.00729869   |   0.01756926   |   0.04346466   |
|         | ( -0.00001560) | ( -0.00001481) | ( -0.00000187) | ( 0.00003212) | ( 0.00014657) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00056192   |   0.00070585   |   0.00244350   |   0.00629048   |   0.01549017   |
|         | ( 0.00000196) | ( 0.00000155) | ( 0.00000036) | ( 0.00001246) | ( 0.00005393) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00378400   |    0.00387296   |    0.00692396   |   0.02084498   |   0.05575567   |
|          | ( -0.00000639) | ( -0.00000689) | ( -0.00000394) | ( 0.00002807) | ( 0.00027823) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 02:22:31,401 - block_trainer.py[357] - INFO: epoch 14 (184.527237s, 55 blocks still need training), blocks loss: 
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-0 |    0.00159400   |   0.00179952   |   0.00423848   |   0.01563926   |   0.06880007   |
|         | ( -0.00000132) | ( 0.00000337) | ( 0.00003713) | ( 0.00026928) | ( 0.00077632) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00234760   |   0.00496166   |   0.01377914   |   0.03126685   |   0.07302376   |
|         | ( 0.00001165) | ( 0.00004151) | ( 0.00016558) | ( 0.00053583) | ( 0.00172137) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00104343   |   0.00147820   |   0.00380448   |   0.00874006   |   0.02464650   |
|         | ( 0.00001657) | ( 0.00001703) | ( 0.00002377) | ( 0.00003979) | ( 0.00016764) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00165921   |   0.00347205   |   0.00814748   |   0.01719505   |   0.03516863   |
|         | ( 0.00000939) | ( 0.00001367) | ( 0.00006250) | ( 0.00011871) | ( 0.00028329) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010980   |   0.00024712   |   0.00074158   |   0.00212903   |   0.00487381   |
|         | ( 0.00000053) | ( 0.00000124) | ( 0.00000402) | ( 0.00000722) | ( 0.00001830) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008527   |    0.00017873   |   0.00058114   |   0.00168640   |   0.00400862   |
|         | ( -0.00000138) | ( -0.00000135) | ( 0.00000172) | ( 0.00000538) | ( 0.00001006) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092817   |   0.00376251   |   0.01067073   |   0.02253185   |   0.04734043   |
|         | ( 0.00000904) | ( 0.00001774) | ( 0.00008027) | ( 0.00014078) | ( 0.00025679) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007961   |   0.00014384   |   0.00052652   |   0.00152667   |   0.00438553   |
|         | ( 0.00000248) | ( 0.00000228) | ( 0.00000285) | ( 0.00000750) | ( 0.00001838) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00253323   |   0.00327464   |   0.00723855   |   0.01748334   |   0.04322970   |
|         | ( 0.00004273) | ( 0.00004346) | ( 0.00006013) | ( 0.00008592) | ( 0.00023496) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00055059   |   0.00069417   |   0.00242782   |   0.00625269   |   0.01540421   |
|         | ( 0.00001132) | ( 0.00001168) | ( 0.00001568) | ( 0.00003779) | ( 0.00008595) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00374206   |   0.00383087   |   0.00687768   |   0.02077286   |   0.05550107   |
|          | ( 0.00004195) | ( 0.00004209) | ( 0.00004628) | ( 0.00007212) | ( 0.00025460) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:25:35,901 - block_trainer.py[357] - INFO: epoch 15 (184.499274s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00159096   |   0.00179482   |   0.00420143   |   0.01539484   |   0.06815177   |
|         | ( 0.00000304) | ( 0.00000471) | ( 0.00003704) | ( 0.00024442) | ( 0.00064830) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00225970   |   0.00486136   |   0.01358876   |   0.03077587   |   0.07159530   |
|         | ( 0.00008789) | ( 0.00010030) | ( 0.00019038) | ( 0.00049098) | ( 0.00142846) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00103715   |   0.00147205   |   0.00379235   |   0.00871995   |   0.02454335   |
|         | ( 0.00000627) | ( 0.00000615) | ( 0.00001213) | ( 0.00002010) | ( 0.00010316) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-3 |    0.00166099   |   0.00347077   |   0.00810709   |   0.01710004   |   0.03497009   |
|         | ( -0.00000178) | ( 0.00000129) | ( 0.00004038) | ( 0.00009501) | ( 0.00019854) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010966   |   0.00024691   |   0.00073904   |   0.00212626   |   0.00486341   |
|         | ( 0.00000014) | ( 0.00000021) | ( 0.00000254) | ( 0.00000276) | ( 0.00001039) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008185   |   0.00017548   |   0.00057627   |   0.00167997   |   0.00400017   |
|         | ( 0.00000342) | ( 0.00000326) | ( 0.00000486) | ( 0.00000644) | ( 0.00000845) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00093367   |    0.00376255   |   0.01060235   |   0.02243843   |   0.04718915   |
|         | ( -0.00000549) | ( -0.00000005) | ( 0.00006837) | ( 0.00009343) | ( 0.00015128) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007868   |   0.00014327   |   0.00052642   |   0.00152457   |   0.00437473   |
|         | ( 0.00000093) | ( 0.00000057) | ( 0.00000010) | ( 0.00000210) | ( 0.00001080) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00254781   |    0.00328844   |    0.00724436   |   0.01746859   |   0.04309510   |
|         | ( -0.00001458) | ( -0.00001380) | ( -0.00000581) | ( 0.00001475) | ( 0.00013461) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00055810   |    0.00070204   |    0.00243657   |   0.00624167   |   0.01536642   |
|         | ( -0.00000751) | ( -0.00000787) | ( -0.00000876) | ( 0.00001101) | ( 0.00003779) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00375317   |    0.00384181   |    0.00688870   |   0.02075755   |   0.05536210   |
|          | ( -0.00001112) | ( -0.00001094) | ( -0.00001102) | ( 0.00001531) | ( 0.00013897) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 02:28:40,578 - block_trainer.py[357] - INFO: epoch 16 (184.676702s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00155737   |   0.00175946   |   0.00414009   |   0.01514966   |   0.06751932   |
|         | ( 0.00003359) | ( 0.00003536) | ( 0.00006134) | ( 0.00024518) | ( 0.00063246) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-1 |    0.00227935   |   0.00485325   |   0.01348295   |   0.03040027   |   0.07041739   |
|         | ( -0.00001964) | ( 0.00000811) | ( 0.00010580) | ( 0.00037560) | ( 0.00117792) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00103530   |   0.00147034   |   0.00378482   |   0.00869780   |   0.02446011   |
|         | ( 0.00000185) | ( 0.00000171) | ( 0.00000753) | ( 0.00002216) | ( 0.00008324) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00165845   |   0.00346285   |   0.00806670   |   0.01700960   |   0.03476905   |
|         | ( 0.00000253) | ( 0.00000792) | ( 0.00004040) | ( 0.00009044) | ( 0.00020104) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-4 |    0.00011017   |    0.00024810   |   0.00073890   |   0.00212435   |   0.00485284   |
|         | ( -0.00000051) | ( -0.00000119) | ( 0.00000013) | ( 0.00000191) | ( 0.00001058) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-5 |    0.00008190   |   0.00017539   |   0.00057468   |   0.00167714   |   0.00399467   |
|         | ( -0.00000005) | ( 0.00000008) | ( 0.00000159) | ( 0.00000283) | ( 0.00000550) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00094005   |   0.00376248   |   0.01054603   |   0.02233519   |   0.04703490   |
|         | ( -0.00000639) | ( 0.00000007) | ( 0.00005632) | ( 0.00010323) | ( 0.00015425) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-7 |    0.00007918   |    0.00014344   |   0.00052474   |   0.00152166   |   0.00436422   |
|         | ( -0.00000050) | ( -0.00000018) | ( 0.00000168) | ( 0.00000291) | ( 0.00001051) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00255977   |    0.00329994   |   0.00724197   |   0.01745204   |   0.04298023   |
|         | ( -0.00001196) | ( -0.00001150) | ( 0.00000239) | ( 0.00001655) | ( 0.00011487) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00055594   |   0.00069930   |   0.00242801   |   0.00621855   |   0.01529968   |
|         | ( 0.00000216) | ( 0.00000274) | ( 0.00000856) | ( 0.00002313) | ( 0.00006675) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00379477   |    0.00388568   |    0.00693547   |    0.02077490   |   0.05522279   |
|          | ( -0.00004159) | ( -0.00004387) | ( -0.00004678) | ( -0.00001735) | ( 0.00013931) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 02:31:47,226 - block_trainer.py[357] - INFO: epoch 17 (186.647749s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00158525   |    0.00178725   |   0.00413089   |   0.01499081   |   0.06703665   |
|         | ( -0.00002787) | ( -0.00002780) | ( 0.00000920) | ( 0.00015885) | ( 0.00048267) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-1 |    0.00229371   |   0.00485088   |   0.01341210   |   0.03010947   |   0.06945426   |
|         | ( -0.00001436) | ( 0.00000237) | ( 0.00007086) | ( 0.00029080) | ( 0.00096312) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00103090   |   0.00146508   |   0.00377710   |   0.00867063   |   0.02438621   |
|         | ( 0.00000440) | ( 0.00000525) | ( 0.00000772) | ( 0.00002717) | ( 0.00007390) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00165330   |   0.00344397   |   0.00802452   |   0.01689668   |   0.03458232   |
|         | ( 0.00000515) | ( 0.00001888) | ( 0.00004217) | ( 0.00011292) | ( 0.00018673) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-4 |    0.00011170   |    0.00024846   |   0.00073698   |   0.00211991   |   0.00484039   |
|         | ( -0.00000153) | ( -0.00000036) | ( 0.00000193) | ( 0.00000445) | ( 0.00001245) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008359   |    0.00017637   |   0.00057344   |   0.00167468   |   0.00398784   |
|         | ( -0.00000169) | ( -0.00000097) | ( 0.00000124) | ( 0.00000246) | ( 0.00000683) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00093665   |   0.00374930   |   0.01049473   |   0.02223081   |   0.04689423   |
|         | ( 0.00000341) | ( 0.00001318) | ( 0.00005131) | ( 0.00010438) | ( 0.00014067) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-7 |    0.00008130   |    0.00014542   |    0.00052496   |   0.00152106   |   0.00435271   |
|         | ( -0.00000212) | ( -0.00000197) | ( -0.00000022) | ( 0.00000060) | ( 0.00001151) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00256796   |    0.00330749   |   0.00723694   |   0.01742396   |   0.04285546   |
|         | ( -0.00000819) | ( -0.00000755) | ( 0.00000503) | ( 0.00002807) | ( 0.00012477) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00055401   |   0.00069724   |   0.00242287   |   0.00620046   |   0.01523501   |
|         | ( 0.00000193) | ( 0.00000207) | ( 0.00000515) | ( 0.00001809) | ( 0.00006467) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00378634   |   0.00387659   |   0.00692071   |   0.02074193   |   0.05499849   |
|          | ( 0.00000843) | ( 0.00000909) | ( 0.00001477) | ( 0.00003297) | ( 0.00022430) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:34:52,943 - block_trainer.py[357] - INFO: epoch 18 (185.715722s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-0 |    0.00164010   |    0.00183843   |    0.00413427   |   0.01488767   |   0.06666989   |
|         | ( -0.00005486) | ( -0.00005118) | ( -0.00000337) | ( 0.00010314) | ( 0.00036676) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-1 |    0.00232443   |    0.00485517   |   0.01333924   |   0.02982411   |   0.06862385   |
|         | ( -0.00003072) | ( -0.00000429) | ( 0.00007286) | ( 0.00028536) | ( 0.00083041) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-2 |    0.00104876   |    0.00148262   |    0.00379130   |    0.00867285   |   0.02434389   |
|         | ( -0.00001786) | ( -0.00001754) | ( -0.00001420) | ( -0.00000222) | ( 0.00004232) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-3 |    0.00165995   |   0.00344371   |   0.00799745   |   0.01679949   |   0.03442663   |
|         | ( -0.00000665) | ( 0.00000026) | ( 0.00002707) | ( 0.00009720) | ( 0.00015569) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010842   |   0.00024473   |   0.00073216   |   0.00211186   |   0.00482646   |
|         | ( 0.00000328) | ( 0.00000373) | ( 0.00000482) | ( 0.00000805) | ( 0.00001393) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008109   |   0.00017385   |   0.00057041   |   0.00167015   |   0.00397886   |
|         | ( 0.00000250) | ( 0.00000251) | ( 0.00000303) | ( 0.00000453) | ( 0.00000898) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092976   |   0.00373480   |   0.01044617   |   0.02212206   |   0.04674765   |
|         | ( 0.00000689) | ( 0.00001451) | ( 0.00004856) | ( 0.00010875) | ( 0.00014658) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007689   |   0.00014084   |   0.00052093   |   0.00151603   |   0.00433691   |
|         | ( 0.00000442) | ( 0.00000457) | ( 0.00000403) | ( 0.00000502) | ( 0.00001580) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00256466   |   0.00330495   |   0.00723295   |   0.01740109   |   0.04275195   |
|         | ( 0.00000330) | ( 0.00000254) | ( 0.00000399) | ( 0.00002287) | ( 0.00010351) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00055457   |    0.00069809   |    0.00242498   |   0.00619235   |   0.01520595   |
|         | ( -0.00000056) | ( -0.00000085) | ( -0.00000211) | ( 0.00000811) | ( 0.00002906) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00375635   |   0.00384738   |   0.00689145   |   0.02068884   |   0.05487707   |
|          | ( 0.00002999) | ( 0.00002921) | ( 0.00002925) | ( 0.00005308) | ( 0.00012142) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:37:58,768 - block_trainer.py[357] - INFO: epoch 19 (185.824835s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00152412   |   0.00172280   |   0.00398209   |   0.01464748   |   0.06617511   |
|         | ( 0.00011598) | ( 0.00011563) | ( 0.00015218) | ( 0.00024019) | ( 0.00049478) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00228302   |   0.00479549   |   0.01322115   |   0.02951064   |   0.06787615   |
|         | ( 0.00004142) | ( 0.00005968) | ( 0.00011809) | ( 0.00031347) | ( 0.00074770) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102102   |   0.00145541   |   0.00376257   |   0.00862995   |   0.02426565   |
|         | ( 0.00002775) | ( 0.00002721) | ( 0.00002873) | ( 0.00004290) | ( 0.00007824) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00163329   |   0.00341208   |   0.00794417   |   0.01668985   |   0.03426138   |
|         | ( 0.00002666) | ( 0.00003163) | ( 0.00005329) | ( 0.00010964) | ( 0.00016526) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010764   |   0.00024443   |   0.00073186   |   0.00210992   |   0.00482044   |
|         | ( 0.00000078) | ( 0.00000030) | ( 0.00000030) | ( 0.00000193) | ( 0.00000602) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008040   |   0.00017357   |   0.00057025   |   0.00166888   |   0.00397361   |
|         | ( 0.00000069) | ( 0.00000028) | ( 0.00000016) | ( 0.00000127) | ( 0.00000525) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092289   |   0.00371918   |   0.01040403   |   0.02203742   |   0.04665409   |
|         | ( 0.00000687) | ( 0.00001562) | ( 0.00004214) | ( 0.00008464) | ( 0.00009355) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-7 |    0.00007835   |    0.00014200   |    0.00052144   |   0.00151429   |   0.00432876   |
|         | ( -0.00000146) | ( -0.00000115) | ( -0.00000051) | ( 0.00000174) | ( 0.00000816) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00251411   |   0.00325425   |   0.00717780   |   0.01732583   |   0.04259409   |
|         | ( 0.00005055) | ( 0.00005070) | ( 0.00005516) | ( 0.00007526) | ( 0.00015786) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00055015   |   0.00069313   |   0.00241549   |   0.00617541   |   0.01517126   |
|         | ( 0.00000442) | ( 0.00000496) | ( 0.00000949) | ( 0.00001694) | ( 0.00003469) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00371088   |   0.00380172   |   0.00685208   |   0.02063619   |   0.05472648   |
|          | ( 0.00004547) | ( 0.00004567) | ( 0.00003938) | ( 0.00005265) | ( 0.00015059) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:41:03,847 - block_trainer.py[357] - INFO: epoch 20 (185.079020s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00153164   |    0.00172686   |   0.00394412   |   0.01453642   |   0.06586389   |
|         | ( -0.00000752) | ( -0.00000406) | ( 0.00003796) | ( 0.00011105) | ( 0.00031122) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00226892   |   0.00475023   |   0.01310715   |   0.02924056   |   0.06723548   |
|         | ( 0.00001409) | ( 0.00004527) | ( 0.00011400) | ( 0.00027008) | ( 0.00064066) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00101834   |   0.00145196   |   0.00375690   |   0.00861398   |   0.02421391   |
|         | ( 0.00000268) | ( 0.00000345) | ( 0.00000567) | ( 0.00001597) | ( 0.00005174) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00161888   |   0.00338828   |   0.00790393   |   0.01660401   |   0.03411886   |
|         | ( 0.00001441) | ( 0.00002380) | ( 0.00004023) | ( 0.00008584) | ( 0.00014251) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00010987   |    0.00024654   |    0.00073275   |   0.00210578   |   0.00481055   |
|         | ( -0.00000223) | ( -0.00000212) | ( -0.00000090) | ( 0.00000414) | ( 0.00000989) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008080   |    0.00017401   |   0.00057022   |   0.00166591   |   0.00396383   |
|         | ( -0.00000040) | ( -0.00000044) | ( 0.00000003) | ( 0.00000298) | ( 0.00000979) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092167   |   0.00370972   |   0.01036655   |   0.02194785   |   0.04654819   |
|         | ( 0.00000122) | ( 0.00000945) | ( 0.00003748) | ( 0.00008956) | ( 0.00010591) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007782   |   0.00014136   |   0.00051963   |   0.00151026   |   0.00431848   |
|         | ( 0.00000053) | ( 0.00000064) | ( 0.00000180) | ( 0.00000403) | ( 0.00001028) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00252178   |    0.00326038   |    0.00718118   |   0.01729227   |   0.04249833   |
|         | ( -0.00000767) | ( -0.00000613) | ( -0.00000339) | ( 0.00003357) | ( 0.00009576) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00054907   |   0.00069225   |   0.00241383   |   0.00616619   |   0.01515136   |
|         | ( 0.00000109) | ( 0.00000088) | ( 0.00000166) | ( 0.00000922) | ( 0.00001990) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00371879   |    0.00381111   |    0.00685556   |   0.02062286   |   0.05459036   |
|          | ( -0.00000791) | ( -0.00000939) | ( -0.00000348) | ( 0.00001333) | ( 0.00013612) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 02:44:09,297 - block_trainer.py[357] - INFO: epoch 21 (185.449531s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-0 |    0.00157823   |    0.00177636   |    0.00396156   |   0.01449307   |   0.06562555   |
|         | ( -0.00004659) | ( -0.00004950) | ( -0.00001744) | ( 0.00004335) | ( 0.00023834) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-1 |    0.00231274   |    0.00478519   |   0.01310497   |   0.02908501   |   0.06677289   |
|         | ( -0.00004381) | ( -0.00003496) | ( 0.00000218) | ( 0.00015555) | ( 0.00046259) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-2 |    0.00103453   |    0.00146834   |    0.00377255   |    0.00861610   |   0.02419052   |
|         | ( -0.00001619) | ( -0.00001638) | ( -0.00001564) | ( -0.00000211) | ( 0.00002339) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-3 |    0.00166406   |    0.00342780   |    0.00792763   |   0.01658365   |   0.03405860   |
|         | ( -0.00004518) | ( -0.00003952) | ( -0.00002370) | ( 0.00002036) | ( 0.00006026) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010837   |   0.00024534   |   0.00073187   |   0.00210316   |   0.00480495   |
|         | ( 0.00000150) | ( 0.00000121) | ( 0.00000088) | ( 0.00000261) | ( 0.00000560) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008125   |    0.00017447   |   0.00057002   |   0.00166302   |   0.00395610   |
|         | ( -0.00000044) | ( -0.00000047) | ( 0.00000020) | ( 0.00000289) | ( 0.00000773) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00093104   |   0.00370724   |   0.01034516   |   0.02187903   |   0.04647545   |
|         | ( -0.00000937) | ( 0.00000249) | ( 0.00002139) | ( 0.00006882) | ( 0.00007274) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00008112   |    0.00014490   |    0.00052290   |    0.00151101   |   0.00431624   |
|         | ( -0.00000329) | ( -0.00000354) | ( -0.00000327) | ( -0.00000074) | ( 0.00000223) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00253021   |    0.00326537   |    0.00718796   |   0.01727016   |   0.04240897   |
|         | ( -0.00000843) | ( -0.00000499) | ( -0.00000678) | ( 0.00002211) | ( 0.00008936) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00054664   |   0.00068331   |   0.00241095   |   0.00615487   |   0.01512383   |
|         | ( 0.00000243) | ( 0.00000893) | ( 0.00000288) | ( 0.00001132) | ( 0.00002752) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00371889   |    0.00381151   |    0.00686365   |    0.02062304   |   0.05453894   |
|          | ( -0.00000010) | ( -0.00000040) | ( -0.00000810) | ( -0.00000018) | ( 0.00005142) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 02:47:14,865 - block_trainer.py[357] - INFO: epoch 22 (185.566858s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00155916   |   0.00175584   |   0.00390597   |   0.01438819   |   0.06539146   |
|         | ( 0.00001907) | ( 0.00002052) | ( 0.00005559) | ( 0.00010488) | ( 0.00023408) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00227127   |   0.00472817   |   0.01300058   |   0.02885878   |   0.06627837   |
|         | ( 0.00004146) | ( 0.00005702) | ( 0.00010439) | ( 0.00022623) | ( 0.00049452) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102736   |   0.00146103   |   0.00376349   |   0.00859672   |   0.02414066   |
|         | ( 0.00000717) | ( 0.00000730) | ( 0.00000906) | ( 0.00001938) | ( 0.00004986) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00161526   |   0.00337218   |   0.00786071   |   0.01648057   |   0.03391543   |
|         | ( 0.00004880) | ( 0.00005562) | ( 0.00006692) | ( 0.00010308) | ( 0.00014317) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010791   |   0.00024461   |   0.00073099   |   0.00209971   |   0.00479776   |
|         | ( 0.00000046) | ( 0.00000072) | ( 0.00000089) | ( 0.00000345) | ( 0.00000719) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-5 |    0.00008227   |    0.00017524   |    0.00057043   |   0.00166242   |   0.00395066   |
|         | ( -0.00000102) | ( -0.00000077) | ( -0.00000041) | ( 0.00000060) | ( 0.00000544) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092012   |   0.00368462   |   0.01030521   |   0.02180150   |   0.04637897   |
|         | ( 0.00001092) | ( 0.00002262) | ( 0.00003995) | ( 0.00007753) | ( 0.00009647) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007897   |   0.00014289   |   0.00052010   |   0.00150726   |   0.00430776   |
|         | ( 0.00000215) | ( 0.00000201) | ( 0.00000280) | ( 0.00000374) | ( 0.00000849) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00251765   |   0.00325193   |   0.00716909   |   0.01722108   |   0.04231266   |
|         | ( 0.00001256) | ( 0.00001344) | ( 0.00001888) | ( 0.00004907) | ( 0.00009631) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00054379   |   0.00067225   |   0.00240586   |   0.00613957   |   0.01508858   |
|         | ( 0.00000285) | ( 0.00001106) | ( 0.00000509) | ( 0.00001530) | ( 0.00003526) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00370154   |   0.00379422   |   0.00684311   |   0.02056924   |   0.05437678   |
|          | ( 0.00001735) | ( 0.00001728) | ( 0.00002055) | ( 0.00005379) | ( 0.00016216) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 02:50:20,648 - block_trainer.py[357] - INFO: epoch 23 (185.782429s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-0 |    0.00163219   |    0.00182834   |    0.00394790   |   0.01438476   |   0.06523284   |
|         | ( -0.00007303) | ( -0.00007250) | ( -0.00004193) | ( 0.00000343) | ( 0.00015862) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00224048   |   0.00467301   |   0.01288971   |   0.02860049   |   0.06579495   |
|         | ( 0.00003079) | ( 0.00005515) | ( 0.00011087) | ( 0.00025829) | ( 0.00048343) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-2 |    0.00104189   |    0.00147533   |    0.00377415   |    0.00859772   |   0.02412805   |
|         | ( -0.00001453) | ( -0.00001429) | ( -0.00001066) | ( -0.00000100) | ( 0.00001261) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-3 |    0.00164065   |    0.00338861   |    0.00786083   |   0.01644112   |   0.03382379   |
|         | ( -0.00002539) | ( -0.00001643) | ( -0.00000012) | ( 0.00003945) | ( 0.00009164) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010418   |   0.00024101   |   0.00072628   |   0.00209268   |   0.00478360   |
|         | ( 0.00000373) | ( 0.00000360) | ( 0.00000471) | ( 0.00000703) | ( 0.00001416) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007962   |   0.00017264   |   0.00056683   |   0.00165591   |   0.00393887   |
|         | ( 0.00000265) | ( 0.00000260) | ( 0.00000360) | ( 0.00000651) | ( 0.00001179) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091403   |   0.00367294   |   0.01026577   |   0.02171825   |   0.04625992   |
|         | ( 0.00000609) | ( 0.00001167) | ( 0.00003944) | ( 0.00008325) | ( 0.00011905) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007738   |   0.00014118   |   0.00051705   |   0.00150264   |   0.00429634   |
|         | ( 0.00000159) | ( 0.00000171) | ( 0.00000305) | ( 0.00000462) | ( 0.00001142) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00252172   |    0.00325555   |   0.00716682   |   0.01719968   |   0.04219515   |
|         | ( -0.00000407) | ( -0.00000362) | ( 0.00000226) | ( 0.00002141) | ( 0.00011751) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00053903   |   0.00066728   |   0.00239779   |   0.00612172   |   0.01505146   |
|         | ( 0.00000476) | ( 0.00000498) | ( 0.00000807) | ( 0.00001785) | ( 0.00003711) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+----------------+----------------+----------------+
|          |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+----------------+----------------+----------------+
| block-10 |    0.00371975   |    0.00381286   |   0.00683276   |   0.02056000   |   0.05425254   |
|          | ( -0.00001820) | ( -0.00001864) | ( 0.00001035) | ( 0.00000924) | ( 0.00012424) |
+----------+-----------------+-----------------+----------------+----------------+----------------+

2024-10-17 02:53:24,362 - block_trainer.py[357] - INFO: epoch 24 (183.713495s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00161415   |   0.00180722   |   0.00389765   |   0.01429222   |   0.06501709   |
|         | ( 0.00001803) | ( 0.00002112) | ( 0.00005025) | ( 0.00009254) | ( 0.00021576) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00234155   |    0.00475435   |    0.01292799   |   0.02851795   |   0.06552922   |
|         | ( -0.00010107) | ( -0.00008133) | ( -0.00003828) | ( 0.00008254) | ( 0.00026573) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-2 |    0.00105419   |    0.00148687   |    0.00377671   |    0.00860383   |   0.02411418   |
|         | ( -0.00001230) | ( -0.00001154) | ( -0.00000256) | ( -0.00000611) | ( 0.00001386) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-3 |    0.00166135   |    0.00340236   |    0.00786505   |   0.01641964   |   0.03376130   |
|         | ( -0.00002070) | ( -0.00001376) | ( -0.00000422) | ( 0.00002148) | ( 0.00006249) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-4 |    0.00011029   |    0.00024725   |    0.00073244   |    0.00209662   |    0.00478439   |
|         | ( -0.00000611) | ( -0.00000624) | ( -0.00000616) | ( -0.00000394) | ( -0.00000079) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00008093   |    0.00017412   |    0.00056868   |    0.00165759   |    0.00393941   |
|         | ( -0.00000131) | ( -0.00000148) | ( -0.00000185) | ( -0.00000168) | ( -0.00000054) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-6 |    0.00093437   |    0.00369014   |    0.01026749   |   0.02168480   |   0.04623618   |
|         | ( -0.00002035) | ( -0.00001719) | ( -0.00000172) | ( 0.00003345) | ( 0.00002374) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007818   |    0.00014211   |    0.00051796   |    0.00150406   |   0.00429604   |
|         | ( -0.00000080) | ( -0.00000093) | ( -0.00000091) | ( -0.00000141) | ( 0.00000029) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-8 |    0.00254450   |    0.00327816   |    0.00718892   |    0.01721147   |   0.04217614   |
|         | ( -0.00002278) | ( -0.00002261) | ( -0.00002209) | ( -0.00001179) | ( 0.00001901) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-9 |    0.00055244   |    0.00068097   |    0.00241154   |    0.00613255   |    0.01505325   |
|         | ( -0.00001341) | ( -0.00001369) | ( -0.00001375) | ( -0.00001083) | ( -0.00000179) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00374338   |    0.00383814   |    0.00684436   |    0.02058091   |    0.05427361   |
|          | ( -0.00002363) | ( -0.00002528) | ( -0.00001159) | ( -0.00002091) | ( -0.00002107) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 02:56:26,762 - block_trainer.py[357] - INFO: epoch 25 (182.399862s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00156959   |   0.00176288   |   0.00383124   |   0.01419157   |   0.06480556   |
|         | ( 0.00004456) | ( 0.00004434) | ( 0.00006641) | ( 0.00010066) | ( 0.00021153) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00234060   |   0.00473637   |   0.01286993   |   0.02834184   |   0.06516198   |
|         | ( 0.00000095) | ( 0.00001798) | ( 0.00005806) | ( 0.00017611) | ( 0.00036724) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00104907   |   0.00148211   |   0.00376557   |   0.00859389   |   0.02408722   |
|         | ( 0.00000512) | ( 0.00000476) | ( 0.00001114) | ( 0.00000995) | ( 0.00002696) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-3 |    0.00166480   |   0.00340116   |   0.00785330   |   0.01637429   |   0.03369362   |
|         | ( -0.00000346) | ( 0.00000120) | ( 0.00001175) | ( 0.00004535) | ( 0.00006768) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+-----------------+----------------+----------------+----------------+
|         |      0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+-----------------+----------------+----------------+----------------+
| block-4 |   0.00011021   |    0.00024776   |   0.00073229   |   0.00209444   |   0.00478027   |
|         | ( 0.00000008) | ( -0.00000051) | ( 0.00000014) | ( 0.00000218) | ( 0.00000412) |
+---------+----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00008133   |    0.00017442   |   0.00056844   |   0.00165113   |   0.00393535   |
|         | ( -0.00000040) | ( -0.00000030) | ( 0.00000024) | ( 0.00000647) | ( 0.00000407) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00093790   |   0.00368884   |   0.01024457   |   0.02163563   |   0.04616093   |
|         | ( -0.00000352) | ( 0.00000130) | ( 0.00002292) | ( 0.00004917) | ( 0.00007524) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007728   |   0.00014113   |   0.00051649   |   0.00150234   |   0.00428899   |
|         | ( 0.00000090) | ( 0.00000098) | ( 0.00000147) | ( 0.00000172) | ( 0.00000705) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00255699   |    0.00329141   |    0.00719468   |   0.01719454   |   0.04209643   |
|         | ( -0.00001249) | ( -0.00001325) | ( -0.00000576) | ( 0.00001693) | ( 0.00007971) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00054724   |   0.00067599   |   0.00240636   |   0.00612292   |   0.01502678   |
|         | ( 0.00000520) | ( 0.00000498) | ( 0.00000518) | ( 0.00000963) | ( 0.00002647) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00374694   |    0.00384123   |    0.00684757   |   0.02057486   |   0.05419942   |
|          | ( -0.00000356) | ( -0.00000309) | ( -0.00000321) | ( 0.00000605) | ( 0.00007419) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 02:59:30,602 - block_trainer.py[357] - INFO: epoch 26 (183.839657s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00156759   |   0.00175996   |   0.00380410   |   0.01412802   |   0.06462945   |
|         | ( 0.00000200) | ( 0.00000292) | ( 0.00002714) | ( 0.00006355) | ( 0.00017611) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-1 |    0.00234762   |   0.00472739   |   0.01282519   |   0.02819931   |   0.06492009   |
|         | ( -0.00000703) | ( 0.00000899) | ( 0.00004474) | ( 0.00014252) | ( 0.00024189) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-2 |    0.00105400   |    0.00148720   |    0.00376978   |    0.00859631   |   0.02408351   |
|         | ( -0.00000493) | ( -0.00000509) | ( -0.00000420) | ( -0.00000243) | ( 0.00000371) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-3 |    0.00167341   |    0.00340795   |   0.00784906   |   0.01634529   |   0.03364412   |
|         | ( -0.00000861) | ( -0.00000679) | ( 0.00000424) | ( 0.00002900) | ( 0.00004950) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-4 |    0.00011091   |   0.00024721   |   0.00073065   |   0.00209195   |   0.00477456   |
|         | ( -0.00000070) | ( 0.00000055) | ( 0.00000165) | ( 0.00000249) | ( 0.00000571) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-5 |    0.00008366   |    0.00017650   |    0.00056966   |   0.00164991   |   0.00393248   |
|         | ( -0.00000233) | ( -0.00000208) | ( -0.00000122) | ( 0.00000122) | ( 0.00000286) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00094130   |   0.00368773   |   0.01022716   |   0.02158157   |   0.04610179   |
|         | ( -0.00000340) | ( 0.00000111) | ( 0.00001742) | ( 0.00005406) | ( 0.00005914) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-7 |    0.00007806   |    0.00014172   |   0.00051610   |   0.00150088   |   0.00428277   |
|         | ( -0.00000079) | ( -0.00000058) | ( 0.00000038) | ( 0.00000146) | ( 0.00000622) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00256626   |    0.00329779   |    0.00719791   |   0.01718597   |   0.04204369   |
|         | ( -0.00000928) | ( -0.00000638) | ( -0.00000323) | ( 0.00000857) | ( 0.00005274) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-9 |    0.00055289   |    0.00068086   |   0.00240186   |   0.00611676   |   0.01501263   |
|         | ( -0.00000565) | ( -0.00000488) | ( 0.00000450) | ( 0.00000617) | ( 0.00001415) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00375874   |    0.00385253   |    0.00685888   |   0.02056772   |   0.05409791   |
|          | ( -0.00001180) | ( -0.00001129) | ( -0.00001131) | ( 0.00000714) | ( 0.00010151) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 03:02:34,674 - block_trainer.py[357] - INFO: epoch 27 (184.071211s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00153089   |   0.00172217   |   0.00374823   |   0.01405264   |   0.06444050   |
|         | ( 0.00003669) | ( 0.00003779) | ( 0.00005587) | ( 0.00007538) | ( 0.00018895) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00221473   |   0.00457808   |   0.01265144   |   0.02792028   |   0.06449660   |
|         | ( 0.00013289) | ( 0.00014930) | ( 0.00017375) | ( 0.00027904) | ( 0.00042349) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00100941   |   0.00144231   |   0.00372221   |   0.00854559   |   0.02402065   |
|         | ( 0.00004459) | ( 0.00004489) | ( 0.00004757) | ( 0.00005073) | ( 0.00006286) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00160310   |   0.00333341   |   0.00776330   |   0.01623672   |   0.03350208   |
|         | ( 0.00007031) | ( 0.00007454) | ( 0.00008576) | ( 0.00010857) | ( 0.00014204) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010950   |   0.00024636   |   0.00072911   |   0.00208831   |   0.00476888   |
|         | ( 0.00000140) | ( 0.00000085) | ( 0.00000153) | ( 0.00000364) | ( 0.00000568) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007805   |   0.00017112   |   0.00056505   |   0.00164497   |   0.00392685   |
|         | ( 0.00000561) | ( 0.00000538) | ( 0.00000461) | ( 0.00000495) | ( 0.00000564) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091360   |   0.00365532   |   0.01018372   |   0.02151143   |   0.04601917   |
|         | ( 0.00002769) | ( 0.00003241) | ( 0.00004344) | ( 0.00007014) | ( 0.00008262) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007760   |   0.00014125   |   0.00051536   |   0.00149954   |   0.00427683   |
|         | ( 0.00000047) | ( 0.00000047) | ( 0.00000074) | ( 0.00000134) | ( 0.00000594) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00250220   |   0.00323559   |   0.00713632   |   0.01710997   |   0.04192348   |
|         | ( 0.00006406) | ( 0.00006220) | ( 0.00006159) | ( 0.00007600) | ( 0.00012021) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00053617   |   0.00066488   |   0.00238196   |   0.00610213   |   0.01498371   |
|         | ( 0.00001672) | ( 0.00001599) | ( 0.00001991) | ( 0.00001463) | ( 0.00002893) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00364945   |   0.00374584   |   0.00675274   |   0.02045202   |   0.05396763   |
|          | ( 0.00010929) | ( 0.00010669) | ( 0.00010614) | ( 0.00011570) | ( 0.00013028) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:05:38,485 - block_trainer.py[357] - INFO: epoch 28 (183.810668s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00158296   |    0.00177309   |    0.00378358   |    0.01406726   |   0.06439474   |
|         | ( -0.00005207) | ( -0.00005092) | ( -0.00003535) | ( -0.00001462) | ( 0.00004577) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00235692   |    0.00470555   |    0.01273986   |   0.02791870   |   0.06440890   |
|         | ( -0.00014220) | ( -0.00012747) | ( -0.00008842) | ( 0.00000158) | ( 0.00008770) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00104442   |    0.00147703   |    0.00375636   |    0.00857375   |    0.02403563   |
|         | ( -0.00003501) | ( -0.00003472) | ( -0.00003414) | ( -0.00002816) | ( -0.00001498) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-3 |    0.00166950   |    0.00339764   |    0.00781776   |    0.01625642   |   0.03350050   |
|         | ( -0.00006639) | ( -0.00006423) | ( -0.00005446) | ( -0.00001970) | ( 0.00000157) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+----------------+-----------------+----------------+----------------+
|         |       0.0       |      0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+-----------------+----------------+----------------+
| block-4 |    0.00010955   |   0.00024626   |    0.00072940   |   0.00208783   |   0.00476362   |
|         | ( -0.00000005) | ( 0.00000010) | ( -0.00000029) | ( 0.00000048) | ( 0.00000526) |
+---------+-----------------+----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-5 |    0.00008255   |    0.00017535   |    0.00056870   |    0.00164605   |   0.00392525   |
|         | ( -0.00000450) | ( -0.00000423) | ( -0.00000365) | ( -0.00000109) | ( 0.00000160) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00093649   |    0.00367430   |   0.01018180   |   0.02146955   |   0.04596109   |
|         | ( -0.00002289) | ( -0.00001898) | ( 0.00000192) | ( 0.00004188) | ( 0.00005809) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007847   |    0.00014235   |    0.00051608   |    0.00149983   |   0.00427294   |
|         | ( -0.00000088) | ( -0.00000110) | ( -0.00000072) | ( -0.00000030) | ( 0.00000390) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-8 |    0.00254652   |    0.00327908   |    0.00717439   |    0.01713491   |   0.04189105   |
|         | ( -0.00004432) | ( -0.00004349) | ( -0.00003807) | ( -0.00002494) | ( 0.00003243) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-9 |    0.00054614   |    0.00067419   |    0.00238546   |    0.00610256   |   0.01497292   |
|         | ( -0.00000997) | ( -0.00000932) | ( -0.00000350) | ( -0.00000044) | ( 0.00001078) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+----------+-----------------+-----------------+-----------------+----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+----------------+----------------+
| block-10 |    0.00368171   |    0.00377573   |    0.00678375   |   0.02043346   |   0.05389828   |
|          | ( -0.00003226) | ( -0.00002989) | ( -0.00003101) | ( 0.00001856) | ( 0.00006935) |
+----------+-----------------+-----------------+-----------------+----------------+----------------+

2024-10-17 03:08:42,552 - block_trainer.py[357] - INFO: epoch 29 (184.066282s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00159311   |    0.00178212   |   0.00377480   |   0.01404208   |   0.06428480   |
|         | ( -0.00001014) | ( -0.00000903) | ( 0.00000878) | ( 0.00002518) | ( 0.00010993) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00231686   |   0.00465058   |   0.01266572   |   0.02777487   |   0.06418507   |
|         | ( 0.00004007) | ( 0.00005497) | ( 0.00007414) | ( 0.00014383) | ( 0.00022384) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+-----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |       0.6       |       0.8       |
+---------+----------------+----------------+----------------+-----------------+-----------------+
| block-2 |   0.00104253   |   0.00147490   |   0.00375448   |    0.00857413   |    0.02403867   |
|         | ( 0.00000189) | ( 0.00000213) | ( 0.00000188) | ( -0.00000038) | ( -0.00000304) |
+---------+----------------+----------------+----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00166516   |   0.00339193   |   0.00780367   |   0.01622081   |   0.03345883   |
|         | ( 0.00000434) | ( 0.00000571) | ( 0.00001409) | ( 0.00003561) | ( 0.00004167) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+-----------------+----------------+----------------+
|         |       0.0       |      0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+-----------------+----------------+----------------+
| block-4 |    0.00011069   |   0.00024361   |    0.00073004   |   0.00208651   |   0.00476145   |
|         | ( -0.00000114) | ( 0.00000265) | ( -0.00000064) | ( 0.00000132) | ( 0.00000217) |
+---------+-----------------+----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+-----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |       0.6       |       0.8       |
+---------+----------------+----------------+----------------+-----------------+-----------------+
| block-5 |   0.00008104   |   0.00017421   |   0.00056826   |    0.00164648   |    0.00392619   |
|         | ( 0.00000151) | ( 0.00000114) | ( 0.00000044) | ( -0.00000042) | ( -0.00000095) |
+---------+----------------+----------------+----------------+-----------------+-----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00093952   |    0.00367686   |   0.01017034   |   0.02143628   |   0.04593838   |
|         | ( -0.00000302) | ( -0.00000256) | ( 0.00001146) | ( 0.00003327) | ( 0.00002270) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007885   |    0.00014285   |    0.00051691   |    0.00150133   |   0.00427291   |
|         | ( -0.00000038) | ( -0.00000049) | ( -0.00000083) | ( -0.00000150) | ( 0.00000002) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00254981   |    0.00328248   |    0.00717733   |   0.01713376   |   0.04187942   |
|         | ( -0.00000329) | ( -0.00000340) | ( -0.00000294) | ( 0.00000115) | ( 0.00001163) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+-----------------+----------------+----------------+-----------------+
|         |      0.0       |       0.2       |      0.4       |      0.6       |       0.8       |
+---------+----------------+-----------------+----------------+----------------+-----------------+
| block-9 |   0.00054568   |    0.00067438   |   0.00238487   |   0.00610090   |    0.01497991   |
|         | ( 0.00000046) | ( -0.00000018) | ( 0.00000059) | ( 0.00000167) | ( -0.00000699) |
+---------+----------------+-----------------+----------------+----------------+-----------------+
+----------+-----------------+-----------------+-----------------+----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+----------------+-----------------+
| block-10 |    0.00369758   |    0.00379068   |    0.00680551   |   0.02042809   |    0.05394940   |
|          | ( -0.00001586) | ( -0.00001495) | ( -0.00002176) | ( 0.00000537) | ( -0.00005113) |
+----------+-----------------+-----------------+-----------------+----------------+-----------------+

2024-10-17 03:11:46,626 - block_trainer.py[357] - INFO: epoch 30 (184.073282s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00152987   |   0.00171966   |   0.00370190   |   0.01394785   |   0.06407709   |
|         | ( 0.00006323) | ( 0.00006246) | ( 0.00007290) | ( 0.00009424) | ( 0.00020771) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00218443   |   0.00450785   |   0.01248126   |   0.02748413   |   0.06378477   |
|         | ( 0.00013243) | ( 0.00014273) | ( 0.00018446) | ( 0.00029074) | ( 0.00040029) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00098486   |   0.00141709   |   0.00369510   |   0.00850886   |   0.02395540   |
|         | ( 0.00005767) | ( 0.00005781) | ( 0.00005938) | ( 0.00006527) | ( 0.00008327) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00160439   |   0.00332890   |   0.00773031   |   0.01611619   |   0.03332403   |
|         | ( 0.00006077) | ( 0.00006303) | ( 0.00007336) | ( 0.00010462) | ( 0.00013480) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010332   |   0.00023466   |   0.00072279   |   0.00207834   |   0.00475204   |
|         | ( 0.00000738) | ( 0.00000895) | ( 0.00000725) | ( 0.00000816) | ( 0.00000941) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007810   |   0.00017120   |   0.00056487   |   0.00164102   |   0.00391834   |
|         | ( 0.00000294) | ( 0.00000301) | ( 0.00000339) | ( 0.00000546) | ( 0.00000785) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00090943   |   0.00364184   |   0.01011822   |   0.02134110   |   0.04582793   |
|         | ( 0.00003009) | ( 0.00003502) | ( 0.00005212) | ( 0.00009519) | ( 0.00011046) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007609   |   0.00013989   |   0.00051332   |   0.00149680   |   0.00426072   |
|         | ( 0.00000276) | ( 0.00000296) | ( 0.00000359) | ( 0.00000454) | ( 0.00001219) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00250915   |   0.00324179   |   0.00713694   |   0.01707618   |   0.04175443   |
|         | ( 0.00004066) | ( 0.00004069) | ( 0.00004039) | ( 0.00005758) | ( 0.00012499) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00053110   |   0.00065970   |   0.00236940   |   0.00608522   |   0.01493664   |
|         | ( 0.00001458) | ( 0.00001467) | ( 0.00001547) | ( 0.00001568) | ( 0.00004328) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00367590   |   0.00376925   |   0.00677680   |   0.02037884   |   0.05377086   |
|          | ( 0.00002167) | ( 0.00002143) | ( 0.00002872) | ( 0.00004925) | ( 0.00017855) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:14:50,739 - block_trainer.py[357] - INFO: epoch 31 (184.113126s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-0 |    0.00153545   |    0.00172517   |   0.00369419   |   0.01392628   |   0.06399427   |
|         | ( -0.00000557) | ( -0.00000551) | ( 0.00000770) | ( 0.00002157) | ( 0.00008282) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-1 |    0.00228398   |    0.00460272   |    0.01256258   |    0.02749947   |   0.06374801   |
|         | ( -0.00009955) | ( -0.00009487) | ( -0.00008131) | ( -0.00001534) | ( 0.00003677) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00103385   |    0.00146636   |    0.00374414   |    0.00855563   |    0.02400156   |
|         | ( -0.00004899) | ( -0.00004927) | ( -0.00004904) | ( -0.00004677) | ( -0.00004616) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00165093   |    0.00337651   |    0.00776995   |    0.01613963   |    0.03333020   |
|         | ( -0.00004654) | ( -0.00004761) | ( -0.00003964) | ( -0.00002344) | ( -0.00000617) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00010730   |    0.00023739   |    0.00072460   |   0.00207745   |   0.00475061   |
|         | ( -0.00000399) | ( -0.00000273) | ( -0.00000181) | ( 0.00000089) | ( 0.00000143) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00008108   |    0.00017412   |    0.00056735   |    0.00164257   |    0.00392067   |
|         | ( -0.00000298) | ( -0.00000292) | ( -0.00000247) | ( -0.00000155) | ( -0.00000232) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-6 |    0.00093311   |    0.00366446   |    0.01012946   |   0.02133005   |   0.04581257   |
|         | ( -0.00002368) | ( -0.00002262) | ( -0.00001124) | ( 0.00001105) | ( 0.00001536) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-7 |    0.00007809   |    0.00014156   |    0.00051442   |    0.00149826   |    0.00426140   |
|         | ( -0.00000200) | ( -0.00000167) | ( -0.00000111) | ( -0.00000146) | ( -0.00000068) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-8 |    0.00255375   |    0.00328628   |    0.00717963   |    0.01711411   |   0.04175177   |
|         | ( -0.00004460) | ( -0.00004449) | ( -0.00004269) | ( -0.00003793) | ( 0.00000266) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-9 |    0.00053817   |    0.00066578   |   0.00236919   |   0.00607727   |   0.01492392   |
|         | ( -0.00000707) | ( -0.00000608) | ( 0.00000021) | ( 0.00000794) | ( 0.00001271) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00373225   |    0.00382595   |    0.00683902   |    0.02043684   |    0.05379007   |
|          | ( -0.00005635) | ( -0.00005670) | ( -0.00006223) | ( -0.00005799) | ( -0.00001922) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 03:17:54,814 - block_trainer.py[357] - INFO: epoch 32 (184.074363s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00151968   |   0.00171008   |   0.00367139   |   0.01388219   |   0.06384064   |
|         | ( 0.00001576) | ( 0.00001509) | ( 0.00002280) | ( 0.00004408) | ( 0.00015363) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00222040   |   0.00453314   |   0.01246222   |   0.02731875   |   0.06349983   |
|         | ( 0.00006358) | ( 0.00006958) | ( 0.00010036) | ( 0.00018073) | ( 0.00024818) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00101007   |   0.00144256   |   0.00371973   |   0.00852964   |   0.02396270   |
|         | ( 0.00002378) | ( 0.00002380) | ( 0.00002440) | ( 0.00002598) | ( 0.00003886) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00162941   |   0.00335240   |   0.00773480   |   0.01608059   |   0.03324191   |
|         | ( 0.00002152) | ( 0.00002410) | ( 0.00003515) | ( 0.00005904) | ( 0.00008829) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010615   |   0.00023663   |   0.00072452   |   0.00207479   |   0.00474469   |
|         | ( 0.00000115) | ( 0.00000076) | ( 0.00000008) | ( 0.00000266) | ( 0.00000592) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007765   |   0.00017109   |   0.00056477   |   0.00163910   |   0.00391384   |
|         | ( 0.00000343) | ( 0.00000304) | ( 0.00000258) | ( 0.00000346) | ( 0.00000682) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091854   |   0.00364724   |   0.01009490   |   0.02126670   |   0.04572416   |
|         | ( 0.00001456) | ( 0.00001722) | ( 0.00003457) | ( 0.00006334) | ( 0.00008840) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007610   |   0.00013985   |   0.00051275   |   0.00149575   |   0.00425248   |
|         | ( 0.00000198) | ( 0.00000171) | ( 0.00000167) | ( 0.00000251) | ( 0.00000892) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00252210   |   0.00325531   |   0.00714741   |   0.01707486   |   0.04168940   |
|         | ( 0.00003165) | ( 0.00003096) | ( 0.00003222) | ( 0.00003926) | ( 0.00006237) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00052744   |   0.00065627   |   0.00236351   |   0.00606726   |   0.01490773   |
|         | ( 0.00001073) | ( 0.00000951) | ( 0.00000568) | ( 0.00001001) | ( 0.00001619) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00366702   |   0.00376098   |   0.00677278   |   0.02036058   |   0.05366255   |
|          | ( 0.00006524) | ( 0.00006498) | ( 0.00006624) | ( 0.00007625) | ( 0.00012753) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:20:58,555 - block_trainer.py[357] - INFO: epoch 33 (183.740876s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00156721   |    0.00175821   |    0.00370815   |    0.01389186   |   0.06374814   |
|         | ( -0.00004752) | ( -0.00004813) | ( -0.00003676) | ( -0.00000967) | ( 0.00009250) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00232873   |    0.00462402   |    0.01252859   |   0.02731784   |   0.06339283   |
|         | ( -0.00010833) | ( -0.00009088) | ( -0.00006637) | ( 0.00000091) | ( 0.00010699) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00102534   |    0.00145753   |    0.00373475   |    0.00854027   |    0.02397439   |
|         | ( -0.00001527) | ( -0.00001497) | ( -0.00001502) | ( -0.00001063) | ( -0.00001169) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-3 |    0.00164240   |    0.00336392   |    0.00774160   |   0.01606934   |   0.03321554   |
|         | ( -0.00001299) | ( -0.00001151) | ( -0.00000680) | ( 0.00001125) | ( 0.00002636) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00010643   |    0.00023690   |    0.00072503   |   0.00207253   |   0.00474310   |
|         | ( -0.00000028) | ( -0.00000026) | ( -0.00000051) | ( 0.00000225) | ( 0.00000159) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00007870   |    0.00017213   |    0.00056582   |    0.00163937   |    0.00391504   |
|         | ( -0.00000105) | ( -0.00000105) | ( -0.00000106) | ( -0.00000027) | ( -0.00000120) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00092536   |    0.00365252   |   0.01008848   |   0.02125007   |   0.04571666   |
|         | ( -0.00000681) | ( -0.00000528) | ( 0.00000641) | ( 0.00001664) | ( 0.00000750) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007699   |    0.00014077   |    0.00051381   |    0.00149670   |   0.00425218   |
|         | ( -0.00000089) | ( -0.00000093) | ( -0.00000106) | ( -0.00000095) | ( 0.00000031) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00252707   |    0.00325943   |    0.00714798   |   0.01707062   |   0.04166054   |
|         | ( -0.00000497) | ( -0.00000412) | ( -0.00000057) | ( 0.00000424) | ( 0.00002885) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
| block-9 |    0.00053370   |    0.00066249   |    0.00236907   |   0.00606369   |    0.01491374   |
|         | ( -0.00000625) | ( -0.00000622) | ( -0.00000557) | ( 0.00000357) | ( -0.00000600) |
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00370386   |    0.00379672   |    0.00680726   |    0.02039075   |    0.05369629   |
|          | ( -0.00003684) | ( -0.00003575) | ( -0.00003449) | ( -0.00003017) | ( -0.00003375) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 03:24:03,127 - block_trainer.py[357] - INFO: epoch 34 (184.571218s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-0 |    0.00157775   |    0.00176825   |    0.00371078   |   0.01387828   |   0.06366389   |
|         | ( -0.00001054) | ( -0.00001003) | ( -0.00000263) | ( 0.00001358) | ( 0.00008424) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00239930   |    0.00468109   |    0.01255251   |   0.02728360   |   0.06330460   |
|         | ( -0.00007057) | ( -0.00005707) | ( -0.00002393) | ( 0.00003423) | ( 0.00008823) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00107315   |    0.00150498   |    0.00377998   |    0.00858132   |    0.02399825   |
|         | ( -0.00004781) | ( -0.00004745) | ( -0.00004523) | ( -0.00004105) | ( -0.00002386) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-3 |    0.00167334   |    0.00339079   |    0.00775808   |    0.01607154   |   0.03319787   |
|         | ( -0.00003093) | ( -0.00002687) | ( -0.00001648) | ( -0.00000220) | ( 0.00001767) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-4 |    0.00011101   |    0.00024106   |    0.00072973   |    0.00207579   |    0.00474687   |
|         | ( -0.00000458) | ( -0.00000416) | ( -0.00000470) | ( -0.00000325) | ( -0.00000377) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-5 |    0.00008171   |    0.00017491   |    0.00056851   |    0.00164069   |   0.00391479   |
|         | ( -0.00000301) | ( -0.00000278) | ( -0.00000269) | ( -0.00000131) | ( 0.00000025) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00094009   |    0.00366538   |   0.01008302   |   0.02121846   |   0.04566025   |
|         | ( -0.00001473) | ( -0.00001286) | ( 0.00000546) | ( 0.00003160) | ( 0.00005641) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007992   |    0.00014351   |    0.00051564   |    0.00149705   |   0.00424754   |
|         | ( -0.00000293) | ( -0.00000273) | ( -0.00000183) | ( -0.00000035) | ( 0.00000463) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00253451   |    0.00326726   |    0.00715134   |   0.01705921   |   0.04160259   |
|         | ( -0.00000744) | ( -0.00000783) | ( -0.00000335) | ( 0.00001142) | ( 0.00005795) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00053459   |    0.00066326   |    0.00236917   |   0.00605703   |   0.01489510   |
|         | ( -0.00000089) | ( -0.00000078) | ( -0.00000009) | ( 0.00000666) | ( 0.00001864) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00369211   |   0.00378624   |   0.00679602   |   0.02037273   |   0.05360775   |
|          | ( 0.00001175) | ( 0.00001048) | ( 0.00001124) | ( 0.00001802) | ( 0.00008854) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:27:07,211 - block_trainer.py[357] - INFO: epoch 35 (184.082983s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00156616   |   0.00175566   |   0.00369150   |   0.01384557   |   0.06355332   |
|         | ( 0.00001160) | ( 0.00001259) | ( 0.00001928) | ( 0.00003271) | ( 0.00011057) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00232435   |   0.00459506   |   0.01243877   |   0.02710482   |   0.06305433   |
|         | ( 0.00007495) | ( 0.00008603) | ( 0.00011374) | ( 0.00017878) | ( 0.00025027) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00104763   |   0.00147947   |   0.00375367   |   0.00855628   |   0.02396854   |
|         | ( 0.00002552) | ( 0.00002550) | ( 0.00002631) | ( 0.00002504) | ( 0.00002972) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00166466   |   0.00337737   |   0.00774004   |   0.01603921   |   0.03314764   |
|         | ( 0.00000867) | ( 0.00001342) | ( 0.00001804) | ( 0.00003233) | ( 0.00005023) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010936   |   0.00023977   |   0.00072741   |   0.00207006   |   0.00473968   |
|         | ( 0.00000165) | ( 0.00000129) | ( 0.00000231) | ( 0.00000572) | ( 0.00000719) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00008153   |   0.00017482   |   0.00056844   |   0.00163858   |   0.00391156   |
|         | ( 0.00000018) | ( 0.00000009) | ( 0.00000007) | ( 0.00000211) | ( 0.00000323) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00094403   |    0.00366831   |   0.01007390   |   0.02118863   |   0.04562752   |
|         | ( -0.00000394) | ( -0.00000292) | ( 0.00000912) | ( 0.00002983) | ( 0.00003273) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+-----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |       0.6       |      0.8       |
+---------+----------------+----------------+----------------+-----------------+----------------+
| block-7 |   0.00007815   |   0.00014207   |   0.00051508   |    0.00149758   |   0.00424433   |
|         | ( 0.00000176) | ( 0.00000143) | ( 0.00000056) | ( -0.00000053) | ( 0.00000321) |
+---------+----------------+----------------+----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00254362   |    0.00327567   |    0.00715997   |   0.01705902   |   0.04155678   |
|         | ( -0.00000912) | ( -0.00000841) | ( -0.00000863) | ( 0.00000019) | ( 0.00004581) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00053243   |   0.00066094   |   0.00236549   |   0.00604700   |   0.01488409   |
|         | ( 0.00000216) | ( 0.00000233) | ( 0.00000368) | ( 0.00001003) | ( 0.00001101) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00371644   |    0.00381094   |    0.00682003   |    0.02038253   |   0.05356995   |
|          | ( -0.00002433) | ( -0.00002470) | ( -0.00002401) | ( -0.00000980) | ( 0.00003780) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 03:30:10,864 - block_trainer.py[357] - INFO: epoch 36 (183.652977s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00151099   |   0.00169754   |   0.00362228   |   0.01376601   |   0.06338175   |
|         | ( 0.00005516) | ( 0.00005812) | ( 0.00006922) | ( 0.00007956) | ( 0.00017157) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00221646   |   0.00448134   |   0.01231598   |   0.02692905   |   0.06282042   |
|         | ( 0.00010789) | ( 0.00011372) | ( 0.00012279) | ( 0.00017578) | ( 0.00023391) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102102   |   0.00145299   |   0.00372734   |   0.00852552   |   0.02392941   |
|         | ( 0.00002661) | ( 0.00002648) | ( 0.00002633) | ( 0.00003075) | ( 0.00003913) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00163721   |   0.00334664   |   0.00770479   |   0.01599289   |   0.03307581   |
|         | ( 0.00002745) | ( 0.00003072) | ( 0.00003525) | ( 0.00004631) | ( 0.00007183) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010480   |   0.00023506   |   0.00072275   |   0.00206424   |   0.00473295   |
|         | ( 0.00000456) | ( 0.00000471) | ( 0.00000467) | ( 0.00000582) | ( 0.00000673) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007793   |   0.00017073   |   0.00056441   |   0.00163404   |   0.00390752   |
|         | ( 0.00000360) | ( 0.00000409) | ( 0.00000404) | ( 0.00000454) | ( 0.00000404) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092524   |   0.00364758   |   0.01003831   |   0.02114087   |   0.04557502   |
|         | ( 0.00001878) | ( 0.00002073) | ( 0.00003560) | ( 0.00004776) | ( 0.00005249) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007585   |   0.00013952   |   0.00051190   |   0.00149394   |   0.00423833   |
|         | ( 0.00000230) | ( 0.00000256) | ( 0.00000319) | ( 0.00000364) | ( 0.00000600) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00253064   |   0.00326308   |   0.00714732   |   0.01703695   |   0.04151083   |
|         | ( 0.00001299) | ( 0.00001259) | ( 0.00001265) | ( 0.00002207) | ( 0.00004596) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-9 |    0.00053452   |    0.00066308   |    0.00236643   |    0.00604766   |   0.01487442   |
|         | ( -0.00000209) | ( -0.00000214) | ( -0.00000094) | ( -0.00000066) | ( 0.00000967) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00369263   |   0.00378661   |   0.00679872   |   0.02036204   |   0.05353825   |
|          | ( 0.00002381) | ( 0.00002433) | ( 0.00002131) | ( 0.00002049) | ( 0.00003170) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:33:14,314 - block_trainer.py[357] - INFO: epoch 37 (183.449015s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00153525   |    0.00172182   |    0.00364057   |    0.01377707   |   0.06331000   |
|         | ( -0.00002425) | ( -0.00002428) | ( -0.00001829) | ( -0.00001106) | ( 0.00007175) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-1 |    0.00233877   |    0.00459186   |    0.01239626   |    0.02694086   |   0.06275501   |
|         | ( -0.00012231) | ( -0.00011052) | ( -0.00008028) | ( -0.00001181) | ( 0.00006541) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00104108   |    0.00147287   |    0.00374626   |    0.00854473   |    0.02394239   |
|         | ( -0.00002006) | ( -0.00001987) | ( -0.00001892) | ( -0.00001920) | ( -0.00001298) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-3 |    0.00164667   |    0.00335397   |   0.00770370   |   0.01597572   |   0.03304122   |
|         | ( -0.00000946) | ( -0.00000733) | ( 0.00000109) | ( 0.00001718) | ( 0.00003459) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-4 |    0.00010974   |    0.00023941   |    0.00072563   |    0.00206493   |   0.00473275   |
|         | ( -0.00000494) | ( -0.00000435) | ( -0.00000288) | ( -0.00000068) | ( 0.00000020) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-5 |    0.00008067   |    0.00017218   |    0.00056663   |   0.00163307   |   0.00390605   |
|         | ( -0.00000274) | ( -0.00000145) | ( -0.00000222) | ( 0.00000097) | ( 0.00000148) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00093716   |    0.00365872   |   0.01003176   |   0.02112222   |   0.04554775   |
|         | ( -0.00001191) | ( -0.00001114) | ( 0.00000654) | ( 0.00001865) | ( 0.00002727) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007774   |    0.00014144   |    0.00051388   |    0.00149552   |   0.00423529   |
|         | ( -0.00000189) | ( -0.00000192) | ( -0.00000199) | ( -0.00000159) | ( 0.00000304) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00253330   |    0.00326641   |   0.00714508   |   0.01703603   |   0.04148488   |
|         | ( -0.00000266) | ( -0.00000333) | ( 0.00000225) | ( 0.00000091) | ( 0.00002594) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00052641   |   0.00065480   |   0.00236021   |   0.00603847   |   0.01486516   |
|         | ( 0.00000811) | ( 0.00000828) | ( 0.00000622) | ( 0.00000918) | ( 0.00000925) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00367579   |   0.00377019   |   0.00678456   |   0.02032765   |   0.05344896   |
|          | ( 0.00001683) | ( 0.00001641) | ( 0.00001416) | ( 0.00003439) | ( 0.00008929) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:36:18,250 - block_trainer.py[357] - INFO: epoch 38 (183.935542s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00152068   |   0.00170834   |   0.00362536   |   0.01375022   |   0.06319953   |
|         | ( 0.00001457) | ( 0.00001349) | ( 0.00001521) | ( 0.00002685) | ( 0.00011047) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00232645   |   0.00457741   |   0.01236947   |   0.02688422   |   0.06266663   |
|         | ( 0.00001232) | ( 0.00001446) | ( 0.00002679) | ( 0.00005663) | ( 0.00008837) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00105991   |    0.00149120   |    0.00376413   |    0.00855812   |    0.02394377   |
|         | ( -0.00001882) | ( -0.00001834) | ( -0.00001786) | ( -0.00001339) | ( -0.00000138) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-3 |    0.00167766   |    0.00338330   |    0.00772340   |    0.01598289   |   0.03302847   |
|         | ( -0.00003099) | ( -0.00002933) | ( -0.00001970) | ( -0.00000717) | ( 0.00001275) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+-----------------+----------------+----------------+----------------+
|         |      0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+-----------------+----------------+----------------+----------------+
| block-4 |   0.00010965   |    0.00023955   |   0.00072475   |   0.00206397   |   0.00472975   |
|         | ( 0.00000010) | ( -0.00000014) | ( 0.00000088) | ( 0.00000096) | ( 0.00000300) |
+---------+----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |       0.8       |
+---------+----------------+----------------+----------------+----------------+-----------------+
| block-5 |   0.00008007   |   0.00017098   |   0.00056630   |   0.00163223   |    0.00390628   |
|         | ( 0.00000060) | ( 0.00000119) | ( 0.00000033) | ( 0.00000084) | ( -0.00000024) |
+---------+----------------+----------------+----------------+----------------+-----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00094745   |    0.00366632   |   0.01001861   |   0.02110549   |   0.04551629   |
|         | ( -0.00001029) | ( -0.00000760) | ( 0.00001315) | ( 0.00001673) | ( 0.00003146) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-7 |    0.00007837   |    0.00014208   |    0.00051392   |   0.00149496   |   0.00423267   |
|         | ( -0.00000063) | ( -0.00000064) | ( -0.00000004) | ( 0.00000056) | ( 0.00000262) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00254652   |    0.00327985   |    0.00715348   |   0.01703055   |   0.04144208   |
|         | ( -0.00001322) | ( -0.00001344) | ( -0.00000840) | ( 0.00000548) | ( 0.00004280) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00053187   |    0.00066025   |    0.00236173   |   0.00603455   |   0.01485084   |
|         | ( -0.00000546) | ( -0.00000545) | ( -0.00000152) | ( 0.00000393) | ( 0.00001432) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00370422   |    0.00380015   |    0.00681167   |    0.02037103   |   0.05343991   |
|          | ( -0.00002843) | ( -0.00002996) | ( -0.00002711) | ( -0.00004338) | ( 0.00000905) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 03:39:22,762 - block_trainer.py[357] - INFO: epoch 39 (184.511636s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00158542   |    0.00177326   |    0.00367770   |    0.01379236   |   0.06317941   |
|         | ( -0.00006475) | ( -0.00006493) | ( -0.00005234) | ( -0.00004214) | ( 0.00002012) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00232594   |   0.00456480   |   0.01232834   |   0.02678711   |   0.06253034   |
|         | ( 0.00000051) | ( 0.00001260) | ( 0.00004113) | ( 0.00009712) | ( 0.00013629) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00104746   |   0.00147824   |   0.00374982   |   0.00854289   |   0.02392075   |
|         | ( 0.00001244) | ( 0.00001296) | ( 0.00001431) | ( 0.00001523) | ( 0.00002302) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00165958   |   0.00336467   |   0.00769890   |   0.01594366   |   0.03296648   |
|         | ( 0.00001808) | ( 0.00001864) | ( 0.00002450) | ( 0.00003923) | ( 0.00006199) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010737   |   0.00023754   |   0.00072195   |   0.00205923   |   0.00472669   |
|         | ( 0.00000228) | ( 0.00000201) | ( 0.00000281) | ( 0.00000473) | ( 0.00000306) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-5 |    0.00008015   |    0.00017116   |    0.00056661   |   0.00163107   |   0.00390578   |
|         | ( -0.00000008) | ( -0.00000017) | ( -0.00000031) | ( 0.00000116) | ( 0.00000050) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092364   |   0.00364238   |   0.00997709   |   0.02106425   |   0.04547103   |
|         | ( 0.00002381) | ( 0.00002394) | ( 0.00004153) | ( 0.00004124) | ( 0.00004526) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007967   |    0.00014355   |    0.00051557   |    0.00149631   |   0.00423160   |
|         | ( -0.00000130) | ( -0.00000148) | ( -0.00000165) | ( -0.00000135) | ( 0.00000107) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00254100   |   0.00327373   |   0.00714621   |   0.01702637   |   0.04141636   |
|         | ( 0.00000552) | ( 0.00000612) | ( 0.00000728) | ( 0.00000418) | ( 0.00002572) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00053011   |   0.00065915   |   0.00235575   |   0.00603416   |   0.01484923   |
|         | ( 0.00000176) | ( 0.00000110) | ( 0.00000598) | ( 0.00000039) | ( 0.00000161) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00368304   |   0.00377718   |   0.00679367   |   0.02034142   |   0.05342707   |
|          | ( 0.00002119) | ( 0.00002297) | ( 0.00001800) | ( 0.00002961) | ( 0.00001283) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:42:26,824 - block_trainer.py[357] - INFO: epoch 40 (184.061747s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00155366   |   0.00174153   |   0.00364057   |   0.01374557   |   0.06306869   |
|         | ( 0.00003176) | ( 0.00003173) | ( 0.00003713) | ( 0.00004678) | ( 0.00011072) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00232263   |   0.00455866   |   0.01230436   |   0.02670564   |   0.06237435   |
|         | ( 0.00000331) | ( 0.00000615) | ( 0.00002398) | ( 0.00008146) | ( 0.00015599) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00104511   |   0.00147593   |   0.00374809   |   0.00853742   |   0.02390590   |
|         | ( 0.00000236) | ( 0.00000232) | ( 0.00000172) | ( 0.00000547) | ( 0.00001484) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00165926   |   0.00336316   |   0.00769406   |   0.01592617   |   0.03291759   |
|         | ( 0.00000033) | ( 0.00000151) | ( 0.00000485) | ( 0.00001749) | ( 0.00004889) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010613   |   0.00023583   |   0.00072035   |   0.00205239   |   0.00472347   |
|         | ( 0.00000123) | ( 0.00000171) | ( 0.00000160) | ( 0.00000684) | ( 0.00000321) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007880   |   0.00016985   |   0.00056523   |   0.00162808   |   0.00390322   |
|         | ( 0.00000136) | ( 0.00000131) | ( 0.00000137) | ( 0.00000299) | ( 0.00000257) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00093295   |    0.00365057   |   0.00997020   |   0.02104226   |   0.04544121   |
|         | ( -0.00000931) | ( -0.00000819) | ( 0.00000689) | ( 0.00002200) | ( 0.00002982) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007693   |   0.00014090   |   0.00051334   |   0.00149439   |   0.00422862   |
|         | ( 0.00000274) | ( 0.00000265) | ( 0.00000223) | ( 0.00000193) | ( 0.00000298) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-8 |    0.00254134   |    0.00327392   |   0.00714266   |   0.01700687   |   0.04134413   |
|         | ( -0.00000034) | ( -0.00000019) | ( 0.00000355) | ( 0.00001950) | ( 0.00007223) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00052183   |   0.00065076   |   0.00233215   |   0.00601890   |   0.01483148   |
|         | ( 0.00000828) | ( 0.00000839) | ( 0.00002360) | ( 0.00001526) | ( 0.00001775) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00365175   |   0.00374608   |   0.00676161   |   0.02031203   |   0.05333581   |
|          | ( 0.00003129) | ( 0.00003110) | ( 0.00003205) | ( 0.00002939) | ( 0.00009127) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:45:30,763 - block_trainer.py[357] - INFO: epoch 41 (183.938888s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00154058   |   0.00172668   |   0.00362551   |   0.01372195   |   0.06297714   |
|         | ( 0.00001308) | ( 0.00001485) | ( 0.00001506) | ( 0.00002363) | ( 0.00009156) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00231214   |   0.00453726   |   0.01226493   |   0.02662876   |   0.06230261   |
|         | ( 0.00001049) | ( 0.00002140) | ( 0.00003943) | ( 0.00007689) | ( 0.00007175) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-2 |    0.00105242   |    0.00148291   |    0.00375512   |    0.00854725   |   0.02390504   |
|         | ( -0.00000732) | ( -0.00000698) | ( -0.00000703) | ( -0.00000984) | ( 0.00000087) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-3 |    0.00166881   |    0.00337095   |   0.00769272   |   0.01592079   |   0.03290072   |
|         | ( -0.00000955) | ( -0.00000780) | ( 0.00000133) | ( 0.00000538) | ( 0.00001688) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
| block-4 |    0.00010871   |    0.00023896   |    0.00072321   |   0.00204911   |    0.00472526   |
|         | ( -0.00000258) | ( -0.00000313) | ( -0.00000286) | ( 0.00000328) | ( -0.00000178) |
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-5 |    0.00008049   |    0.00017130   |    0.00056657   |    0.00162839   |   0.00390211   |
|         | ( -0.00000169) | ( -0.00000146) | ( -0.00000133) | ( -0.00000031) | ( 0.00000111) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00093830   |    0.00365501   |   0.00995777   |   0.02102626   |   0.04542796   |
|         | ( -0.00000535) | ( -0.00000444) | ( 0.00001243) | ( 0.00001600) | ( 0.00001325) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007839   |    0.00014205   |    0.00051390   |    0.00149477   |   0.00422572   |
|         | ( -0.00000146) | ( -0.00000115) | ( -0.00000056) | ( -0.00000038) | ( 0.00000290) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00253781   |   0.00327040   |   0.00713919   |   0.01700344   |   0.04131430   |
|         | ( 0.00000353) | ( 0.00000352) | ( 0.00000347) | ( 0.00000343) | ( 0.00002983) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-9 |    0.00053201   |    0.00066086   |    0.00233927   |    0.00602161   |   0.01482942   |
|         | ( -0.00001019) | ( -0.00001011) | ( -0.00000712) | ( -0.00000271) | ( 0.00000206) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00366355   |    0.00375770   |    0.00677766   |    0.02032626   |    0.05335088   |
|          | ( -0.00001180) | ( -0.00001162) | ( -0.00001604) | ( -0.00001423) | ( -0.00001507) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 03:48:35,024 - block_trainer.py[357] - INFO: epoch 42 (184.259635s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00153633   |   0.00172467   |   0.00361711   |   0.01370401   |   0.06287741   |
|         | ( 0.00000426) | ( 0.00000202) | ( 0.00000840) | ( 0.00001794) | ( 0.00009973) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00221843   |   0.00444290   |   0.01216649   |   0.02648819   |   0.06214552   |
|         | ( 0.00009372) | ( 0.00009436) | ( 0.00009844) | ( 0.00014056) | ( 0.00015709) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00101448   |   0.00144497   |   0.00371939   |   0.00851046   |   0.02387080   |
|         | ( 0.00003794) | ( 0.00003794) | ( 0.00003573) | ( 0.00003680) | ( 0.00003424) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00162207   |   0.00332582   |   0.00764259   |   0.01586010   |   0.03283007   |
|         | ( 0.00004674) | ( 0.00004513) | ( 0.00005013) | ( 0.00006069) | ( 0.00007064) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010326   |   0.00023294   |   0.00071727   |   0.00204216   |   0.00472002   |
|         | ( 0.00000545) | ( 0.00000601) | ( 0.00000594) | ( 0.00000695) | ( 0.00000523) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007749   |   0.00016820   |   0.00056353   |   0.00162505   |   0.00390065   |
|         | ( 0.00000300) | ( 0.00000310) | ( 0.00000303) | ( 0.00000334) | ( 0.00000146) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091075   |   0.00362780   |   0.00992229   |   0.02099089   |   0.04538949   |
|         | ( 0.00002755) | ( 0.00002721) | ( 0.00003548) | ( 0.00003537) | ( 0.00003847) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |       0.8       |
+---------+----------------+----------------+----------------+----------------+-----------------+
| block-7 |   0.00007735   |   0.00014109   |   0.00051313   |   0.00149395   |    0.00422605   |
|         | ( 0.00000105) | ( 0.00000096) | ( 0.00000077) | ( 0.00000082) | ( -0.00000033) |
+---------+----------------+----------------+----------------+----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00250044   |   0.00323289   |   0.00709972   |   0.01696104   |   0.04125330   |
|         | ( 0.00003737) | ( 0.00003751) | ( 0.00003946) | ( 0.00004240) | ( 0.00006100) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00052063   |   0.00064905   |   0.00232543   |   0.00600320   |   0.01481325   |
|         | ( 0.00001138) | ( 0.00001181) | ( 0.00001384) | ( 0.00001841) | ( 0.00001617) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00363873   |   0.00373285   |   0.00675481   |   0.02029487   |   0.05330171   |
|          | ( 0.00002482) | ( 0.00002485) | ( 0.00002284) | ( 0.00003138) | ( 0.00004917) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 03:51:38,654 - block_trainer.py[357] - INFO: epoch 43 (183.630245s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00158909   |    0.00177982   |    0.00367129   |    0.01376281   |   0.06287265   |
|         | ( -0.00005276) | ( -0.00005516) | ( -0.00005419) | ( -0.00005880) | ( 0.00000476) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-1 |    0.00238006   |    0.00459627   |    0.01229380   |    0.02657380   |    0.06219696   |
|         | ( -0.00016163) | ( -0.00015338) | ( -0.00012731) | ( -0.00008561) | ( -0.00005144) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00106862   |    0.00149864   |    0.00377228   |    0.00856402   |    0.02392226   |
|         | ( -0.00005414) | ( -0.00005367) | ( -0.00005288) | ( -0.00005356) | ( -0.00005146) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00169433   |    0.00339659   |    0.00770857   |    0.01590990   |    0.03285890   |
|         | ( -0.00007226) | ( -0.00007077) | ( -0.00006598) | ( -0.00004980) | ( -0.00002882) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-4 |    0.00010765   |    0.00023746   |    0.00072095   |    0.00204356   |   0.00471970   |
|         | ( -0.00000439) | ( -0.00000452) | ( -0.00000368) | ( -0.00000140) | ( 0.00000033) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00008136   |    0.00017222   |    0.00056723   |    0.00162697   |    0.00390131   |
|         | ( -0.00000388) | ( -0.00000402) | ( -0.00000370) | ( -0.00000192) | ( -0.00000067) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-6 |    0.00093248   |    0.00364791   |    0.00992956   |   0.02098433   |   0.04537280   |
|         | ( -0.00002174) | ( -0.00002011) | ( -0.00000727) | ( 0.00000656) | ( 0.00001669) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007659   |   0.00014037   |   0.00051238   |   0.00149271   |   0.00422310   |
|         | ( 0.00000076) | ( 0.00000072) | ( 0.00000075) | ( 0.00000124) | ( 0.00000296) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-8 |    0.00253533   |    0.00326950   |    0.00713565   |    0.01698816   |    0.04125964   |
|         | ( -0.00003489) | ( -0.00003661) | ( -0.00003593) | ( -0.00002713) | ( -0.00000634) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-9 |    0.00052607   |    0.00065466   |    0.00233094   |    0.00600650   |    0.01482093   |
|         | ( -0.00000544) | ( -0.00000561) | ( -0.00000551) | ( -0.00000329) | ( -0.00000769) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+----------+-----------------+-----------------+-----------------+----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |      0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+----------------+-----------------+
| block-10 |    0.00363935   |    0.00373382   |    0.00675732   |   0.02029297   |    0.05330294   |
|          | ( -0.00000062) | ( -0.00000098) | ( -0.00000251) | ( 0.00000190) | ( -0.00000123) |
+----------+-----------------+-----------------+-----------------+----------------+-----------------+

2024-10-17 03:54:42,167 - block_trainer.py[357] - INFO: epoch 44 (183.511661s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00152190   |   0.00170838   |   0.00359232   |   0.01367117   |   0.06266260   |
|         | ( 0.00006719) | ( 0.00007144) | ( 0.00007897) | ( 0.00009164) | ( 0.00021004) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00225336   |   0.00445509   |   0.01212092   |   0.02634615   |   0.06192689   |
|         | ( 0.00012670) | ( 0.00014119) | ( 0.00017288) | ( 0.00022764) | ( 0.00027007) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102029   |   0.00144992   |   0.00372342   |   0.00851331   |   0.02385961   |
|         | ( 0.00004833) | ( 0.00004872) | ( 0.00004886) | ( 0.00005071) | ( 0.00006266) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00163022   |   0.00333033   |   0.00763603   |   0.01583288   |   0.03276357   |
|         | ( 0.00006411) | ( 0.00006625) | ( 0.00007254) | ( 0.00007702) | ( 0.00009533) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-4 |    0.00010834   |    0.00023793   |   0.00071984   |   0.00204262   |   0.00471768   |
|         | ( -0.00000069) | ( -0.00000046) | ( 0.00000110) | ( 0.00000094) | ( 0.00000202) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-5 |    0.00008209   |   0.00017208   |   0.00056625   |   0.00162481   |   0.00389852   |
|         | ( -0.00000073) | ( 0.00000015) | ( 0.00000098) | ( 0.00000216) | ( 0.00000279) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092886   |   0.00363602   |   0.00990757   |   0.02095720   |   0.04534803   |
|         | ( 0.00000363) | ( 0.00001189) | ( 0.00002198) | ( 0.00002713) | ( 0.00002477) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007809   |    0.00014161   |    0.00051300   |    0.00149290   |   0.00422156   |
|         | ( -0.00000150) | ( -0.00000124) | ( -0.00000061) | ( -0.00000019) | ( 0.00000153) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00252051   |   0.00325372   |   0.00711450   |   0.01696283   |   0.04121378   |
|         | ( 0.00001482) | ( 0.00001578) | ( 0.00002116) | ( 0.00002533) | ( 0.00004586) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00052671   |    0.00065475   |    0.00233219   |   0.00600555   |   0.01481146   |
|         | ( -0.00000064) | ( -0.00000009) | ( -0.00000125) | ( 0.00000095) | ( 0.00000947) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00365435   |    0.00374736   |    0.00676871   |    0.02029708   |   0.05323635   |
|          | ( -0.00001500) | ( -0.00001353) | ( -0.00001139) | ( -0.00000411) | ( 0.00006659) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 03:57:46,393 - block_trainer.py[357] - INFO: epoch 45 (184.225794s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00153270   |    0.00172036   |    0.00359911   |    0.01367385   |   0.06259267   |
|         | ( -0.00001080) | ( -0.00001197) | ( -0.00000679) | ( -0.00000268) | ( 0.00006993) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00226012   |    0.00446088   |    0.01212188   |   0.02632213   |   0.06188116   |
|         | ( -0.00000676) | ( -0.00000580) | ( -0.00000096) | ( 0.00002403) | ( 0.00004572) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00102928   |    0.00145753   |    0.00373170   |    0.00852033   |    0.02386306   |
|         | ( -0.00000898) | ( -0.00000761) | ( -0.00000828) | ( -0.00000702) | ( -0.00000346) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-3 |    0.00163250   |    0.00333226   |   0.00763237   |   0.01581972   |   0.03272466   |
|         | ( -0.00000229) | ( -0.00000193) | ( 0.00000366) | ( 0.00001316) | ( 0.00003891) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010173   |   0.00023239   |   0.00071467   |   0.00203783   |   0.00471210   |
|         | ( 0.00000661) | ( 0.00000554) | ( 0.00000517) | ( 0.00000479) | ( 0.00000558) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007623   |   0.00016738   |   0.00056291   |   0.00162183   |   0.00389534   |
|         | ( 0.00000586) | ( 0.00000470) | ( 0.00000335) | ( 0.00000299) | ( 0.00000318) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091382   |   0.00361786   |   0.00987989   |   0.02091863   |   0.04529704   |
|         | ( 0.00001503) | ( 0.00001816) | ( 0.00002768) | ( 0.00003857) | ( 0.00005099) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007510   |   0.00013897   |   0.00051036   |   0.00149099   |   0.00421744   |
|         | ( 0.00000299) | ( 0.00000264) | ( 0.00000264) | ( 0.00000191) | ( 0.00000412) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00251350   |   0.00324665   |   0.00710582   |   0.01694668   |   0.04118325   |
|         | ( 0.00000701) | ( 0.00000707) | ( 0.00000868) | ( 0.00001615) | ( 0.00003053) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00051356   |   0.00064248   |   0.00232170   |   0.00599274   |   0.01479001   |
|         | ( 0.00001315) | ( 0.00001226) | ( 0.00001048) | ( 0.00001281) | ( 0.00002146) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00364565   |   0.00373877   |   0.00676275   |   0.02029330   |   0.05321356   |
|          | ( 0.00000870) | ( 0.00000859) | ( 0.00000597) | ( 0.00000378) | ( 0.00002279) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:00:50,324 - block_trainer.py[357] - INFO: epoch 46 (183.931164s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00156900   |    0.00175419   |    0.00362968   |    0.01369106   |   0.06252366   |
|         | ( -0.00003630) | ( -0.00003383) | ( -0.00003057) | ( -0.00001722) | ( 0.00006901) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-1 |    0.00234555   |    0.00453917   |    0.01218001   |    0.02632659   |   0.06187610   |
|         | ( -0.00008543) | ( -0.00007829) | ( -0.00005813) | ( -0.00000446) | ( 0.00000506) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00104555   |    0.00147328   |    0.00374767   |    0.00853275   |    0.02386654   |
|         | ( -0.00001628) | ( -0.00001576) | ( -0.00001597) | ( -0.00001242) | ( -0.00000348) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00165021   |    0.00334893   |    0.00764267   |    0.01582548   |    0.03272784   |
|         | ( -0.00001770) | ( -0.00001666) | ( -0.00001029) | ( -0.00000576) | ( -0.00000318) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-4 |    0.00010679   |    0.00023675   |    0.00071800   |    0.00204018   |   0.00471197   |
|         | ( -0.00000506) | ( -0.00000436) | ( -0.00000333) | ( -0.00000235) | ( 0.00000013) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00008002   |    0.00017071   |    0.00056531   |    0.00162349   |    0.00389677   |
|         | ( -0.00000379) | ( -0.00000333) | ( -0.00000241) | ( -0.00000166) | ( -0.00000143) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-6 |    0.00093551   |    0.00363731   |    0.00988488   |   0.02091644   |   0.04527378   |
|         | ( -0.00002168) | ( -0.00001945) | ( -0.00000498) | ( 0.00000219) | ( 0.00002326) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007568   |    0.00013953   |    0.00051045   |    0.00149157   |   0.00421694   |
|         | ( -0.00000058) | ( -0.00000057) | ( -0.00000009) | ( -0.00000058) | ( 0.00000050) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-8 |    0.00253307   |    0.00326612   |    0.00711898   |    0.01695392   |   0.04115086   |
|         | ( -0.00001956) | ( -0.00001947) | ( -0.00001316) | ( -0.00000725) | ( 0.00003239) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-9 |    0.00051641   |    0.00064498   |   0.00232125   |   0.00598981   |   0.01478991   |
|         | ( -0.00000285) | ( -0.00000250) | ( 0.00000045) | ( 0.00000293) | ( 0.00000010) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00361040   |   0.00370456   |   0.00672952   |   0.02025500   |   0.05317678   |
|          | ( 0.00003525) | ( 0.00003421) | ( 0.00003323) | ( 0.00003830) | ( 0.00003677) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:03:54,189 - block_trainer.py[357] - INFO: epoch 47 (183.863988s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00152690   |   0.00171227   |   0.00358384   |   0.01364444   |   0.06240814   |
|         | ( 0.00004210) | ( 0.00004192) | ( 0.00004584) | ( 0.00004663) | ( 0.00011552) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00224802   |   0.00443919   |   0.01207254   |   0.02618922   |   0.06173450   |
|         | ( 0.00009752) | ( 0.00009998) | ( 0.00010747) | ( 0.00013737) | ( 0.00014160) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102660   |   0.00145422   |   0.00372788   |   0.00851619   |   0.02385093   |
|         | ( 0.00001895) | ( 0.00001906) | ( 0.00001978) | ( 0.00001656) | ( 0.00001561) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00162632   |   0.00332400   |   0.00761620   |   0.01578630   |   0.03268588   |
|         | ( 0.00002389) | ( 0.00002492) | ( 0.00002647) | ( 0.00003918) | ( 0.00004196) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+-----------------+----------------+----------------+----------------+
|         |      0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+-----------------+----------------+----------------+----------------+
| block-4 |   0.00010654   |    0.00023676   |   0.00071796   |   0.00203966   |   0.00471110   |
|         | ( 0.00000025) | ( -0.00000001) | ( 0.00000004) | ( 0.00000053) | ( 0.00000087) |
+---------+----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007779   |   0.00016875   |   0.00056349   |   0.00162142   |   0.00389444   |
|         | ( 0.00000222) | ( 0.00000196) | ( 0.00000183) | ( 0.00000207) | ( 0.00000233) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092353   |   0.00362365   |   0.00986380   |   0.02088746   |   0.04524967   |
|         | ( 0.00001197) | ( 0.00001366) | ( 0.00002108) | ( 0.00002897) | ( 0.00002411) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007735   |    0.00014089   |    0.00051070   |    0.00149179   |   0.00421690   |
|         | ( -0.00000167) | ( -0.00000135) | ( -0.00000025) | ( -0.00000022) | ( 0.00000004) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00249180   |   0.00322554   |   0.00707924   |   0.01690826   |   0.04110427   |
|         | ( 0.00004127) | ( 0.00004058) | ( 0.00003973) | ( 0.00004566) | ( 0.00004659) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+-----------------+----------------+----------------+
|         |      0.0       |      0.2       |       0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+-----------------+----------------+----------------+
| block-9 |   0.00051578   |   0.00064440   |    0.00232148   |   0.00598481   |   0.01478270   |
|         | ( 0.00000063) | ( 0.00000058) | ( -0.00000023) | ( 0.00000499) | ( 0.00000721) |
+---------+----------------+----------------+-----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00360633   |   0.00369866   |   0.00671939   |   0.02024257   |   0.05312935   |
|          | ( 0.00000407) | ( 0.00000590) | ( 0.00001014) | ( 0.00001243) | ( 0.00004743) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:06:57,611 - block_trainer.py[357] - INFO: epoch 48 (183.421358s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00156755   |    0.00175495   |    0.00362619   |    0.01367843   |   0.06231270   |
|         | ( -0.00004065) | ( -0.00004268) | ( -0.00004236) | ( -0.00003399) | ( 0.00009544) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00222743   |   0.00440322   |   0.01200647   |   0.02607199   |   0.06160329   |
|         | ( 0.00002059) | ( 0.00003598) | ( 0.00006608) | ( 0.00011724) | ( 0.00013121) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00101341   |   0.00144069   |   0.00371444   |   0.00849927   |   0.02382344   |
|         | ( 0.00001319) | ( 0.00001353) | ( 0.00001344) | ( 0.00001692) | ( 0.00002749) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00162181   |   0.00331854   |   0.00760263   |   0.01576448   |   0.03263191   |
|         | ( 0.00000451) | ( 0.00000546) | ( 0.00001357) | ( 0.00002182) | ( 0.00005398) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010261   |   0.00023330   |   0.00071481   |   0.00203601   |   0.00470614   |
|         | ( 0.00000393) | ( 0.00000347) | ( 0.00000314) | ( 0.00000364) | ( 0.00000496) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007639   |   0.00016732   |   0.00056178   |   0.00161924   |   0.00389197   |
|         | ( 0.00000141) | ( 0.00000143) | ( 0.00000171) | ( 0.00000218) | ( 0.00000247) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092216   |   0.00362200   |   0.00985673   |   0.02087419   |   0.04522224   |
|         | ( 0.00000137) | ( 0.00000164) | ( 0.00000707) | ( 0.00001327) | ( 0.00002743) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007716   |   0.00014078   |   0.00050851   |   0.00149134   |   0.00421357   |
|         | ( 0.00000019) | ( 0.00000011) | ( 0.00000219) | ( 0.00000044) | ( 0.00000333) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00249917   |    0.00323187   |    0.00708009   |   0.01689805   |   0.04105625   |
|         | ( -0.00000737) | ( -0.00000633) | ( -0.00000085) | ( 0.00001021) | ( 0.00004801) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00051529   |   0.00064368   |   0.00232110   |   0.00597865   |   0.01476543   |
|         | ( 0.00000048) | ( 0.00000072) | ( 0.00000038) | ( 0.00000616) | ( 0.00001727) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00359966   |   0.00369142   |   0.00671527   |   0.02022749   |   0.05305954   |
|          | ( 0.00000667) | ( 0.00000724) | ( 0.00000412) | ( 0.00001508) | ( 0.00006981) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:10:02,663 - block_trainer.py[357] - INFO: epoch 49 (185.052411s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00150629   |   0.00169174   |   0.00356146   |   0.01361113   |   0.06220697   |
|         | ( 0.00006126) | ( 0.00006321) | ( 0.00006474) | ( 0.00006730) | ( 0.00010573) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00221881   |   0.00439410   |   0.01199392   |   0.02604505   |   0.06157951   |
|         | ( 0.00000862) | ( 0.00000911) | ( 0.00001255) | ( 0.00002694) | ( 0.00002378) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+-----------------+-----------------+-----------------+
|         |      0.0       |      0.2       |       0.4       |       0.6       |       0.8       |
+---------+----------------+----------------+-----------------+-----------------+-----------------+
| block-2 |   0.00101305   |   0.00144027   |    0.00371489   |    0.00850244   |    0.02382659   |
|         | ( 0.00000036) | ( 0.00000042) | ( -0.00000045) | ( -0.00000317) | ( -0.00000315) |
+---------+----------------+----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00161569   |   0.00331010   |   0.00759562   |   0.01574850   |   0.03260651   |
|         | ( 0.00000612) | ( 0.00000844) | ( 0.00000701) | ( 0.00001598) | ( 0.00002540) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-4 |    0.00010451   |    0.00023506   |    0.00071623   |    0.00203671   |    0.00470657   |
|         | ( -0.00000189) | ( -0.00000176) | ( -0.00000141) | ( -0.00000070) | ( -0.00000043) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00007740   |    0.00016841   |    0.00056242   |    0.00162003   |    0.00389300   |
|         | ( -0.00000101) | ( -0.00000108) | ( -0.00000065) | ( -0.00000079) | ( -0.00000103) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091862   |   0.00361888   |   0.00984269   |   0.02085112   |   0.04521051   |
|         | ( 0.00000354) | ( 0.00000312) | ( 0.00001404) | ( 0.00002307) | ( 0.00001173) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+-----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |       0.6       |      0.8       |
+---------+----------------+----------------+----------------+-----------------+----------------+
| block-7 |   0.00007502   |   0.00013931   |   0.00050735   |    0.00149169   |   0.00421285   |
|         | ( 0.00000214) | ( 0.00000147) | ( 0.00000116) | ( -0.00000034) | ( 0.00000073) |
+---------+----------------+----------------+----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-8 |    0.00251222   |    0.00324580   |    0.00709604   |    0.01692385   |    0.04106999   |
|         | ( -0.00001305) | ( -0.00001393) | ( -0.00001595) | ( -0.00002580) | ( -0.00001374) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+-----------------+-----------------+-----------------+
|         |      0.0       |      0.2       |       0.4       |       0.6       |       0.8       |
+---------+----------------+----------------+-----------------+-----------------+-----------------+
| block-9 |   0.00051371   |   0.00064227   |    0.00232120   |    0.00598184   |    0.01476985   |
|         | ( 0.00000159) | ( 0.00000141) | ( -0.00000010) | ( -0.00000320) | ( -0.00000442) |
+---------+----------------+----------------+-----------------+-----------------+-----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00362602   |    0.00371548   |    0.00674432   |    0.02026400   |    0.05310680   |
|          | ( -0.00002636) | ( -0.00002405) | ( -0.00002905) | ( -0.00003651) | ( -0.00004725) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 04:13:07,392 - block_trainer.py[357] - INFO: epoch 50 (184.728267s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00159970   |    0.00178725   |    0.00365137   |    0.01368693   |   0.06220303   |
|         | ( -0.00009341) | ( -0.00009551) | ( -0.00008991) | ( -0.00007580) | ( 0.00000394) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-1 |    0.00234319   |    0.00451130   |    0.01210108   |    0.02610247   |    0.06159470   |
|         | ( -0.00012438) | ( -0.00011720) | ( -0.00010717) | ( -0.00005742) | ( -0.00001518) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00105208   |    0.00147900   |    0.00375349   |    0.00853968   |    0.02386457   |
|         | ( -0.00003903) | ( -0.00003873) | ( -0.00003860) | ( -0.00003724) | ( -0.00003797) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00167817   |    0.00337005   |    0.00765243   |    0.01579773   |    0.03263060   |
|         | ( -0.00006248) | ( -0.00005995) | ( -0.00005681) | ( -0.00004924) | ( -0.00002409) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-4 |    0.00010968   |    0.00023925   |    0.00071977   |    0.00203933   |    0.00470777   |
|         | ( -0.00000518) | ( -0.00000419) | ( -0.00000355) | ( -0.00000262) | ( -0.00000120) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-5 |    0.00007919   |    0.00017004   |    0.00056299   |    0.00162078   |   0.00389201   |
|         | ( -0.00000179) | ( -0.00000163) | ( -0.00000057) | ( -0.00000075) | ( 0.00000099) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-6 |    0.00094235   |    0.00364005   |    0.00985371   |   0.02084172   |   0.04519436   |
|         | ( -0.00002373) | ( -0.00002116) | ( -0.00001102) | ( 0.00000940) | ( 0.00001614) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-7 |    0.00008032   |    0.00014407   |    0.00051030   |    0.00149355   |    0.00421370   |
|         | ( -0.00000530) | ( -0.00000476) | ( -0.00000295) | ( -0.00000186) | ( -0.00000085) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-8 |    0.00254320   |    0.00327625   |    0.00712169   |    0.01694087   |   0.04106883   |
|         | ( -0.00003098) | ( -0.00003045) | ( -0.00002565) | ( -0.00001702) | ( 0.00000116) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-9 |    0.00052199   |    0.00065017   |    0.00232626   |   0.00598028   |   0.01476787   |
|         | ( -0.00000829) | ( -0.00000790) | ( -0.00000507) | ( 0.00000156) | ( 0.00000198) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00361696   |   0.00370675   |   0.00673384   |   0.02024917   |   0.05308848   |
|          | ( 0.00000906) | ( 0.00000872) | ( 0.00001048) | ( 0.00001482) | ( 0.00001832) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:16:12,936 - block_trainer.py[357] - INFO: epoch 51 (185.543604s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00162800   |    0.00181517   |    0.00368046   |    0.01371542   |   0.06219522   |
|         | ( -0.00002830) | ( -0.00002791) | ( -0.00002909) | ( -0.00002849) | ( 0.00000781) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-1 |    0.00239123   |    0.00455950   |    0.01215072   |    0.02612393   |    0.06163370   |
|         | ( -0.00004804) | ( -0.00004820) | ( -0.00004963) | ( -0.00002146) | ( -0.00003901) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00106374   |    0.00149063   |    0.00376442   |    0.00854878   |    0.02386652   |
|         | ( -0.00001165) | ( -0.00001163) | ( -0.00001093) | ( -0.00000910) | ( -0.00000195) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00169449   |    0.00338404   |    0.00766660   |    0.01580869   |    0.03264041   |
|         | ( -0.00001632) | ( -0.00001399) | ( -0.00001417) | ( -0.00001096) | ( -0.00000981) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010762   |   0.00023809   |   0.00071946   |   0.00203843   |   0.00470626   |
|         | ( 0.00000207) | ( 0.00000116) | ( 0.00000032) | ( 0.00000090) | ( 0.00000150) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007826   |   0.00016929   |   0.00055972   |   0.00162052   |   0.00389200   |
|         | ( 0.00000093) | ( 0.00000075) | ( 0.00000327) | ( 0.00000026) | ( 0.00000001) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00093776   |   0.00363586   |   0.00984504   |   0.02083008   |   0.04517761   |
|         | ( 0.00000458) | ( 0.00000419) | ( 0.00000866) | ( 0.00001164) | ( 0.00001675) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007805   |   0.00014187   |   0.00050816   |   0.00149290   |   0.00421197   |
|         | ( 0.00000228) | ( 0.00000221) | ( 0.00000213) | ( 0.00000065) | ( 0.00000173) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00251724   |   0.00325122   |   0.00709921   |   0.01691832   |   0.04102976   |
|         | ( 0.00002596) | ( 0.00002503) | ( 0.00002249) | ( 0.00002255) | ( 0.00003907) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00050842   |   0.00063644   |   0.00231461   |   0.00597253   |   0.01475666   |
|         | ( 0.00001358) | ( 0.00001373) | ( 0.00001166) | ( 0.00000775) | ( 0.00001122) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00363769   |    0.00372735   |    0.00675637   |    0.02026876   |   0.05307551   |
|          | ( -0.00002073) | ( -0.00002060) | ( -0.00002253) | ( -0.00001958) | ( 0.00001297) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 04:19:17,717 - block_trainer.py[357] - INFO: epoch 52 (184.780125s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00149958   |   0.00168546   |   0.00354069   |   0.01356493   |   0.06196201   |
|         | ( 0.00012841) | ( 0.00012971) | ( 0.00013977) | ( 0.00015049) | ( 0.00023321) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00223852   |   0.00438613   |   0.01193230   |   0.02585905   |   0.06131886   |
|         | ( 0.00015271) | ( 0.00017337) | ( 0.00021841) | ( 0.00026488) | ( 0.00031485) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00100693   |   0.00143311   |   0.00370700   |   0.00848898   |   0.02380292   |
|         | ( 0.00005681) | ( 0.00005752) | ( 0.00005742) | ( 0.00005980) | ( 0.00006360) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00160398   |   0.00328771   |   0.00756717   |   0.01570448   |   0.03252254   |
|         | ( 0.00009051) | ( 0.00009632) | ( 0.00009943) | ( 0.00010421) | ( 0.00011787) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010542   |   0.00023560   |   0.00071655   |   0.00203481   |   0.00470104   |
|         | ( 0.00000220) | ( 0.00000249) | ( 0.00000291) | ( 0.00000362) | ( 0.00000522) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-5 |    0.00007974   |    0.00017054   |   0.00055764   |   0.00161973   |   0.00389115   |
|         | ( -0.00000147) | ( -0.00000125) | ( 0.00000208) | ( 0.00000079) | ( 0.00000085) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00089894   |   0.00359547   |   0.00979693   |   0.02076925   |   0.04512150   |
|         | ( 0.00003882) | ( 0.00004039) | ( 0.00004812) | ( 0.00006084) | ( 0.00005611) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007492   |   0.00013873   |   0.00050459   |   0.00148859   |   0.00420761   |
|         | ( 0.00000313) | ( 0.00000314) | ( 0.00000357) | ( 0.00000430) | ( 0.00000436) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00246707   |   0.00320123   |   0.00704554   |   0.01686029   |   0.04094002   |
|         | ( 0.00005017) | ( 0.00004999) | ( 0.00005366) | ( 0.00005802) | ( 0.00008974) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00050762   |   0.00063604   |   0.00231170   |   0.00596429   |   0.01475442   |
|         | ( 0.00000080) | ( 0.00000039) | ( 0.00000291) | ( 0.00000824) | ( 0.00000223) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00356268   |   0.00365212   |   0.00668239   |   0.02018925   |   0.05299916   |
|          | ( 0.00007501) | ( 0.00007523) | ( 0.00007398) | ( 0.00007951) | ( 0.00007635) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:22:22,426 - block_trainer.py[357] - INFO: epoch 53 (184.709073s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00155552   |    0.00174240   |    0.00360091   |    0.01362042   |   0.06194167   |
|         | ( -0.00005594) | ( -0.00005694) | ( -0.00006022) | ( -0.00005549) | ( 0.00002035) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00225461   |    0.00439939   |    0.01194980   |   0.02584343   |   0.06130645   |
|         | ( -0.00001610) | ( -0.00001326) | ( -0.00001749) | ( 0.00001563) | ( 0.00001240) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00102234   |    0.00144852   |    0.00372304   |    0.00850613   |    0.02381968   |
|         | ( -0.00001540) | ( -0.00001541) | ( -0.00001605) | ( -0.00001715) | ( -0.00001676) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-3 |    0.00162628   |    0.00330567   |    0.00758376   |    0.01570771   |   0.03250303   |
|         | ( -0.00002230) | ( -0.00001795) | ( -0.00001660) | ( -0.00000323) | ( 0.00001951) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+-----------------+----------------+----------------+----------------+
|         |      0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+-----------------+----------------+----------------+----------------+
| block-4 |   0.00010522   |    0.00023562   |   0.00071649   |   0.00203377   |   0.00470003   |
|         | ( 0.00000020) | ( -0.00000002) | ( 0.00000006) | ( 0.00000104) | ( 0.00000101) |
+---------+----------------+-----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007809   |   0.00016888   |   0.00055549   |   0.00161778   |   0.00388900   |
|         | ( 0.00000165) | ( 0.00000166) | ( 0.00000215) | ( 0.00000196) | ( 0.00000216) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-6 |    0.00091730   |    0.00361350   |    0.00980633   |   0.02076656   |   0.04510497   |
|         | ( -0.00001836) | ( -0.00001802) | ( -0.00000941) | ( 0.00000269) | ( 0.00001653) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-7 |    0.00007594   |    0.00013938   |   0.00050301   |   0.00148719   |   0.00420502   |
|         | ( -0.00000102) | ( -0.00000065) | ( 0.00000158) | ( 0.00000140) | ( 0.00000259) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-8 |    0.00250773   |    0.00324063   |    0.00708271   |    0.01688550   |    0.04094414   |
|         | ( -0.00004066) | ( -0.00003941) | ( -0.00003717) | ( -0.00002520) | ( -0.00000412) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-9 |    0.00050839   |    0.00063652   |   0.00231119   |   0.00596225   |   0.01474366   |
|         | ( -0.00000078) | ( -0.00000047) | ( 0.00000051) | ( 0.00000204) | ( 0.00001077) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00358282   |    0.00367182   |    0.00669917   |    0.02020737   |   0.05297481   |
|          | ( -0.00002014) | ( -0.00001970) | ( -0.00001678) | ( -0.00001813) | ( 0.00002435) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 04:25:27,896 - block_trainer.py[357] - INFO: epoch 54 (185.469328s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00153387   |   0.00172053   |   0.00357646   |   0.01359476   |   0.06183476   |
|         | ( 0.00002165) | ( 0.00002187) | ( 0.00002446) | ( 0.00002566) | ( 0.00010691) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00222217   |   0.00436783   |   0.01191821   |   0.02579122   |   0.06124741   |
|         | ( 0.00003244) | ( 0.00003157) | ( 0.00003159) | ( 0.00005220) | ( 0.00005904) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |       0.8       |
+---------+----------------+----------------+----------------+----------------+-----------------+
| block-2 |   0.00101898   |   0.00144526   |   0.00372022   |   0.00850287   |    0.02382072   |
|         | ( 0.00000336) | ( 0.00000326) | ( 0.00000282) | ( 0.00000326) | ( -0.00000104) |
+---------+----------------+----------------+----------------+----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00161853   |   0.00329651   |   0.00757393   |   0.01569402   |   0.03249042   |
|         | ( 0.00000775) | ( 0.00000916) | ( 0.00000983) | ( 0.00001368) | ( 0.00001261) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010439   |   0.00023478   |   0.00071562   |   0.00203229   |   0.00469811   |
|         | ( 0.00000083) | ( 0.00000084) | ( 0.00000087) | ( 0.00000148) | ( 0.00000192) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007576   |   0.00016704   |   0.00055411   |   0.00161710   |   0.00388797   |
|         | ( 0.00000232) | ( 0.00000184) | ( 0.00000139) | ( 0.00000068) | ( 0.00000103) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |       0.8       |
+---------+----------------+----------------+----------------+----------------+-----------------+
| block-6 |   0.00091548   |   0.00360973   |   0.00979702   |   0.02075619   |    0.04510559   |
|         | ( 0.00000182) | ( 0.00000377) | ( 0.00000932) | ( 0.00001037) | ( -0.00000062) |
+---------+----------------+----------------+----------------+----------------+-----------------+
+---------+----------------+----------------+----------------+-----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |       0.6       |       0.8       |
+---------+----------------+----------------+----------------+-----------------+-----------------+
| block-7 |   0.00007493   |   0.00013893   |   0.00050251   |    0.00148808   |    0.00420522   |
|         | ( 0.00000101) | ( 0.00000044) | ( 0.00000050) | ( -0.00000088) | ( -0.00000019) |
+---------+----------------+----------------+----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00249954   |   0.00323309   |   0.00707422   |   0.01686515   |   0.04091231   |
|         | ( 0.00000819) | ( 0.00000754) | ( 0.00000850) | ( 0.00002035) | ( 0.00003183) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00050185   |   0.00063011   |   0.00230610   |   0.00595625   |   0.01473599   |
|         | ( 0.00000655) | ( 0.00000640) | ( 0.00000509) | ( 0.00000599) | ( 0.00000767) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00357877   |   0.00366813   |   0.00669143   |   0.02019116   |   0.05293254   |
|          | ( 0.00000406) | ( 0.00000370) | ( 0.00000774) | ( 0.00001621) | ( 0.00004227) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:28:33,205 - block_trainer.py[357] - INFO: epoch 55 (185.308361s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00153299   |   0.00172042   |   0.00357403   |   0.01358377   |   0.06176967   |
|         | ( 0.00000088) | ( 0.00000011) | ( 0.00000242) | ( 0.00001100) | ( 0.00006509) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-1 |    0.00229170   |    0.00443455   |    0.01197614   |    0.02581449   |    0.06126690   |
|         | ( -0.00006953) | ( -0.00006673) | ( -0.00005793) | ( -0.00002326) | ( -0.00001948) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00104252   |    0.00146854   |    0.00374205   |    0.00852528   |    0.02383645   |
|         | ( -0.00002354) | ( -0.00002328) | ( -0.00002184) | ( -0.00002242) | ( -0.00001573) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00165000   |    0.00332571   |    0.00760225   |    0.01571298   |    0.03249146   |
|         | ( -0.00003147) | ( -0.00002920) | ( -0.00002831) | ( -0.00001896) | ( -0.00000104) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00010507   |    0.00023543   |    0.00071597   |   0.00203145   |   0.00469654   |
|         | ( -0.00000068) | ( -0.00000064) | ( -0.00000036) | ( 0.00000084) | ( 0.00000157) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-5 |    0.00007921   |    0.00017028   |    0.00055672   |    0.00161918   |   0.00388791   |
|         | ( -0.00000344) | ( -0.00000324) | ( -0.00000261) | ( -0.00000208) | ( 0.00000006) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00091730   |   0.00360658   |   0.00979644   |   0.02074233   |   0.04508793   |
|         | ( -0.00000182) | ( 0.00000315) | ( 0.00000057) | ( 0.00001386) | ( 0.00001766) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-7 |    0.00007567   |    0.00013957   |    0.00050282   |    0.00148812   |    0.00420532   |
|         | ( -0.00000074) | ( -0.00000063) | ( -0.00000030) | ( -0.00000005) | ( -0.00000011) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-8 |    0.00252018   |    0.00325299   |    0.00708971   |    0.01687787   |    0.04091405   |
|         | ( -0.00002064) | ( -0.00001990) | ( -0.00001549) | ( -0.00001272) | ( -0.00000174) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-9 |    0.00050888   |    0.00063777   |    0.00231502   |    0.00596571   |    0.01474069   |
|         | ( -0.00000704) | ( -0.00000766) | ( -0.00000892) | ( -0.00000945) | ( -0.00000471) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00363128   |    0.00372021   |    0.00675067   |    0.02024825   |    0.05299037   |
|          | ( -0.00005251) | ( -0.00005208) | ( -0.00005925) | ( -0.00005709) | ( -0.00005783) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 04:31:37,898 - block_trainer.py[357] - INFO: epoch 56 (184.692462s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00156283   |    0.00174962   |    0.00360130   |    0.01360005   |   0.06168524   |
|         | ( -0.00002983) | ( -0.00002920) | ( -0.00002727) | ( -0.00001628) | ( 0.00008443) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00229007   |   0.00442445   |   0.01195946   |   0.02576367   |   0.06120082   |
|         | ( 0.00000163) | ( 0.00001010) | ( 0.00001668) | ( 0.00005081) | ( 0.00006608) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00103866   |   0.00146434   |   0.00373811   |   0.00852113   |   0.02383597   |
|         | ( 0.00000386) | ( 0.00000420) | ( 0.00000394) | ( 0.00000415) | ( 0.00000049) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00164042   |   0.00331350   |   0.00758876   |   0.01569670   |   0.03246575   |
|         | ( 0.00000958) | ( 0.00001220) | ( 0.00001349) | ( 0.00001628) | ( 0.00002571) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-4 |    0.00010741   |    0.00023745   |    0.00071795   |    0.00203340   |    0.00469786   |
|         | ( -0.00000234) | ( -0.00000202) | ( -0.00000198) | ( -0.00000195) | ( -0.00000132) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |       0.8       |
+---------+----------------+----------------+----------------+----------------+-----------------+
| block-5 |   0.00007728   |   0.00016863   |   0.00055525   |   0.00161894   |    0.00388816   |
|         | ( 0.00000193) | ( 0.00000165) | ( 0.00000147) | ( 0.00000024) | ( -0.00000026) |
+---------+----------------+----------------+----------------+----------------+-----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-6 |    0.00091928   |   0.00360474   |   0.00978985   |   0.02071996   |   0.04507221   |
|         | ( -0.00000198) | ( 0.00000184) | ( 0.00000659) | ( 0.00002238) | ( 0.00001572) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-7 |    0.00007714   |    0.00014096   |    0.00050401   |    0.00148895   |   0.00420416   |
|         | ( -0.00000147) | ( -0.00000140) | ( -0.00000119) | ( -0.00000083) | ( 0.00000116) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00249447   |   0.00322801   |   0.00706619   |   0.01685647   |   0.04087790   |
|         | ( 0.00002571) | ( 0.00002499) | ( 0.00002352) | ( 0.00002140) | ( 0.00003615) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00050590   |   0.00063393   |   0.00230740   |   0.00595859   |   0.01473830   |
|         | ( 0.00000298) | ( 0.00000384) | ( 0.00000762) | ( 0.00000712) | ( 0.00000239) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00354796   |   0.00363674   |   0.00666745   |   0.02016221   |   0.05288579   |
|          | ( 0.00008332) | ( 0.00008347) | ( 0.00008323) | ( 0.00008605) | ( 0.00010458) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:34:42,425 - block_trainer.py[357] - INFO: epoch 57 (184.526529s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00160985   |    0.00179709   |    0.00364491   |    0.01363824   |   0.06166825   |
|         | ( -0.00004702) | ( -0.00004747) | ( -0.00004361) | ( -0.00003820) | ( 0.00001698) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-1 |    0.00239390   |    0.00451817   |    0.01203203   |    0.02580107   |    0.06121767   |
|         | ( -0.00010383) | ( -0.00009372) | ( -0.00007258) | ( -0.00003739) | ( -0.00001685) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00106801   |    0.00149397   |    0.00376750   |    0.00855054   |    0.02385397   |
|         | ( -0.00002935) | ( -0.00002963) | ( -0.00002939) | ( -0.00002940) | ( -0.00001800) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00168409   |    0.00335406   |    0.00762701   |    0.01572955   |    0.03248642   |
|         | ( -0.00004367) | ( -0.00004056) | ( -0.00003825) | ( -0.00003285) | ( -0.00002067) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-4 |    0.00010922   |    0.00023969   |    0.00072047   |    0.00203386   |    0.00469865   |
|         | ( -0.00000181) | ( -0.00000224) | ( -0.00000252) | ( -0.00000046) | ( -0.00000080) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-5 |    0.00007742   |   0.00016834   |   0.00055367   |   0.00161715   |   0.00388479   |
|         | ( -0.00000014) | ( 0.00000029) | ( 0.00000158) | ( 0.00000179) | ( 0.00000337) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-6 |    0.00093871   |    0.00362328   |    0.00980190   |   0.02071293   |   0.04506250   |
|         | ( -0.00001943) | ( -0.00001854) | ( -0.00001205) | ( 0.00000702) | ( 0.00000971) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007483   |   0.00013870   |   0.00050194   |   0.00148651   |   0.00420069   |
|         | ( 0.00000231) | ( 0.00000227) | ( 0.00000206) | ( 0.00000245) | ( 0.00000347) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-8 |    0.00250698   |    0.00324028   |    0.00707286   |   0.01685422   |   0.04085583   |
|         | ( -0.00001251) | ( -0.00001227) | ( -0.00000667) | ( 0.00000225) | ( 0.00002207) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00050297   |   0.00063107   |   0.00230540   |   0.00595409   |   0.01472462   |
|         | ( 0.00000294) | ( 0.00000286) | ( 0.00000200) | ( 0.00000450) | ( 0.00001368) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00356521   |    0.00365259   |    0.00668974   |    0.02017173   |   0.05287901   |
|          | ( -0.00001725) | ( -0.00001585) | ( -0.00002230) | ( -0.00000953) | ( 0.00000677) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 04:37:47,683 - block_trainer.py[357] - INFO: epoch 58 (185.257148s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00152140   |   0.00170792   |   0.00355902   |   0.01356132   |   0.06155986   |
|         | ( 0.00008845) | ( 0.00008918) | ( 0.00008589) | ( 0.00007693) | ( 0.00010839) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00227440   |   0.00439333   |   0.01190337   |   0.02564253   |   0.06105747   |
|         | ( 0.00011950) | ( 0.00012485) | ( 0.00012866) | ( 0.00015854) | ( 0.00016019) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00102654   |   0.00145211   |   0.00372625   |   0.00850776   |   0.02381041   |
|         | ( 0.00004147) | ( 0.00004186) | ( 0.00004126) | ( 0.00004277) | ( 0.00004356) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00162498   |   0.00329387   |   0.00756477   |   0.01565607   |   0.03239479   |
|         | ( 0.00005911) | ( 0.00006019) | ( 0.00006224) | ( 0.00007348) | ( 0.00009163) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010443   |   0.00023448   |   0.00071478   |   0.00202621   |   0.00469201   |
|         | ( 0.00000480) | ( 0.00000522) | ( 0.00000569) | ( 0.00000765) | ( 0.00000664) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007453   |   0.00016536   |   0.00054997   |   0.00161448   |   0.00388199   |
|         | ( 0.00000289) | ( 0.00000299) | ( 0.00000370) | ( 0.00000267) | ( 0.00000279) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00091742   |   0.00360066   |   0.00977429   |   0.02067687   |   0.04503976   |
|         | ( 0.00002129) | ( 0.00002262) | ( 0.00002762) | ( 0.00003606) | ( 0.00002274) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
| block-7 |    0.00007528   |    0.00013920   |    0.00050240   |   0.00148633   |    0.00420119   |
|         | ( -0.00000046) | ( -0.00000051) | ( -0.00000045) | ( 0.00000018) | ( -0.00000050) |
+---------+-----------------+-----------------+-----------------+----------------+-----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-8 |    0.00250755   |   0.00324003   |   0.00706920   |   0.01684611   |   0.04084601   |
|         | ( -0.00000057) | ( 0.00000025) | ( 0.00000367) | ( 0.00000811) | ( 0.00000982) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00049801   |   0.00062589   |   0.00229725   |   0.00594661   |   0.01471343   |
|         | ( 0.00000495) | ( 0.00000518) | ( 0.00000815) | ( 0.00000749) | ( 0.00001118) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00358959   |    0.00367709   |    0.00671141   |    0.02019791   |    0.05291608   |
|          | ( -0.00002439) | ( -0.00002450) | ( -0.00002166) | ( -0.00002618) | ( -0.00003707) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 04:40:51,953 - block_trainer.py[357] - INFO: epoch 59 (184.269396s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00155487   |    0.00174062   |    0.00358684   |    0.01357532   |   0.06145158   |
|         | ( -0.00003347) | ( -0.00003270) | ( -0.00002782) | ( -0.00001400) | ( 0.00010828) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00230678   |    0.00442269   |    0.01192107   |   0.02563916   |   0.06104911   |
|         | ( -0.00003237) | ( -0.00002937) | ( -0.00001770) | ( 0.00000337) | ( 0.00000836) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00104731   |    0.00147232   |    0.00374638   |    0.00852755   |    0.02383284   |
|         | ( -0.00002077) | ( -0.00002021) | ( -0.00002013) | ( -0.00001979) | ( -0.00002243) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00164435   |    0.00331253   |    0.00758231   |    0.01566709   |    0.03240190   |
|         | ( -0.00001937) | ( -0.00001866) | ( -0.00001754) | ( -0.00001102) | ( -0.00000710) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-4 |    0.00010844   |    0.00023811   |    0.00071794   |    0.00202703   |    0.00469377   |
|         | ( -0.00000401) | ( -0.00000363) | ( -0.00000316) | ( -0.00000082) | ( -0.00000176) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00007823   |    0.00016874   |    0.00055180   |    0.00161628   |    0.00388418   |
|         | ( -0.00000370) | ( -0.00000338) | ( -0.00000183) | ( -0.00000180) | ( -0.00000218) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-6 |    0.00094481   |    0.00362663   |    0.00979435   |    0.02068471   |    0.04505095   |
|         | ( -0.00002740) | ( -0.00002597) | ( -0.00002006) | ( -0.00000784) | ( -0.00001119) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-7 |    0.00007706   |    0.00014090   |    0.00050388   |    0.00148714   |    0.00420163   |
|         | ( -0.00000177) | ( -0.00000170) | ( -0.00000148) | ( -0.00000081) | ( -0.00000044) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-8 |    0.00253374   |    0.00326705   |    0.00709305   |    0.01687276   |   0.04084447   |
|         | ( -0.00002619) | ( -0.00002702) | ( -0.00002385) | ( -0.00002666) | ( 0.00000154) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-9 |    0.00051402   |    0.00064207   |    0.00231243   |    0.00596140   |    0.01472633   |
|         | ( -0.00001601) | ( -0.00001618) | ( -0.00001518) | ( -0.00001480) | ( -0.00001290) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-10 |    0.00361384   |    0.00370163   |    0.00673842   |    0.02022219   |    0.05291743   |
|          | ( -0.00002425) | ( -0.00002454) | ( -0.00002702) | ( -0.00002427) | ( -0.00000136) |
+----------+-----------------+-----------------+-----------------+-----------------+-----------------+

2024-10-17 04:43:55,507 - block_trainer.py[357] - INFO: epoch 60 (183.553505s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00145543   |   0.00164376   |   0.00349167   |   0.01345762   |   0.06126425   |
|         | ( 0.00009944) | ( 0.00009685) | ( 0.00009517) | ( 0.00011770) | ( 0.00018733) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00224042   |   0.00434894   |   0.01183082   |   0.02549718   |   0.06086976   |
|         | ( 0.00006635) | ( 0.00007375) | ( 0.00009025) | ( 0.00014198) | ( 0.00017935) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00100384   |   0.00142917   |   0.00370277   |   0.00848328   |   0.02378281   |
|         | ( 0.00004348) | ( 0.00004316) | ( 0.00004361) | ( 0.00004427) | ( 0.00005002) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00157021   |   0.00323582   |   0.00749831   |   0.01558230   |   0.03228758   |
|         | ( 0.00007414) | ( 0.00007672) | ( 0.00008400) | ( 0.00008479) | ( 0.00011432) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00010454   |   0.00023458   |   0.00071457   |   0.00202202   |   0.00468795   |
|         | ( 0.00000389) | ( 0.00000353) | ( 0.00000337) | ( 0.00000502) | ( 0.00000582) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007438   |   0.00016544   |   0.00054811   |   0.00161367   |   0.00387979   |
|         | ( 0.00000385) | ( 0.00000330) | ( 0.00000369) | ( 0.00000261) | ( 0.00000439) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00090101   |   0.00357990   |   0.00974484   |   0.02062074   |   0.04499306   |
|         | ( 0.00004381) | ( 0.00004673) | ( 0.00004951) | ( 0.00006397) | ( 0.00005789) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007643   |   0.00014028   |   0.00050304   |   0.00148601   |   0.00419896   |
|         | ( 0.00000063) | ( 0.00000063) | ( 0.00000084) | ( 0.00000113) | ( 0.00000267) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00245684   |   0.00318994   |   0.00701442   |   0.01677926   |   0.04074394   |
|         | ( 0.00007690) | ( 0.00007711) | ( 0.00007863) | ( 0.00009350) | ( 0.00010053) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00050085   |   0.00062919   |   0.00229937   |   0.00594468   |   0.01469863   |
|         | ( 0.00001317) | ( 0.00001288) | ( 0.00001306) | ( 0.00001672) | ( 0.00002770) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00353563   |   0.00362298   |   0.00665779   |   0.02015047   |   0.05282195   |
|          | ( 0.00007821) | ( 0.00007864) | ( 0.00008064) | ( 0.00007172) | ( 0.00009548) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:46:59,446 - block_trainer.py[357] - INFO: epoch 61 (183.938242s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-0 |    0.00159340   |    0.00178062   |    0.00362348   |    0.01358847   |    0.06129698   |
|         | ( -0.00013796) | ( -0.00013686) | ( -0.00013180) | ( -0.00013085) | ( -0.00003274) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-1 |    0.00227791   |    0.00438658   |    0.01187235   |    0.02553120   |    0.06091283   |
|         | ( -0.00003749) | ( -0.00003763) | ( -0.00004153) | ( -0.00003402) | ( -0.00004307) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00103321   |    0.00145787   |    0.00373138   |    0.00851170   |    0.02380263   |
|         | ( -0.00002937) | ( -0.00002870) | ( -0.00002861) | ( -0.00002842) | ( -0.00001982) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00164655   |    0.00331163   |    0.00757515   |    0.01565557   |    0.03235729   |
|         | ( -0.00007634) | ( -0.00007582) | ( -0.00007684) | ( -0.00007327) | ( -0.00006971) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+----------------+----------------+----------------+----------------+
|         |       0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+----------------+----------------+----------------+----------------+
| block-4 |    0.00010459   |   0.00023457   |   0.00071450   |   0.00202077   |   0.00468749   |
|         | ( -0.00000005) | ( 0.00000001) | ( 0.00000007) | ( 0.00000124) | ( 0.00000046) |
+---------+-----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-5 |    0.00007642   |    0.00016721   |    0.00054829   |    0.00161369   |   0.00387879   |
|         | ( -0.00000204) | ( -0.00000177) | ( -0.00000018) | ( -0.00000003) | ( 0.00000100) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-6 |    0.00092710   |    0.00360342   |    0.00976065   |    0.02062760   |    0.04499964   |
|         | ( -0.00002609) | ( -0.00002351) | ( -0.00001582) | ( -0.00000686) | ( -0.00000658) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+-----------------+-----------------+-----------------+
|         |      0.0       |      0.2       |       0.4       |       0.6       |       0.8       |
+---------+----------------+----------------+-----------------+-----------------+-----------------+
| block-7 |   0.00007612   |   0.00014017   |    0.00050344   |    0.00148602   |    0.00419975   |
|         | ( 0.00000031) | ( 0.00000010) | ( -0.00000039) | ( -0.00000001) | ( -0.00000080) |
+---------+----------------+----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-8 |    0.00250747   |    0.00324029   |    0.00706287   |    0.01682330   |    0.04079600   |
|         | ( -0.00005062) | ( -0.00005035) | ( -0.00004845) | ( -0.00004404) | ( -0.00005206) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-9 |    0.00050564   |    0.00063450   |    0.00230662   |    0.00595397   |    0.01470698   |
|         | ( -0.00000479) | ( -0.00000531) | ( -0.00000725) | ( -0.00000929) | ( -0.00000835) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
|          |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-10 |    0.00355536   |    0.00364373   |    0.00668457   |    0.02016705   |   0.05281875   |
|          | ( -0.00001973) | ( -0.00002075) | ( -0.00002679) | ( -0.00001658) | ( 0.00000320) |
+----------+-----------------+-----------------+-----------------+-----------------+----------------+

2024-10-17 04:50:03,724 - block_trainer.py[357] - INFO: epoch 62 (184.278115s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00152927   |   0.00171762   |   0.00356238   |   0.01352060   |   0.06121378   |
|         | ( 0.00006412) | ( 0.00006300) | ( 0.00006109) | ( 0.00006787) | ( 0.00008320) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00226124   |   0.00436305   |   0.01183207   |   0.02546700   |   0.06083672   |
|         | ( 0.00001666) | ( 0.00002353) | ( 0.00004028) | ( 0.00006420) | ( 0.00007611) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+-----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |       0.8       |
+---------+----------------+----------------+----------------+----------------+-----------------+
| block-2 |   0.00102844   |   0.00145359   |   0.00372785   |   0.00850904   |    0.02380662   |
|         | ( 0.00000477) | ( 0.00000427) | ( 0.00000353) | ( 0.00000265) | ( -0.00000399) |
+---------+----------------+----------------+----------------+----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00162897   |   0.00329343   |   0.00755403   |   0.01563374   |   0.03232195   |
|         | ( 0.00001758) | ( 0.00001821) | ( 0.00002112) | ( 0.00002183) | ( 0.00003534) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-4 |    0.00010545   |    0.00023592   |    0.00071614   |    0.00202139   |   0.00468731   |
|         | ( -0.00000086) | ( -0.00000135) | ( -0.00000164) | ( -0.00000061) | ( 0.00000018) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-5 |    0.00007698   |    0.00016805   |    0.00054982   |    0.00161590   |    0.00388288   |
|         | ( -0.00000056) | ( -0.00000084) | ( -0.00000153) | ( -0.00000220) | ( -0.00000409) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00092094   |   0.00359518   |   0.00975117   |   0.02060476   |   0.04498144   |
|         | ( 0.00000616) | ( 0.00000824) | ( 0.00000949) | ( 0.00002285) | ( 0.00001820) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007530   |   0.00013889   |   0.00050154   |   0.00148455   |   0.00419641   |
|         | ( 0.00000082) | ( 0.00000128) | ( 0.00000190) | ( 0.00000147) | ( 0.00000334) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00250591   |   0.00323919   |   0.00706121   |   0.01680022   |   0.04078560   |
|         | ( 0.00000156) | ( 0.00000109) | ( 0.00000166) | ( 0.00002308) | ( 0.00001040) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00049723   |   0.00062605   |   0.00229747   |   0.00594480   |   0.01469317   |
|         | ( 0.00000841) | ( 0.00000845) | ( 0.00000915) | ( 0.00000917) | ( 0.00001381) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+-----------------+-----------------+----------------+----------------+----------------+
|          |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+----------+-----------------+-----------------+----------------+----------------+----------------+
| block-10 |    0.00356034   |    0.00364657   |   0.00668447   |   0.02015041   |   0.05281219   |
|          | ( -0.00000498) | ( -0.00000283) | ( 0.00000010) | ( 0.00001665) | ( 0.00000656) |
+----------+-----------------+-----------------+----------------+----------------+----------------+

2024-10-17 04:53:08,093 - block_trainer.py[357] - INFO: epoch 63 (184.368246s, 55 blocks still need training), blocks loss: 
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-0 |   0.00151521   |   0.00170119   |   0.00354456   |   0.01349177   |   0.06112032   |
|         | ( 0.00001406) | ( 0.00001642) | ( 0.00001782) | ( 0.00002882) | ( 0.00009346) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-1 |   0.00221172   |   0.00430993   |   0.01176954   |   0.02538828   |   0.06077753   |
|         | ( 0.00004952) | ( 0.00005312) | ( 0.00006253) | ( 0.00007872) | ( 0.00005919) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-2 |   0.00101554   |   0.00143994   |   0.00371297   |   0.00849135   |   0.02378526   |
|         | ( 0.00001289) | ( 0.00001365) | ( 0.00001487) | ( 0.00001770) | ( 0.00002136) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-3 |   0.00161143   |   0.00327507   |   0.00753564   |   0.01560513   |   0.03228235   |
|         | ( 0.00001754) | ( 0.00001836) | ( 0.00001839) | ( 0.00002861) | ( 0.00003960) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-4 |    0.00010853   |    0.00023804   |    0.00071734   |   0.00202063   |   0.00468575   |
|         | ( -0.00000308) | ( -0.00000212) | ( -0.00000121) | ( 0.00000076) | ( 0.00000156) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-5 |    0.00007954   |    0.00017022   |    0.00055072   |    0.00161594   |   0.00388216   |
|         | ( -0.00000256) | ( -0.00000217) | ( -0.00000090) | ( -0.00000004) | ( 0.00000072) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+----------------+----------------+----------------+
|         |       0.0       |       0.2       |      0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+----------------+----------------+----------------+
| block-6 |    0.00092303   |    0.00359555   |   0.00974245   |   0.02059307   |   0.04495625   |
|         | ( -0.00000209) | ( -0.00000037) | ( 0.00000872) | ( 0.00001169) | ( 0.00002520) |
+---------+-----------------+-----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-7 |    0.00007817   |    0.00014192   |    0.00050447   |    0.00148664   |    0.00419662   |
|         | ( -0.00000287) | ( -0.00000303) | ( -0.00000293) | ( -0.00000209) | ( -0.00000020) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00250262   |   0.00323658   |   0.00706032   |   0.01678028   |   0.04075690   |
|         | ( 0.00000329) | ( 0.00000262) | ( 0.00000089) | ( 0.00001993) | ( 0.00002870) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-9 |    0.00050724   |    0.00063628   |    0.00230600   |    0.00594714   |    0.01469323   |
|         | ( -0.00001001) | ( -0.00001023) | ( -0.00000854) | ( -0.00000234) | ( -0.00000006) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+----------+----------------+----------------+----------------+-----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |       0.6       |      0.8       |
+----------+----------------+----------------+----------------+-----------------+----------------+
| block-10 |   0.00354583   |   0.00363267   |   0.00667301   |    0.02015265   |   0.05279227   |
|          | ( 0.00001451) | ( 0.00001390) | ( 0.00001146) | ( -0.00000225) | ( 0.00001992) |
+----------+----------------+----------------+----------------+-----------------+----------------+

2024-10-17 04:56:12,374 - block_trainer.py[357] - INFO: epoch 64 (184.280870s, 55 blocks still need training), blocks loss: 
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
| block-0 |    0.00152037   |    0.00171027   |    0.00355382   |    0.01349258   |   0.06105512   |
|         | ( -0.00000516) | ( -0.00000907) | ( -0.00000927) | ( -0.00000081) | ( 0.00006520) |
+---------+-----------------+-----------------+-----------------+-----------------+----------------+
+---------+-----------------+-----------------+-----------------+----------------+----------------+
|         |       0.0       |       0.2       |       0.4       |      0.6       |      0.8       |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
| block-1 |    0.00221553   |    0.00431430   |    0.01177138   |   0.02535824   |   0.06070523   |
|         | ( -0.00000381) | ( -0.00000437) | ( -0.00000184) | ( 0.00003004) | ( 0.00007230) |
+---------+-----------------+-----------------+-----------------+----------------+----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-2 |    0.00101618   |    0.00144103   |    0.00371553   |    0.00849531   |    0.02379338   |
|         | ( -0.00000064) | ( -0.00000109) | ( -0.00000255) | ( -0.00000396) | ( -0.00000811) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
|         |       0.0       |       0.2       |       0.4       |       0.6       |       0.8       |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
| block-3 |    0.00162648   |    0.00329132   |    0.00754639   |    0.01562121   |    0.03228465   |
|         | ( -0.00001505) | ( -0.00001625) | ( -0.00001075) | ( -0.00001608) | ( -0.00000231) |
+---------+-----------------+-----------------+-----------------+-----------------+-----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-4 |   0.00009958   |   0.00023060   |   0.00071166   |   0.00201618   |   0.00468277   |
|         | ( 0.00000895) | ( 0.00000745) | ( 0.00000569) | ( 0.00000445) | ( 0.00000298) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-5 |   0.00007568   |   0.00016679   |   0.00054833   |   0.00161438   |   0.00388150   |
|         | ( 0.00000386) | ( 0.00000343) | ( 0.00000239) | ( 0.00000155) | ( 0.00000066) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-6 |   0.00090901   |   0.00357938   |   0.00972300   |   0.02057530   |   0.04494614   |
|         | ( 0.00001402) | ( 0.00001617) | ( 0.00001945) | ( 0.00001777) | ( 0.00001011) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-7 |   0.00007252   |   0.00013632   |   0.00049921   |   0.00148212   |   0.00419344   |
|         | ( 0.00000565) | ( 0.00000560) | ( 0.00000525) | ( 0.00000453) | ( 0.00000317) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-8 |   0.00247524   |   0.00320925   |   0.00703156   |   0.01673927   |   0.04072225   |
|         | ( 0.00002738) | ( 0.00002733) | ( 0.00002876) | ( 0.00004102) | ( 0.00003465) |
+---------+----------------+----------------+----------------+----------------+----------------+
+---------+----------------+----------------+----------------+----------------+----------------+
|         |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+---------+----------------+----------------+----------------+----------------+----------------+
| block-9 |   0.00049173   |   0.00062010   |   0.00229146   |   0.00593492   |   0.01468002   |
|         | ( 0.00001551) | ( 0.00001619) | ( 0.00001454) | ( 0.00001221) | ( 0.00001321) |
+---------+----------------+----------------+----------------+----------------+----------------+
+----------+----------------+----------------+----------------+----------------+----------------+
|          |      0.0       |      0.2       |      0.4       |      0.6       |      0.8       |
+----------+----------------+----------------+----------------+----------------+----------------+
| block-10 |   0.00351205   |   0.00359740   |   0.00663980   |   0.02011277   |   0.05274182   |
|          | ( 0.00003378) | ( 0.00003527) | ( 0.00003321) | ( 0.00003988) | ( 0.00005045) |
+----------+----------------+----------------+----------------+----------------+----------------+

2024-10-17 04:56:12,460 - server_block_profiler.py[193] - INFO: raw block info: {"index": 0, "id": "block-0", "size": 131144, "FLOPs": 46137344.0, "param": 22528.0, "input_size": [3, 32, 32], "output_size": [48, 32, 32]}
2024-10-17 04:56:12,496 - server_block_profiler.py[193] - INFO: raw block info: {"index": 1, "id": "block-1", "size": 318841, "FLOPs": 105234432.0, "param": 67584.0, "input_size": [48, 32, 32], "output_size": [64, 16, 16]}
2024-10-17 04:56:12,533 - server_block_profiler.py[193] - INFO: raw block info: {"index": 2, "id": "block-2", "size": 488682, "FLOPs": 55640064.0, "param": 108672.0, "input_size": [64, 16, 16], "output_size": [384, 16, 16]}
2024-10-17 04:56:12,568 - server_block_profiler.py[193] - INFO: raw block info: {"index": 3, "id": "block-3", "size": 1202105, "FLOPs": 46317568.0, "param": 285824.0, "input_size": [64, 16, 16], "output_size": [768, 8, 8]}
2024-10-17 04:56:12,598 - server_block_profiler.py[193] - INFO: raw block info: {"index": 4, "id": "block-4", "size": 867966, "FLOPs": 26443776.0, "param": 206592.0, "input_size": [128, 8, 8], "output_size": [128, 8, 8]}
2024-10-17 04:56:12,630 - server_block_profiler.py[193] - INFO: raw block info: {"index": 5, "id": "block-5", "size": 867966, "FLOPs": 26443776.0, "param": 206592.0, "input_size": [128, 8, 8], "output_size": [128, 8, 8]}
2024-10-17 04:56:12,668 - server_block_profiler.py[193] - INFO: raw block info: {"index": 6, "id": "block-6", "size": 2927609, "FLOPs": 91373568.0, "param": 713856.0, "input_size": [128, 8, 8], "output_size": [1152, 8, 8]}
2024-10-17 04:56:12,699 - server_block_profiler.py[193] - INFO: raw block info: {"index": 7, "id": "block-7", "size": 1877118, "FLOPs": 58540032.0, "param": 457344.0, "input_size": [192, 8, 8], "output_size": [192, 8, 8]}
2024-10-17 04:56:12,744 - server_block_profiler.py[193] - INFO: raw block info: {"index": 8, "id": "block-8", "size": 7532025, "FLOPs": 80969728.0, "param": 1859840.0, "input_size": [192, 8, 8], "output_size": [1920, 4, 4]}
2024-10-17 04:56:12,781 - server_block_profiler.py[193] - INFO: raw block info: {"index": 9, "id": "block-9", "size": 5075070, "FLOPs": 40120320.0, "param": 1253760.0, "input_size": [320, 4, 4], "output_size": [320, 4, 4]}
2024-10-17 04:56:12,832 - server_block_profiler.py[193] - INFO: raw block info: {"index": 10, "id": "block-10", "size": 14098860, "FLOPs": 112250880.0, "param": 3507840.0, "input_size": [320, 4, 4], "output_size": [2560, 4, 4]}
2024-10-17 04:56:15,536 - server_block_profiler.py[264] - INFO: profile blocks acc drop
2024-10-17 04:56:28,392 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:56:31,000 - server_block_profiler.py[70] - INFO: get -1-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:56:33,286 - server_block_profiler.py[70] - INFO: get -1-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:56:34,824 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:56:38,233 - server_block_profiler.py[70] - INFO: get -1-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:56:40,603 - server_block_profiler.py[70] - INFO: get -1-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:56:42,012 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:56:44,548 - server_block_profiler.py[70] - INFO: get -1-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:56:46,669 - server_block_profiler.py[70] - INFO: get -1-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:56:48,069 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:56:50,922 - server_block_profiler.py[70] - INFO: get -1-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:56:53,049 - server_block_profiler.py[70] - INFO: get -1-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:56:54,459 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:05,669 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:08,268 - server_block_profiler.py[70] - INFO: get 2--1-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:08,269 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:08,269 - server_block_profiler.py[70] - INFO: get 8--1-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:09,737 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:12,211 - server_block_profiler.py[70] - INFO: get 2--1-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:14,427 - server_block_profiler.py[70] - INFO: get 8--1-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:15,952 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:18,276 - server_block_profiler.py[70] - INFO: get 2--1-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:20,330 - server_block_profiler.py[70] - INFO: get 8--1-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:22,030 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:24,274 - server_block_profiler.py[70] - INFO: get 2--1-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:26,309 - server_block_profiler.py[70] - INFO: get 8--1-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:26,310 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:26,310 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:36,542 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:39,133 - server_block_profiler.py[70] - INFO: get 2-2--1-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:39,133 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:39,134 - server_block_profiler.py[70] - INFO: get 8-8--1-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:40,516 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:43,087 - server_block_profiler.py[70] - INFO: get 2-2--1-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:45,280 - server_block_profiler.py[70] - INFO: get 8-8--1-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:46,691 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:49,199 - server_block_profiler.py[70] - INFO: get 2-2--1-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:51,484 - server_block_profiler.py[70] - INFO: get 8-8--1-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:52,996 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:57:55,471 - server_block_profiler.py[70] - INFO: get 2-2--1-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:57:57,622 - server_block_profiler.py[70] - INFO: get 8-8--1-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:57,623 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:57:57,624 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:07,629 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:10,234 - server_block_profiler.py[70] - INFO: get 2-2-2--1-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:10,235 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:10,236 - server_block_profiler.py[70] - INFO: get 8-8-8--1-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:11,615 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:14,910 - server_block_profiler.py[70] - INFO: get 2-2-2--1-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:17,232 - server_block_profiler.py[70] - INFO: get 8-8-8--1-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:18,604 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:21,191 - server_block_profiler.py[70] - INFO: get 2-2-2--1-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:23,417 - server_block_profiler.py[70] - INFO: get 8-8-8--1-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:24,852 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:27,389 - server_block_profiler.py[70] - INFO: get 2-2-2--1-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:29,819 - server_block_profiler.py[70] - INFO: get 8-8-8--1-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:29,820 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:29,821 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:40,013 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:42,668 - server_block_profiler.py[70] - INFO: get 2-2-2-2--1-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:42,668 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:42,669 - server_block_profiler.py[70] - INFO: get 8-8-8-8--1-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:44,117 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:46,733 - server_block_profiler.py[70] - INFO: get 2-2-2-2--1-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:48,951 - server_block_profiler.py[70] - INFO: get 8-8-8-8--1-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:50,332 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:52,910 - server_block_profiler.py[70] - INFO: get 2-2-2-2--1-2-2-2-2-2-2 metrics in cache
2024-10-17 04:58:55,166 - server_block_profiler.py[70] - INFO: get 8-8-8-8--1-8-8-8-8-8-8 metrics in cache
2024-10-17 04:58:56,531 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:58:59,169 - server_block_profiler.py[70] - INFO: get 2-2-2-2--1-2-2-2-2-2-2 metrics in cache
2024-10-17 04:59:01,429 - server_block_profiler.py[70] - INFO: get 8-8-8-8--1-8-8-8-8-8-8 metrics in cache
2024-10-17 04:59:01,430 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:59:01,431 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:12,294 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:14,898 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2--1-2-2-2-2-2 metrics in cache
2024-10-17 04:59:14,898 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:59:14,899 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8--1-8-8-8-8-8 metrics in cache
2024-10-17 04:59:16,295 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:18,955 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2--1-2-2-2-2-2 metrics in cache
2024-10-17 04:59:21,224 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8--1-8-8-8-8-8 metrics in cache
2024-10-17 04:59:22,593 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:25,249 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2--1-2-2-2-2-2 metrics in cache
2024-10-17 04:59:27,539 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8--1-8-8-8-8-8 metrics in cache
2024-10-17 04:59:28,909 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:31,478 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2--1-2-2-2-2-2 metrics in cache
2024-10-17 04:59:33,665 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8--1-8-8-8-8-8 metrics in cache
2024-10-17 04:59:33,665 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 04:59:33,666 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:43,752 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:46,452 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2--1-2-2-2-2 metrics in cache
2024-10-17 04:59:46,453 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 04:59:46,453 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8--1-8-8-8-8 metrics in cache
2024-10-17 04:59:47,995 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:50,629 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2--1-2-2-2-2 metrics in cache
2024-10-17 04:59:53,045 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8--1-8-8-8-8 metrics in cache
2024-10-17 04:59:54,902 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 04:59:57,479 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2--1-2-2-2-2 metrics in cache
2024-10-17 04:59:59,684 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8--1-8-8-8-8 metrics in cache
2024-10-17 05:00:01,089 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:03,566 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2--1-2-2-2-2 metrics in cache
2024-10-17 05:00:05,710 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8--1-8-8-8-8 metrics in cache
2024-10-17 05:00:05,711 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 05:00:05,712 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:15,739 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:18,401 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2--1-2-2-2 metrics in cache
2024-10-17 05:00:18,402 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 05:00:18,402 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8--1-8-8-8 metrics in cache
2024-10-17 05:00:19,762 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:22,391 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2--1-2-2-2 metrics in cache
2024-10-17 05:00:24,630 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8--1-8-8-8 metrics in cache
2024-10-17 05:00:26,007 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:28,562 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2--1-2-2-2 metrics in cache
2024-10-17 05:00:30,917 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8--1-8-8-8 metrics in cache
2024-10-17 05:00:32,447 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:35,053 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2--1-2-2-2 metrics in cache
2024-10-17 05:00:37,203 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8--1-8-8-8 metrics in cache
2024-10-17 05:00:37,206 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 05:00:37,207 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:47,936 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:50,596 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2--1-2-2 metrics in cache
2024-10-17 05:00:50,597 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 05:00:50,597 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8--1-8-8 metrics in cache
2024-10-17 05:00:51,913 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:00:54,475 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2--1-2-2 metrics in cache
2024-10-17 05:00:56,769 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8--1-8-8 metrics in cache
2024-10-17 05:00:58,186 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:00,754 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2--1-2-2 metrics in cache
2024-10-17 05:01:02,970 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8--1-8-8 metrics in cache
2024-10-17 05:01:04,394 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:06,927 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2--1-2-2 metrics in cache
2024-10-17 05:01:09,258 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8--1-8-8 metrics in cache
2024-10-17 05:01:09,258 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 05:01:09,258 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:19,503 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:22,154 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2--1-2 metrics in cache
2024-10-17 05:01:22,154 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 05:01:22,155 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8--1-8 metrics in cache
2024-10-17 05:01:23,517 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:27,104 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2--1-2 metrics in cache
2024-10-17 05:01:29,355 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8--1-8 metrics in cache
2024-10-17 05:01:30,708 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:33,359 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2--1-2 metrics in cache
2024-10-17 05:01:35,598 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8--1-8 metrics in cache
2024-10-17 05:01:36,987 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:39,590 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2--1-2 metrics in cache
2024-10-17 05:01:41,809 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8--1-8 metrics in cache
2024-10-17 05:01:41,809 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 05:01:41,810 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:52,112 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:54,692 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2--1 metrics in cache
2024-10-17 05:01:54,692 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2-2 metrics in cache
2024-10-17 05:01:54,692 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8--1 metrics in cache
2024-10-17 05:01:56,155 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:01:58,706 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2--1 metrics in cache
2024-10-17 05:02:00,993 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8--1 metrics in cache
2024-10-17 05:02:02,467 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:02:05,010 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2--1 metrics in cache
2024-10-17 05:02:07,243 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8--1 metrics in cache
2024-10-17 05:02:08,624 - server_block_profiler.py[70] - INFO: get -1--1--1--1--1--1--1--1--1--1--1 metrics in cache
2024-10-17 05:02:12,121 - server_block_profiler.py[70] - INFO: get 2-2-2-2-2-2-2-2-2-2--1 metrics in cache
2024-10-17 05:02:14,214 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8--1 metrics in cache
2024-10-17 05:02:14,215 - server_block_profiler.py[70] - INFO: get 8-8-8-8-8-8-8-8-8-8-8 metrics in cache
2024-10-17 05:02:14,218 - server_block_profiler.py[307] - INFO: block block-0 (sparsity 0.0) acc drop: 0.0004666646321614583
2024-10-17 05:02:14,234 - server_block_profiler.py[335] - INFO: block block-0 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,234 - server_block_profiler.py[307] - INFO: block block-0 (sparsity 0.2) acc drop: 3.331899642944336e-05
2024-10-17 05:02:14,247 - server_block_profiler.py[335] - INFO: block block-0 (sparsity 0.2) size drop: 28352B (0.027MB), FLOPs drop: 13.885M, param drop: 0.007M
2024-10-17 05:02:14,247 - server_block_profiler.py[307] - INFO: block block-0 (sparsity 0.4) acc drop: 0.0015666683514912922
2024-10-17 05:02:14,259 - server_block_profiler.py[335] - INFO: block block-0 (sparsity 0.4) size drop: 50944B (0.049MB), FLOPs drop: 25.043M, param drop: 0.012M
2024-10-17 05:02:14,260 - server_block_profiler.py[307] - INFO: block block-0 (sparsity 0.6) acc drop: 0.0031333367029825845
2024-10-17 05:02:14,272 - server_block_profiler.py[335] - INFO: block block-0 (sparsity 0.6) size drop: 71232B (0.068MB), FLOPs drop: 34.689M, param drop: 0.017M
2024-10-17 05:02:14,272 - server_block_profiler.py[307] - INFO: block block-0 (sparsity 0.8) acc drop: 0.02346666653951009
2024-10-17 05:02:14,286 - server_block_profiler.py[335] - INFO: block block-0 (sparsity 0.8) size drop: 85120B (0.081MB), FLOPs drop: 41.386M, param drop: 0.020M
2024-10-17 05:02:14,287 - server_block_profiler.py[307] - INFO: block block-1 (sparsity 0.0) acc drop: 0.000733335812886556
2024-10-17 05:02:14,300 - server_block_profiler.py[335] - INFO: block block-1 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,301 - server_block_profiler.py[307] - INFO: block block-1 (sparsity 0.2) acc drop: 0.0012999971707661946
2024-10-17 05:02:14,316 - server_block_profiler.py[335] - INFO: block block-1 (sparsity 0.2) size drop: 75072B (0.072MB), FLOPs drop: 27.313M, param drop: 0.018M
2024-10-17 05:02:14,316 - server_block_profiler.py[307] - INFO: block block-1 (sparsity 0.4) acc drop: 0.0028333465258280435
2024-10-17 05:02:14,330 - server_block_profiler.py[335] - INFO: block block-1 (sparsity 0.4) size drop: 144448B (0.138MB), FLOPs drop: 52.417M, param drop: 0.035M
2024-10-17 05:02:14,330 - server_block_profiler.py[307] - INFO: block block-1 (sparsity 0.6) acc drop: 0.006800015767415364
2024-10-17 05:02:14,343 - server_block_profiler.py[335] - INFO: block block-1 (sparsity 0.6) size drop: 203328B (0.194MB), FLOPs drop: 74.376M, param drop: 0.049M
2024-10-17 05:02:14,343 - server_block_profiler.py[307] - INFO: block block-1 (sparsity 0.8) acc drop: 0.020066678524017334
2024-10-17 05:02:14,356 - server_block_profiler.py[335] - INFO: block block-1 (sparsity 0.8) size drop: 246528B (0.235MB), FLOPs drop: 91.499M, param drop: 0.059M
2024-10-17 05:02:14,357 - server_block_profiler.py[307] - INFO: block block-2 (sparsity 0.0) acc drop: 9.999672571818034e-05
2024-10-17 05:02:14,371 - server_block_profiler.py[335] - INFO: block block-2 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,371 - server_block_profiler.py[307] - INFO: block block-2 (sparsity 0.2) acc drop: 0.0003333290417989095
2024-10-17 05:02:14,387 - server_block_profiler.py[335] - INFO: block block-2 (sparsity 0.2) size drop: 110784B (0.106MB), FLOPs drop: 13.789M, param drop: 0.027M
2024-10-17 05:02:14,387 - server_block_profiler.py[307] - INFO: block block-2 (sparsity 0.4) acc drop: 0.0019333362579345703
2024-10-17 05:02:14,401 - server_block_profiler.py[335] - INFO: block block-2 (sparsity 0.4) size drop: 214400B (0.204MB), FLOPs drop: 26.639M, param drop: 0.052M
2024-10-17 05:02:14,401 - server_block_profiler.py[307] - INFO: block block-2 (sparsity 0.6) acc drop: 0.003466665744781494
2024-10-17 05:02:14,415 - server_block_profiler.py[335] - INFO: block block-2 (sparsity 0.6) size drop: 297920B (0.284MB), FLOPs drop: 37.034M, param drop: 0.072M
2024-10-17 05:02:14,416 - server_block_profiler.py[307] - INFO: block block-2 (sparsity 0.8) acc drop: 0.007399996121724446
2024-10-17 05:02:14,431 - server_block_profiler.py[335] - INFO: block block-2 (sparsity 0.8) size drop: 386368B (0.368MB), FLOPs drop: 48.080M, param drop: 0.094M
2024-10-17 05:02:14,431 - server_block_profiler.py[307] - INFO: block block-3 (sparsity 0.0) acc drop: 0.00026667118072509766
2024-10-17 05:02:14,447 - server_block_profiler.py[335] - INFO: block block-3 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,447 - server_block_profiler.py[307] - INFO: block block-3 (sparsity 0.2) acc drop: 0.0008999903996785482
2024-10-17 05:02:14,462 - server_block_profiler.py[335] - INFO: block block-3 (sparsity 0.2) size drop: 302976B (0.289MB), FLOPs drop: 12.217M, param drop: 0.074M
2024-10-17 05:02:14,462 - server_block_profiler.py[307] - INFO: block block-3 (sparsity 0.4) acc drop: 0.0028000076611836753
2024-10-17 05:02:14,476 - server_block_profiler.py[335] - INFO: block block-3 (sparsity 0.4) size drop: 572992B (0.546MB), FLOPs drop: 23.074M, param drop: 0.141M
2024-10-17 05:02:14,476 - server_block_profiler.py[307] - INFO: block block-3 (sparsity 0.6) acc drop: 0.00550001859664917
2024-10-17 05:02:14,490 - server_block_profiler.py[335] - INFO: block block-3 (sparsity 0.6) size drop: 809408B (0.772MB), FLOPs drop: 32.606M, param drop: 0.199M
2024-10-17 05:02:14,490 - server_block_profiler.py[307] - INFO: block block-3 (sparsity 0.8) acc drop: 0.012499988079071045
2024-10-17 05:02:14,503 - server_block_profiler.py[335] - INFO: block block-3 (sparsity 0.8) size drop: 1002048B (0.956MB), FLOPs drop: 40.040M, param drop: 0.246M
2024-10-17 05:02:14,503 - server_block_profiler.py[307] - INFO: block block-4 (sparsity 0.0) acc drop: -9.999672571818034e-05
2024-10-17 05:02:14,512 - server_block_profiler.py[335] - INFO: block block-4 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,513 - server_block_profiler.py[307] - INFO: block block-4 (sparsity 0.2) acc drop: 0.0007000168164571127
2024-10-17 05:02:14,521 - server_block_profiler.py[335] - INFO: block block-4 (sparsity 0.2) size drop: 216192B (0.206MB), FLOPs drop: 6.818M, param drop: 0.053M
2024-10-17 05:02:14,521 - server_block_profiler.py[307] - INFO: block block-4 (sparsity 0.4) acc drop: 0.0012000203132629395
2024-10-17 05:02:14,531 - server_block_profiler.py[335] - INFO: block block-4 (sparsity 0.4) size drop: 403072B (0.384MB), FLOPs drop: 12.705M, param drop: 0.099M
2024-10-17 05:02:14,531 - server_block_profiler.py[307] - INFO: block block-4 (sparsity 0.6) acc drop: 0.0017666816711425781
2024-10-17 05:02:14,539 - server_block_profiler.py[335] - INFO: block block-4 (sparsity 0.6) size drop: 584192B (0.557MB), FLOPs drop: 18.421M, param drop: 0.144M
2024-10-17 05:02:14,539 - server_block_profiler.py[307] - INFO: block block-4 (sparsity 0.8) acc drop: 0.002300004164377848
2024-10-17 05:02:14,547 - server_block_profiler.py[335] - INFO: block block-4 (sparsity 0.8) size drop: 736192B (0.702MB), FLOPs drop: 23.207M, param drop: 0.181M
2024-10-17 05:02:14,547 - server_block_profiler.py[307] - INFO: block block-5 (sparsity 0.0) acc drop: -3.333886464436849e-05
2024-10-17 05:02:14,556 - server_block_profiler.py[335] - INFO: block block-5 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,557 - server_block_profiler.py[307] - INFO: block block-5 (sparsity 0.2) acc drop: 0.00026667118072509766
2024-10-17 05:02:14,565 - server_block_profiler.py[335] - INFO: block block-5 (sparsity 0.2) size drop: 221504B (0.211MB), FLOPs drop: 6.990M, param drop: 0.055M
2024-10-17 05:02:14,565 - server_block_profiler.py[307] - INFO: block block-5 (sparsity 0.4) acc drop: 0.0012000004450480144
2024-10-17 05:02:14,574 - server_block_profiler.py[335] - INFO: block block-5 (sparsity 0.4) size drop: 408384B (0.389MB), FLOPs drop: 12.878M, param drop: 0.101M
2024-10-17 05:02:14,574 - server_block_profiler.py[307] - INFO: block block-5 (sparsity 0.6) acc drop: 0.001899997393290202
2024-10-17 05:02:14,582 - server_block_profiler.py[335] - INFO: block block-5 (sparsity 0.6) size drop: 590592B (0.563MB), FLOPs drop: 18.628M, param drop: 0.146M
2024-10-17 05:02:14,582 - server_block_profiler.py[307] - INFO: block block-5 (sparsity 0.8) acc drop: 0.003533323605855306
2024-10-17 05:02:14,590 - server_block_profiler.py[335] - INFO: block block-5 (sparsity 0.8) size drop: 742528B (0.708MB), FLOPs drop: 23.414M, param drop: 0.183M
2024-10-17 05:02:14,590 - server_block_profiler.py[307] - INFO: block block-6 (sparsity 0.0) acc drop: 0.00019997358322143555
2024-10-17 05:02:14,610 - server_block_profiler.py[335] - INFO: block block-6 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,610 - server_block_profiler.py[307] - INFO: block block-6 (sparsity 0.2) acc drop: 0.0023333231608072915
2024-10-17 05:02:14,627 - server_block_profiler.py[335] - INFO: block block-6 (sparsity 0.2) size drop: 804544B (0.767MB), FLOPs drop: 25.455M, param drop: 0.199M
2024-10-17 05:02:14,627 - server_block_profiler.py[307] - INFO: block block-6 (sparsity 0.4) acc drop: 0.006399989128112793
2024-10-17 05:02:14,645 - server_block_profiler.py[335] - INFO: block block-6 (sparsity 0.4) size drop: 1459008B (1.391MB), FLOPs drop: 46.189M, param drop: 0.361M
2024-10-17 05:02:14,645 - server_block_profiler.py[307] - INFO: block block-6 (sparsity 0.6) acc drop: 0.00753331184387207
2024-10-17 05:02:14,659 - server_block_profiler.py[335] - INFO: block block-6 (sparsity 0.6) size drop: 2027456B (1.934MB), FLOPs drop: 64.168M, param drop: 0.501M
2024-10-17 05:02:14,660 - server_block_profiler.py[307] - INFO: block block-6 (sparsity 0.8) acc drop: 0.02486664056777954
2024-10-17 05:02:14,673 - server_block_profiler.py[335] - INFO: block block-6 (sparsity 0.8) size drop: 2549568B (2.431MB), FLOPs drop: 80.692M, param drop: 0.630M
2024-10-17 05:02:14,674 - server_block_profiler.py[307] - INFO: block block-7 (sparsity 0.0) acc drop: 0.00026665131251017254
2024-10-17 05:02:14,684 - server_block_profiler.py[335] - INFO: block block-7 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,685 - server_block_profiler.py[307] - INFO: block block-7 (sparsity 0.2) acc drop: 0.0005333224932352701
2024-10-17 05:02:14,695 - server_block_profiler.py[335] - INFO: block block-7 (sparsity 0.2) size drop: 477824B (0.456MB), FLOPs drop: 15.143M, param drop: 0.118M
2024-10-17 05:02:14,695 - server_block_profiler.py[307] - INFO: block block-7 (sparsity 0.4) acc drop: 0.0010666648546854656
2024-10-17 05:02:14,704 - server_block_profiler.py[335] - INFO: block block-7 (sparsity 0.4) size drop: 907840B (0.866MB), FLOPs drop: 28.762M, param drop: 0.225M
2024-10-17 05:02:14,704 - server_block_profiler.py[307] - INFO: block block-7 (sparsity 0.6) acc drop: 0.0012333194414774578
2024-10-17 05:02:14,713 - server_block_profiler.py[335] - INFO: block block-7 (sparsity 0.6) size drop: 1271872B (1.213MB), FLOPs drop: 40.297M, param drop: 0.315M
2024-10-17 05:02:14,713 - server_block_profiler.py[307] - INFO: block block-7 (sparsity 0.8) acc drop: 0.005466659863789876
2024-10-17 05:02:14,721 - server_block_profiler.py[335] - INFO: block block-7 (sparsity 0.8) size drop: 1599232B (1.525MB), FLOPs drop: 50.664M, param drop: 0.396M
2024-10-17 05:02:14,721 - server_block_profiler.py[307] - INFO: block block-8 (sparsity 0.0) acc drop: -9.999672571818034e-05
2024-10-17 05:02:14,750 - server_block_profiler.py[335] - INFO: block block-8 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,751 - server_block_profiler.py[307] - INFO: block block-8 (sparsity 0.2) acc drop: 0.0007666548093159994
2024-10-17 05:02:14,775 - server_block_profiler.py[335] - INFO: block block-8 (sparsity 0.2) size drop: 1896768B (1.809MB), FLOPs drop: 19.693M, param drop: 0.471M
2024-10-17 05:02:14,776 - server_block_profiler.py[307] - INFO: block block-8 (sparsity 0.4) acc drop: 0.004066665967305501
2024-10-17 05:02:14,796 - server_block_profiler.py[335] - INFO: block block-8 (sparsity 0.4) size drop: 3630784B (3.463MB), FLOPs drop: 38.076M, param drop: 0.902M
2024-10-17 05:02:14,796 - server_block_profiler.py[307] - INFO: block block-8 (sparsity 0.6) acc drop: 0.007899979750315348
2024-10-17 05:02:14,814 - server_block_profiler.py[335] - INFO: block block-8 (sparsity 0.6) size drop: 5076672B (4.841MB), FLOPs drop: 53.944M, param drop: 1.261M
2024-10-17 05:02:14,814 - server_block_profiler.py[307] - INFO: block block-8 (sparsity 0.8) acc drop: 0.018433332443237305
2024-10-17 05:02:14,829 - server_block_profiler.py[335] - INFO: block block-8 (sparsity 0.8) size drop: 6423680B (6.126MB), FLOPs drop: 68.428M, param drop: 1.595M
2024-10-17 05:02:14,830 - server_block_profiler.py[307] - INFO: block block-9 (sparsity 0.0) acc drop: -0.0005666613578796387
2024-10-17 05:02:14,846 - server_block_profiler.py[335] - INFO: block block-9 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,846 - server_block_profiler.py[307] - INFO: block block-9 (sparsity 0.2) acc drop: -0.0012333393096923828
2024-10-17 05:02:14,862 - server_block_profiler.py[335] - INFO: block block-9 (sparsity 0.2) size drop: 1216448B (1.160MB), FLOPs drop: 9.675M, param drop: 0.302M
2024-10-17 05:02:14,862 - server_block_profiler.py[307] - INFO: block block-9 (sparsity 0.4) acc drop: 0.0019666552543640137
2024-10-17 05:02:14,874 - server_block_profiler.py[335] - INFO: block block-9 (sparsity 0.4) size drop: 2570304B (2.451MB), FLOPs drop: 20.436M, param drop: 0.639M
2024-10-17 05:02:14,874 - server_block_profiler.py[307] - INFO: block block-9 (sparsity 0.6) acc drop: 0.0032999912897745767
2024-10-17 05:02:14,884 - server_block_profiler.py[335] - INFO: block block-9 (sparsity 0.6) size drop: 3576832B (3.411MB), FLOPs drop: 28.439M, param drop: 0.889M
2024-10-17 05:02:14,884 - server_block_profiler.py[307] - INFO: block block-9 (sparsity 0.8) acc drop: 0.008933345476786295
2024-10-17 05:02:14,893 - server_block_profiler.py[335] - INFO: block block-9 (sparsity 0.8) size drop: 4483264B (4.276MB), FLOPs drop: 35.649M, param drop: 1.114M
2024-10-17 05:02:14,893 - server_block_profiler.py[307] - INFO: block block-10 (sparsity 0.0) acc drop: -0.0004666646321614583
2024-10-17 05:02:14,929 - server_block_profiler.py[335] - INFO: block block-10 (sparsity 0.0) size drop: 0B (0.000MB), FLOPs drop: 0.000M, param drop: 0.000M
2024-10-17 05:02:14,930 - server_block_profiler.py[307] - INFO: block block-10 (sparsity 0.2) acc drop: -0.00016667445500691733
2024-10-17 05:02:14,957 - server_block_profiler.py[335] - INFO: block block-10 (sparsity 0.2) size drop: 3810112B (3.634MB), FLOPs drop: 30.418M, param drop: 0.951M
2024-10-17 05:02:14,958 - server_block_profiler.py[307] - INFO: block block-10 (sparsity 0.4) acc drop: 0.00096664826075236
2024-10-17 05:02:14,983 - server_block_profiler.py[335] - INFO: block block-10 (sparsity 0.4) size drop: 7100672B (6.772MB), FLOPs drop: 56.678M, param drop: 1.771M
2024-10-17 05:02:14,983 - server_block_profiler.py[307] - INFO: block block-10 (sparsity 0.6) acc drop: 0.004833300908406575
2024-10-17 05:02:15,000 - server_block_profiler.py[335] - INFO: block block-10 (sparsity 0.6) size drop: 10025408B (9.561MB), FLOPs drop: 80.011M, param drop: 2.500M
2024-10-17 05:02:15,000 - server_block_profiler.py[307] - INFO: block block-10 (sparsity 0.8) acc drop: 0.021266659100850422
2024-10-17 05:02:15,012 - server_block_profiler.py[335] - INFO: block block-10 (sparsity 0.8) size drop: 12343488B (11.772MB), FLOPs drop: 98.497M, param drop: 3.078M
2024-10-17 05:02:15,329 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 0, "id": "block-0", "latency": 0.0014578019189834593}
2024-10-17 05:02:15,662 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 1, "id": "block-1", "latency": 0.0016432108759880065}
2024-10-17 05:02:16,027 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 2, "id": "block-2", "latency": 0.0018209555172920221}
2024-10-17 05:02:16,361 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 3, "id": "block-3", "latency": 0.0016280012774467461}
2024-10-17 05:02:16,508 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 4, "id": "block-4", "latency": 0.00074995744407177}
2024-10-17 05:02:16,657 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 5, "id": "block-5", "latency": 0.0007415590435266494}
2024-10-17 05:02:16,987 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 6, "id": "block-6", "latency": 0.0016202409577369683}
2024-10-17 05:02:17,140 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 7, "id": "block-7", "latency": 0.0007997926402091981}
2024-10-17 05:02:17,470 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 8, "id": "block-8", "latency": 0.001626052483320235}
2024-10-17 05:02:17,637 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 9, "id": "block-9", "latency": 0.0009059347206354144}
2024-10-17 05:02:17,834 - edge_block_profiler.py[64] - INFO: raw block info: {"index": 10, "id": "block-10", "latency": 0.0009768057614564897}
2024-10-17 05:02:17,840 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2024-10-17 05:02:17,847 - pure_runtime.py[42] - DEBUG: load 0th block (block-0) (sparsity 0.0) from file
2024-10-17 05:02:17,856 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:17,865 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:17,876 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.0) from file
2024-10-17 05:02:17,880 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.0) from file
2024-10-17 05:02:17,885 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.0) from file
2024-10-17 05:02:17,896 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.0) from file
2024-10-17 05:02:17,902 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.0) from file
2024-10-17 05:02:17,916 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.0) from file
2024-10-17 05:02:17,923 - pure_runtime.py[42] - DEBUG: load 9th block (block-9) (sparsity 0.0) from file
2024-10-17 05:02:17,935 - pure_runtime.py[42] - DEBUG: load 10th block (block-10) (sparsity 0.0) from file
2024-10-17 05:02:19,134 - edge_block_profiler.py[126] - INFO: block block-0 (sparsity 0.0) latency rel drop: 0.000% (0.001s -> 0.001s)
2024-10-17 05:02:19,421 - edge_block_profiler.py[126] - INFO: block block-0 (sparsity 0.2) latency rel drop: 4.264% (0.001s -> 0.001s)
2024-10-17 05:02:19,710 - edge_block_profiler.py[126] - INFO: block block-0 (sparsity 0.4) latency rel drop: 1.912% (0.001s -> 0.001s)
2024-10-17 05:02:20,002 - edge_block_profiler.py[126] - INFO: block block-0 (sparsity 0.6) latency rel drop: 1.568% (0.001s -> 0.001s)
2024-10-17 05:02:20,293 - edge_block_profiler.py[126] - INFO: block block-0 (sparsity 0.8) latency rel drop: 2.265% (0.001s -> 0.001s)
2024-10-17 05:02:20,956 - edge_block_profiler.py[126] - INFO: block block-1 (sparsity 0.0) latency rel drop: 0.000% (0.002s -> 0.002s)
2024-10-17 05:02:21,282 - edge_block_profiler.py[126] - INFO: block block-1 (sparsity 0.2) latency rel drop: 4.180% (0.002s -> 0.002s)
2024-10-17 05:02:21,604 - edge_block_profiler.py[126] - INFO: block block-1 (sparsity 0.4) latency rel drop: 2.632% (0.002s -> 0.002s)
2024-10-17 05:02:21,930 - edge_block_profiler.py[126] - INFO: block block-1 (sparsity 0.6) latency rel drop: 2.445% (0.002s -> 0.002s)
2024-10-17 05:02:22,250 - edge_block_profiler.py[126] - INFO: block block-1 (sparsity 0.8) latency rel drop: 4.856% (0.002s -> 0.002s)
2024-10-17 05:02:22,968 - edge_block_profiler.py[126] - INFO: block block-2 (sparsity 0.0) latency rel drop: 0.000% (0.002s -> 0.002s)
2024-10-17 05:02:23,329 - edge_block_profiler.py[126] - INFO: block block-2 (sparsity 0.2) latency rel drop: -0.578% (0.002s -> 0.002s)
2024-10-17 05:02:23,700 - edge_block_profiler.py[126] - INFO: block block-2 (sparsity 0.4) latency rel drop: -7.884% (0.002s -> 0.002s)
2024-10-17 05:02:24,062 - edge_block_profiler.py[126] - INFO: block block-2 (sparsity 0.6) latency rel drop: -1.333% (0.002s -> 0.002s)
2024-10-17 05:02:24,426 - edge_block_profiler.py[126] - INFO: block block-2 (sparsity 0.8) latency rel drop: -3.123% (0.002s -> 0.002s)
2024-10-17 05:02:25,099 - edge_block_profiler.py[126] - INFO: block block-3 (sparsity 0.0) latency rel drop: 0.000% (0.002s -> 0.002s)
2024-10-17 05:02:25,424 - edge_block_profiler.py[126] - INFO: block block-3 (sparsity 0.2) latency rel drop: 4.278% (0.002s -> 0.002s)
2024-10-17 05:02:25,752 - edge_block_profiler.py[126] - INFO: block block-3 (sparsity 0.4) latency rel drop: 3.678% (0.002s -> 0.002s)
2024-10-17 05:02:26,077 - edge_block_profiler.py[126] - INFO: block block-3 (sparsity 0.6) latency rel drop: 5.401% (0.002s -> 0.002s)
2024-10-17 05:02:26,407 - edge_block_profiler.py[126] - INFO: block block-3 (sparsity 0.8) latency rel drop: 2.510% (0.002s -> 0.002s)
2024-10-17 05:02:26,699 - edge_block_profiler.py[126] - INFO: block block-4 (sparsity 0.0) latency rel drop: 0.000% (0.001s -> 0.001s)
2024-10-17 05:02:26,844 - edge_block_profiler.py[126] - INFO: block block-4 (sparsity 0.2) latency rel drop: 3.312% (0.001s -> 0.001s)
2024-10-17 05:02:26,983 - edge_block_profiler.py[126] - INFO: block block-4 (sparsity 0.4) latency rel drop: 9.039% (0.001s -> 0.001s)
2024-10-17 05:02:27,124 - edge_block_profiler.py[126] - INFO: block block-4 (sparsity 0.6) latency rel drop: 7.820% (0.001s -> 0.001s)
2024-10-17 05:02:27,264 - edge_block_profiler.py[126] - INFO: block block-4 (sparsity 0.8) latency rel drop: 9.679% (0.001s -> 0.001s)
2024-10-17 05:02:27,553 - edge_block_profiler.py[126] - INFO: block block-5 (sparsity 0.0) latency rel drop: 0.000% (0.001s -> 0.001s)
2024-10-17 05:02:27,701 - edge_block_profiler.py[126] - INFO: block block-5 (sparsity 0.2) latency rel drop: -0.158% (0.001s -> 0.001s)
2024-10-17 05:02:27,847 - edge_block_profiler.py[126] - INFO: block block-5 (sparsity 0.4) latency rel drop: 3.096% (0.001s -> 0.001s)
2024-10-17 05:02:27,992 - edge_block_profiler.py[126] - INFO: block block-5 (sparsity 0.6) latency rel drop: 5.284% (0.001s -> 0.001s)
2024-10-17 05:02:28,237 - edge_block_profiler.py[126] - INFO: block block-5 (sparsity 0.8) latency rel drop: 9.154% (0.001s -> 0.001s)
2024-10-17 05:02:28,886 - edge_block_profiler.py[126] - INFO: block block-6 (sparsity 0.0) latency rel drop: 0.000% (0.002s -> 0.002s)
2024-10-17 05:02:29,207 - edge_block_profiler.py[126] - INFO: block block-6 (sparsity 0.2) latency rel drop: 0.653% (0.002s -> 0.002s)
2024-10-17 05:02:29,531 - edge_block_profiler.py[126] - INFO: block block-6 (sparsity 0.4) latency rel drop: -1.624% (0.002s -> 0.002s)
2024-10-17 05:02:29,852 - edge_block_profiler.py[126] - INFO: block block-6 (sparsity 0.6) latency rel drop: 1.735% (0.002s -> 0.002s)
2024-10-17 05:02:30,172 - edge_block_profiler.py[126] - INFO: block block-6 (sparsity 0.8) latency rel drop: -0.547% (0.002s -> 0.002s)
2024-10-17 05:02:30,474 - edge_block_profiler.py[126] - INFO: block block-7 (sparsity 0.0) latency rel drop: 0.000% (0.001s -> 0.001s)
2024-10-17 05:02:30,619 - edge_block_profiler.py[126] - INFO: block block-7 (sparsity 0.2) latency rel drop: 3.272% (0.001s -> 0.001s)
2024-10-17 05:02:30,762 - edge_block_profiler.py[126] - INFO: block block-7 (sparsity 0.4) latency rel drop: 8.592% (0.001s -> 0.001s)
2024-10-17 05:02:30,904 - edge_block_profiler.py[126] - INFO: block block-7 (sparsity 0.6) latency rel drop: 11.633% (0.001s -> 0.001s)
2024-10-17 05:02:31,043 - edge_block_profiler.py[126] - INFO: block block-7 (sparsity 0.8) latency rel drop: 12.191% (0.001s -> 0.001s)
2024-10-17 05:02:31,719 - edge_block_profiler.py[126] - INFO: block block-8 (sparsity 0.0) latency rel drop: 0.000% (0.002s -> 0.002s)
2024-10-17 05:02:32,059 - edge_block_profiler.py[126] - INFO: block block-8 (sparsity 0.2) latency rel drop: -1.295% (0.002s -> 0.002s)
2024-10-17 05:02:32,385 - edge_block_profiler.py[126] - INFO: block block-8 (sparsity 0.4) latency rel drop: 4.244% (0.002s -> 0.002s)
2024-10-17 05:02:32,720 - edge_block_profiler.py[126] - INFO: block block-8 (sparsity 0.6) latency rel drop: 0.086% (0.002s -> 0.002s)
2024-10-17 05:02:33,053 - edge_block_profiler.py[126] - INFO: block block-8 (sparsity 0.8) latency rel drop: 1.704% (0.002s -> 0.002s)
2024-10-17 05:02:33,373 - edge_block_profiler.py[126] - INFO: block block-9 (sparsity 0.0) latency rel drop: 0.000% (0.001s -> 0.001s)
2024-10-17 05:02:33,531 - edge_block_profiler.py[126] - INFO: block block-9 (sparsity 0.2) latency rel drop: 3.147% (0.001s -> 0.001s)
2024-10-17 05:02:33,686 - edge_block_profiler.py[126] - INFO: block block-9 (sparsity 0.4) latency rel drop: 7.214% (0.001s -> 0.001s)
2024-10-17 05:02:33,831 - edge_block_profiler.py[126] - INFO: block block-9 (sparsity 0.6) latency rel drop: 16.213% (0.001s -> 0.001s)
2024-10-17 05:02:33,974 - edge_block_profiler.py[126] - INFO: block block-9 (sparsity 0.8) latency rel drop: 18.682% (0.001s -> 0.001s)
2024-10-17 05:02:34,358 - edge_block_profiler.py[126] - INFO: block block-10 (sparsity 0.0) latency rel drop: 0.000% (0.001s -> 0.001s)
2024-10-17 05:02:34,553 - edge_block_profiler.py[126] - INFO: block block-10 (sparsity 0.2) latency rel drop: 2.968% (0.001s -> 0.001s)
2024-10-17 05:02:34,740 - edge_block_profiler.py[126] - INFO: block block-10 (sparsity 0.4) latency rel drop: 6.970% (0.001s -> 0.001s)
2024-10-17 05:02:34,925 - edge_block_profiler.py[126] - INFO: block block-10 (sparsity 0.6) latency rel drop: 5.438% (0.001s -> 0.001s)
2024-10-17 05:02:35,110 - edge_block_profiler.py[126] - INFO: block block-10 (sparsity 0.8) latency rel drop: 4.886% (0.001s -> 0.001s)
2024-10-17 05:02:35,110 - optimal_runtime.py[21] - INFO: init adaptive model runtime
2024-10-17 05:02:35,118 - optimal_runtime.py[147] - INFO: load blocks metrics
2024-10-17 05:02:35,137 - optimal_runtime.py[176] - INFO: load model metrics
2024-10-17 05:02:35,143 - optimal_runtime.py[187] - INFO: load sparest blocks for initializing model
2024-10-17 05:02:35,144 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:35,150 - pure_runtime.py[42] - DEBUG: load 0th block (block-0) (sparsity 0.8) from file
2024-10-17 05:02:35,158 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.8) from file
2024-10-17 05:02:35,167 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.8) from file
2024-10-17 05:02:35,176 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.8) from file
2024-10-17 05:02:35,180 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.8) from file
2024-10-17 05:02:35,185 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.8) from file
2024-10-17 05:02:35,194 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.8) from file
2024-10-17 05:02:35,199 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.8) from file
2024-10-17 05:02:35,210 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.8) from file
2024-10-17 05:02:35,215 - pure_runtime.py[42] - DEBUG: load 9th block (block-9) (sparsity 0.8) from file
2024-10-17 05:02:35,222 - pure_runtime.py[42] - DEBUG: load 10th block (block-10) (sparsity 0.8) from file
2024-10-17 05:02:35,345 - gen_series_legodnn_models.py[17] - INFO: min model size: 1.060MB, max model size: 35.534MB
2024-10-17 05:02:35,346 - gen_series_legodnn_models.py[28] - INFO: target model size: 1.060MB
2024-10-17 05:02:35,346 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 1111252.0B (1.060MB), try to adapt blocks
2024-10-17 05:02:35,350 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:35,380 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016166208267211914
2024-10-17 05:02:35,381 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010329920165240765, 0.0007791360020637512, 0.000816032003611326, 0.000770143996924162, 0.0003303040005266666, 0.00033078400045633314, 0.0007710719965398309, 0.00033340800181031225, 0.0007873599976301192, 0.0003541759960353375, 0.0004931520037353039]
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.379
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.007
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.301
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.061
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.051
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.037
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.113
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.106
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.030
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.080
2024-10-17 05:02:35,381 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.884
2024-10-17 05:02:35,382 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:35,382 - optimal_runtime.py[116] - INFO: avg ratio: 2.0408842930289657
2024-10-17 05:02:35,382 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.001874949605273286
2024-10-17 05:02:35,383 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00105693 0.0008189  0.00079132 0.00078997 0.0003657  0.00036412
 0.00076688 0.0003797  0.00080101 0.00043554 0.00051848]
2024-10-17 05:02:35,386 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:35,531 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:35,531 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:35,534 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00105693 0.0008189  0.00079132 0.00078997 0.0003657  0.00036412
 0.00076688 0.0003797  0.00080101 0.00043554 0.00051848]
2024-10-17 05:02:35,536 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:35,682 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:35,682 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:35,685 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00105693 0.0008189  0.00079132 0.00078997 0.0003657  0.00036412
 0.00076688 0.0003797  0.00080101 0.00043554 0.00051848]
2024-10-17 05:02:35,688 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:35,837 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:35,837 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:35,853 - pure_runtime.py[42] - DEBUG: load 0th block (block-0) (sparsity 0.2) from file
2024-10-17 05:02:35,862 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.6) from file
2024-10-17 05:02:35,863 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 133438944.0,
  'blocks_sparsity': [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
  'esti_latency': 0.001583578512739634,
  'esti_test_accuracy': 0.6558666825294495,
  'is_relaxed': True,
  'model_size': 5713741.0,
  'update_swap_mem_cost': 0,
  'update_swap_time_cost': 0.025489091873168945}
2024-10-17 05:02:35,897 - gen_series_legodnn_models.py[28] - INFO: target model size: 1.408MB
2024-10-17 05:02:35,897 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 1476394.5555555555B (1.408MB), try to adapt blocks
2024-10-17 05:02:35,899 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:35,932 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016898303985595702
2024-10-17 05:02:35,932 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001033024001866579, 0.0008368640020489694, 0.0008623999990522862, 0.0007992639876902104, 0.0003547839969396591, 0.00034982399642467495, 0.0007995520010590553, 0.0003521919921040535, 0.0008430400006473066, 0.0003744639977812767, 0.0004987839981913567]
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.351
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.916
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.177
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.986
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.909
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.926
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.038
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.994
2024-10-17 05:02:35,932 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.896
2024-10-17 05:02:35,933 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.967
2024-10-17 05:02:35,933 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.863
2024-10-17 05:02:35,933 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:35,933 - optimal_runtime.py[116] - INFO: avg ratio: 1.9437568667161953
2024-10-17 05:02:35,933 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0019686388072227027
2024-10-17 05:02:35,934 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107903 0.00085784 0.00083629 0.00081984 0.0003928  0.00038507
 0.0007952  0.00040109 0.00085765 0.00046049 0.0005244 ]
2024-10-17 05:02:35,936 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:36,076 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:36,077 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:36,080 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107903 0.00085784 0.00083629 0.00081984 0.0003928  0.00038507
 0.0007952  0.00040109 0.00085765 0.00046049 0.0005244 ]
2024-10-17 05:02:36,082 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:36,226 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:36,227 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:36,234 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107903 0.00085784 0.00083629 0.00081984 0.0003928  0.00038507
 0.0007952  0.00040109 0.00085765 0.00046049 0.0005244 ]
2024-10-17 05:02:36,238 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:36,388 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:36,389 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:36,389 - gen_series_legodnn_models.py[28] - INFO: target model size: 1.756MB
2024-10-17 05:02:36,389 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 1841537.111111111B (1.756MB), try to adapt blocks
2024-10-17 05:02:36,391 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:36,431 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01810633659362793
2024-10-17 05:02:36,432 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001156384006142616, 0.0008746239952743054, 0.0008822720013558863, 0.0008643840067088602, 0.0003542400002479553, 0.00035372800379991534, 0.0008243840038776398, 0.0003680640049278737, 0.0008537279888987541, 0.0003710399977862835, 0.0005117760002613067]
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.207
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.833
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.128
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.836
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.912
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.904
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.976
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.908
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.872
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.985
2024-10-17 05:02:36,432 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.815
2024-10-17 05:02:36,433 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:36,433 - optimal_runtime.py[116] - INFO: avg ratio: 1.8936620850124828
2024-10-17 05:02:36,433 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020207170169950803
2024-10-17 05:02:36,434 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00120789 0.00089655 0.00085556 0.00088663 0.0003922  0.00038937
 0.0008199  0.00041916 0.00086853 0.00045628 0.00053806]
2024-10-17 05:02:36,436 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:36,580 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:36,581 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:36,586 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00120789 0.00089655 0.00085556 0.00088663 0.0003922  0.00038937
 0.0008199  0.00041916 0.00086853 0.00045628 0.00053806]
2024-10-17 05:02:36,590 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:36,734 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:36,734 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:36,737 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00120789 0.00089655 0.00085556 0.00088663 0.0003922  0.00038937
 0.0008199  0.00041916 0.00086853 0.00045628 0.00053806]
2024-10-17 05:02:36,739 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:36,891 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:36,893 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:36,893 - gen_series_legodnn_models.py[28] - INFO: target model size: 2.104MB
2024-10-17 05:02:36,894 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 2206679.666666667B (2.104MB), try to adapt blocks
2024-10-17 05:02:36,901 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:36,936 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017950719833374023
2024-10-17 05:02:36,937 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010947200022637844, 0.0008542080037295817, 0.0009114880003035069, 0.0008141760006546974, 0.0003524800017476082, 0.00035564799606800076, 0.0008458559960126878, 0.00036956799402832987, 0.0008537919968366624, 0.0003919679969549179, 0.000525952000170946]
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.275
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.877
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.060
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.949
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.922
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.894
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.926
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.900
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.872
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.879
2024-10-17 05:02:36,937 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.766
2024-10-17 05:02:36,938 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:36,938 - optimal_runtime.py[116] - INFO: avg ratio: 1.8873613464591668
2024-10-17 05:02:36,938 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002027462948100541
2024-10-17 05:02:36,938 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00114348 0.00087562 0.00088389 0.00083513 0.00039025 0.00039149
 0.00084125 0.00042088 0.00086859 0.00048202 0.00055297]
2024-10-17 05:02:36,940 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:37,081 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:37,081 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:37,084 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00114348 0.00087562 0.00088389 0.00083513 0.00039025 0.00039149
 0.00084125 0.00042088 0.00086859 0.00048202 0.00055297]
2024-10-17 05:02:37,086 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:37,232 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:37,233 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:37,241 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00114348 0.00087562 0.00088389 0.00083513 0.00039025 0.00039149
 0.00084125 0.00042088 0.00086859 0.00048202 0.00055297]
2024-10-17 05:02:37,244 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:37,396 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:37,398 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:37,399 - gen_series_legodnn_models.py[28] - INFO: target model size: 2.453MB
2024-10-17 05:02:37,399 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 2571822.222222222B (2.453MB), try to adapt blocks
2024-10-17 05:02:37,405 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:37,439 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017189056396484374
2024-10-17 05:02:37,439 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001089375987648964, 0.0008320959880948065, 0.0008435840010643003, 0.0008323199935257435, 0.0003402559980750084, 0.00033897600695490833, 0.0007723199948668479, 0.00037430400401353836, 0.0008241599984467028, 0.00035644800588488577, 0.0005088960044085979]
2024-10-17 05:02:37,439 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.281
2024-10-17 05:02:37,439 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.926
2024-10-17 05:02:37,439 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.226
2024-10-17 05:02:37,439 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.907
2024-10-17 05:02:37,439 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.991
2024-10-17 05:02:37,439 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.987
2024-10-17 05:02:37,440 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.109
2024-10-17 05:02:37,440 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.876
2024-10-17 05:02:37,440 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.939
2024-10-17 05:02:37,440 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.067
2024-10-17 05:02:37,440 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.826
2024-10-17 05:02:37,440 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:37,440 - optimal_runtime.py[116] - INFO: avg ratio: 1.9587742628367144
2024-10-17 05:02:37,440 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.001953545782290123
2024-10-17 05:02:37,441 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00113789 0.00085295 0.00081804 0.00085374 0.00037672 0.00037313
 0.00076812 0.00042627 0.00083845 0.00043834 0.00053504]
2024-10-17 05:02:37,443 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:37,585 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:37,585 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:37,588 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00113789 0.00085295 0.00081804 0.00085374 0.00037672 0.00037313
 0.00076812 0.00042627 0.00083845 0.00043834 0.00053504]
2024-10-17 05:02:37,594 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:37,741 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:37,742 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:37,745 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00113789 0.00085295 0.00081804 0.00085374 0.00037672 0.00037313
 0.00076812 0.00042627 0.00083845 0.00043834 0.00053504]
2024-10-17 05:02:37,747 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:37,899 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:37,901 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:37,901 - gen_series_legodnn_models.py[28] - INFO: target model size: 2.801MB
2024-10-17 05:02:37,902 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 2936964.777777778B (2.801MB), try to adapt blocks
2024-10-17 05:02:37,908 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:37,944 - optimal_runtime.py[77] - INFO: infer time of current model: 0.018049087524414063
2024-10-17 05:02:37,944 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011156799867749216, 0.0008506239987909795, 0.000922944001853466, 0.0008424960002303124, 0.0003548800013959408, 0.00035654400289058684, 0.0008505919985473156, 0.0003706240057945251, 0.0008380480110645294, 0.0003712320029735565, 0.0004848960004746914]
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.251
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.885
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.035
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.884
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.909
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.889
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.915
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.895
2024-10-17 05:02:37,944 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.907
2024-10-17 05:02:37,945 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.984
2024-10-17 05:02:37,945 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.916
2024-10-17 05:02:37,945 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:37,945 - optimal_runtime.py[116] - INFO: avg ratio: 1.9219047617396847
2024-10-17 05:02:37,945 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.001991022279459549
2024-10-17 05:02:37,946 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116537 0.00087194 0.000895   0.00086418 0.00039291 0.00039247
 0.00084596 0.00042208 0.00085257 0.00045652 0.0005098 ]
2024-10-17 05:02:37,948 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:38,091 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:38,091 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:38,094 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116537 0.00087194 0.000895   0.00086418 0.00039291 0.00039247
 0.00084596 0.00042208 0.00085257 0.00045652 0.0005098 ]
2024-10-17 05:02:38,096 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:38,240 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:38,241 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:38,247 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116537 0.00087194 0.000895   0.00086418 0.00039291 0.00039247
 0.00084596 0.00042208 0.00085257 0.00045652 0.0005098 ]
2024-10-17 05:02:38,251 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:38,400 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:38,400 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:38,401 - gen_series_legodnn_models.py[28] - INFO: target model size: 3.149MB
2024-10-17 05:02:38,401 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 3302107.3333333335B (3.149MB), try to adapt blocks
2024-10-17 05:02:38,403 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:38,443 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017578527450561522
2024-10-17 05:02:38,443 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001117568016052246, 0.0008499840050935745, 0.0008401600010693074, 0.000860864009708166, 0.0003745279982686043, 0.00035040000081062317, 0.0007722880020737649, 0.0003568000011146068, 0.0008773759901523592, 0.0003551359996199608, 0.0004688960053026676]
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.249
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.886
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.235
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.844
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.809
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.923
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.109
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.968
2024-10-17 05:02:38,443 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.822
2024-10-17 05:02:38,444 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.074
2024-10-17 05:02:38,444 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.981
2024-10-17 05:02:38,444 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:38,444 - optimal_runtime.py[116] - INFO: avg ratio: 1.935124097657447
2024-10-17 05:02:38,444 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0019774210885262202
2024-10-17 05:02:38,445 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116734 0.00087129 0.00081472 0.00088302 0.00041466 0.00038571
 0.00076809 0.00040634 0.00089258 0.00043672 0.00049298]
2024-10-17 05:02:38,447 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:38,588 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:38,589 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:38,592 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116734 0.00087129 0.00081472 0.00088302 0.00041466 0.00038571
 0.00076809 0.00040634 0.00089258 0.00043672 0.00049298]
2024-10-17 05:02:38,594 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:38,737 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:38,738 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:38,740 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116734 0.00087129 0.00081472 0.00088302 0.00041466 0.00038571
 0.00076809 0.00040634 0.00089258 0.00043672 0.00049298]
2024-10-17 05:02:38,743 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:38,892 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:38,893 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:38,893 - gen_series_legodnn_models.py[28] - INFO: target model size: 3.497MB
2024-10-17 05:02:38,893 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 3667249.888888889B (3.497MB), try to adapt blocks
2024-10-17 05:02:38,895 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:38,935 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017358848571777344
2024-10-17 05:02:38,935 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010982079952955247, 0.0008824320025742054, 0.0008433919996023179, 0.0008276479952037335, 0.000349727999418974, 0.0003319039978086948, 0.0007703040018677712, 0.0003704960010945797, 0.0008434239961206914, 0.00035424000397324564, 0.00047030400112271304]
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.271
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.817
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.227
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.918
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.937
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.030
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.115
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.896
2024-10-17 05:02:38,935 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.895
2024-10-17 05:02:38,936 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.080
2024-10-17 05:02:38,936 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.975
2024-10-17 05:02:38,936 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:38,936 - optimal_runtime.py[116] - INFO: avg ratio: 1.9623857488809502
2024-10-17 05:02:38,936 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0019499505649208878
2024-10-17 05:02:38,937 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00114712 0.00090455 0.00081785 0.00084895 0.00038721 0.00036535
 0.00076611 0.00042193 0.00085804 0.00043562 0.00049446]
2024-10-17 05:02:38,939 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:39,080 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:39,080 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:39,083 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00114712 0.00090455 0.00081785 0.00084895 0.00038721 0.00036535
 0.00076611 0.00042193 0.00085804 0.00043562 0.00049446]
2024-10-17 05:02:39,085 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:39,232 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:39,233 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:39,236 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00114712 0.00090455 0.00081785 0.00084895 0.00038721 0.00036535
 0.00076611 0.00042193 0.00085804 0.00043562 0.00049446]
2024-10-17 05:02:39,238 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:39,386 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:39,386 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:39,388 - gen_series_legodnn_models.py[28] - INFO: target model size: 3.846MB
2024-10-17 05:02:39,388 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 4032392.4444444445B (3.846MB), try to adapt blocks
2024-10-17 05:02:39,396 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:39,446 - optimal_runtime.py[77] - INFO: infer time of current model: 0.02610643196105957
2024-10-17 05:02:39,447 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001499551992863417, 0.0011820480041205882, 0.0012381760030984877, 0.0011446719802916052, 0.0004919679947197437, 0.0004912640005350112, 0.0011435840204358102, 0.0004847359992563725, 0.00114147200062871, 0.000492832001298666, 0.0006470400094985961]
2024-10-17 05:02:39,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.931
2024-10-17 05:02:39,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.356
2024-10-17 05:02:39,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.517
2024-10-17 05:02:39,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.387
2024-10-17 05:02:39,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.377
2024-10-17 05:02:39,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.371
2024-10-17 05:02:39,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.425
2024-10-17 05:02:39,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.449
2024-10-17 05:02:39,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.400
2024-10-17 05:02:39,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.495
2024-10-17 05:02:39,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.436
2024-10-17 05:02:39,450 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:39,451 - optimal_runtime.py[116] - INFO: avg ratio: 1.421179563063091
2024-10-17 05:02:39,451 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0026925205646608605
2024-10-17 05:02:39,453 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00156634 0.00121168 0.00120068 0.00117414 0.00054469 0.00054077
 0.00113736 0.00055203 0.00116126 0.00060605 0.00068027]
2024-10-17 05:02:39,460 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:39,603 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:39,603 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:39,610 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00156634 0.00121168 0.00120068 0.00117414 0.00054469 0.00054077
 0.00113736 0.00055203 0.00116126 0.00060605 0.00068027]
2024-10-17 05:02:39,615 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:39,767 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:39,768 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:39,774 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00156634 0.00121168 0.00120068 0.00117414 0.00054469 0.00054077
 0.00113736 0.00055203 0.00116126 0.00060605 0.00068027]
2024-10-17 05:02:39,779 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:39,940 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:39,941 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:39,942 - gen_series_legodnn_models.py[28] - INFO: target model size: 4.194MB
2024-10-17 05:02:39,942 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 4397535.0B (4.194MB), try to adapt blocks
2024-10-17 05:02:39,950 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:39,997 - optimal_runtime.py[77] - INFO: infer time of current model: 0.024661344528198244
2024-10-17 05:02:39,998 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0015101760029792785, 0.0011954880133271217, 0.001237536009401083, 0.0011514559984207155, 0.0004955840036273003, 0.0004942720010876656, 0.001137632004916668, 0.0004911680035293103, 0.00112940801307559, 0.0005049599930644035, 0.0006489920057356357]
2024-10-17 05:02:39,998 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.002 = 0.924
2024-10-17 05:02:39,998 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.341
2024-10-17 05:02:39,998 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.517
2024-10-17 05:02:39,998 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.378
2024-10-17 05:02:39,998 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.367
2024-10-17 05:02:39,998 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.363
2024-10-17 05:02:39,998 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.432
2024-10-17 05:02:39,999 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.430
2024-10-17 05:02:39,999 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.415
2024-10-17 05:02:39,999 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.459
2024-10-17 05:02:39,999 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.432
2024-10-17 05:02:39,999 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:39,999 - optimal_runtime.py[116] - INFO: avg ratio: 1.4133989893179906
2024-10-17 05:02:39,999 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002707342532818381
2024-10-17 05:02:40,000 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00157744 0.00122545 0.00120006 0.0011811  0.00054869 0.00054408
 0.00113144 0.00055936 0.00114899 0.00062097 0.00068233]
2024-10-17 05:02:40,003 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:40,151 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:40,152 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:40,159 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00157744 0.00122545 0.00120006 0.0011811  0.00054869 0.00054408
 0.00113144 0.00055936 0.00114899 0.00062097 0.00068233]
2024-10-17 05:02:40,164 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:40,307 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:40,308 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:40,320 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00157744 0.00122545 0.00120006 0.0011811  0.00054869 0.00054408
 0.00113144 0.00055936 0.00114899 0.00062097 0.00068233]
2024-10-17 05:02:40,323 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:40,480 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:40,482 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:40,483 - gen_series_legodnn_models.py[28] - INFO: target model size: 4.542MB
2024-10-17 05:02:40,483 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 4762677.555555556B (4.542MB), try to adapt blocks
2024-10-17 05:02:40,489 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:40,539 - optimal_runtime.py[77] - INFO: infer time of current model: 0.02447279930114746
2024-10-17 05:02:40,539 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0014871360100805761, 0.0011824320144951342, 0.0012495039999485017, 0.0011296000070869924, 0.00048777600005269053, 0.0004900479987263678, 0.001131232015788555, 0.00048335999995470047, 0.0011279359869658947, 0.0004864320009946823, 0.0006473599970340728]
2024-10-17 05:02:40,539 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.938
2024-10-17 05:02:40,539 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.356
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.503
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.405
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.389
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.375
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.440
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.453
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.417
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.514
2024-10-17 05:02:40,540 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.435
2024-10-17 05:02:40,541 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:40,541 - optimal_runtime.py[116] - INFO: avg ratio: 1.4286782485452298
2024-10-17 05:02:40,541 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002678388365973618
2024-10-17 05:02:40,541 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00155337 0.00121207 0.00121167 0.00115868 0.00054005 0.00053943
 0.00112508 0.00055047 0.00114749 0.00059818 0.00068061]
2024-10-17 05:02:40,544 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:40,694 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:40,695 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:40,705 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00155337 0.00121207 0.00121167 0.00115868 0.00054005 0.00053943
 0.00112508 0.00055047 0.00114749 0.00059818 0.00068061]
2024-10-17 05:02:40,709 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:40,858 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:40,859 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:40,866 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00155337 0.00121207 0.00121167 0.00115868 0.00054005 0.00053943
 0.00112508 0.00055047 0.00114749 0.00059818 0.00068061]
2024-10-17 05:02:40,869 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:41,032 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:41,033 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:41,034 - gen_series_legodnn_models.py[28] - INFO: target model size: 4.890MB
2024-10-17 05:02:41,034 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 5127820.111111111B (4.890MB), try to adapt blocks
2024-10-17 05:02:41,040 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:41,090 - optimal_runtime.py[77] - INFO: infer time of current model: 0.026436256408691405
2024-10-17 05:02:41,090 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0014725759848952294, 0.0011740160062909126, 0.0012254719920456408, 0.0011237759999930858, 0.0005103679969906807, 0.0004907839931547642, 0.001120288010686636, 0.00048444799706339836, 0.0011230400055646896, 0.0004912320040166378, 0.00064249599725008]
2024-10-17 05:02:41,090 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.948
2024-10-17 05:02:41,090 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.365
2024-10-17 05:02:41,090 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.532
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.412
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.327
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.373
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.454
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.450
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.423
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.500
2024-10-17 05:02:41,091 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.446
2024-10-17 05:02:41,092 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:41,092 - optimal_runtime.py[116] - INFO: avg ratio: 1.428277036126455
2024-10-17 05:02:41,092 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002679140742891786
2024-10-17 05:02:41,092 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00153816 0.00120344 0.00118837 0.0011527  0.00056506 0.00054024
 0.00111419 0.0005517  0.00114251 0.00060409 0.0006755 ]
2024-10-17 05:02:41,095 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:41,248 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:41,249 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:41,257 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00153816 0.00120344 0.00118837 0.0011527  0.00056506 0.00054024
 0.00111419 0.0005517  0.00114251 0.00060409 0.0006755 ]
2024-10-17 05:02:41,260 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:41,416 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:41,417 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:41,424 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00153816 0.00120344 0.00118837 0.0011527  0.00056506 0.00054024
 0.00111419 0.0005517  0.00114251 0.00060409 0.0006755 ]
2024-10-17 05:02:41,428 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:41,590 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:41,592 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:41,593 - gen_series_legodnn_models.py[28] - INFO: target model size: 5.238MB
2024-10-17 05:02:41,593 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 5492962.666666667B (5.238MB), try to adapt blocks
2024-10-17 05:02:41,599 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:41,647 - optimal_runtime.py[77] - INFO: infer time of current model: 0.024379743576049804
2024-10-17 05:02:41,647 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0014994240067899229, 0.0011777280084788798, 0.001225632004439831, 0.0011271359995007515, 0.0004920320063829422, 0.0005134720057249069, 0.0011251839995384215, 0.0004929599985480309, 0.001122144013643265, 0.0004912959970533848, 0.0006431999988853931]
2024-10-17 05:02:41,647 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.931
2024-10-17 05:02:41,647 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.361
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.532
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.408
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.377
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.312
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.448
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.425
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.424
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.499
2024-10-17 05:02:41,648 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.444
2024-10-17 05:02:41,649 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:41,649 - optimal_runtime.py[116] - INFO: avg ratio: 1.4109716179640794
2024-10-17 05:02:41,649 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002712000121692402
2024-10-17 05:02:41,649 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0015662  0.00120725 0.00118852 0.00115615 0.00054476 0.00056521
 0.00111906 0.0005614  0.0011416  0.00060417 0.00067624]
2024-10-17 05:02:41,652 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:41,810 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:41,810 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5611597B (5.352MB) and continue finding solution
2024-10-17 05:02:41,817 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0015662  0.00120725 0.00118852 0.00115615 0.00054476 0.00056521
 0.00111906 0.0005614  0.0011416  0.00060417 0.00067624]
2024-10-17 05:02:41,822 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:41,975 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:41,975 - optimal_runtime.py[312] - INFO: no solution found, relax the memory constraint to 5716454.6B (5.452MB) and continue finding solution
2024-10-17 05:02:41,982 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0015662  0.00120725 0.00118852 0.00115615 0.00054476 0.00056521
 0.00111906 0.0005614  0.0011416  0.00060417 0.00067624]
2024-10-17 05:02:41,988 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:42,150 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:42,152 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:42,152 - gen_series_legodnn_models.py[28] - INFO: target model size: 5.587MB
2024-10-17 05:02:42,152 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 5858105.222222222B (5.587MB), try to adapt blocks
2024-10-17 05:02:42,158 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:42,208 - optimal_runtime.py[77] - INFO: infer time of current model: 0.02624015998840332
2024-10-17 05:02:42,208 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0014594239890575408, 0.0011791040003299713, 0.0012456000037491322, 0.0011296640038490294, 0.0005071039907634259, 0.0004956160001456738, 0.0011295360065996646, 0.00048614399880170825, 0.0011347199864685535, 0.000491712000221014, 0.0006470720060169696]
2024-10-17 05:02:42,208 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.956
2024-10-17 05:02:42,208 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.360
2024-10-17 05:02:42,208 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.508
2024-10-17 05:02:42,208 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.405
2024-10-17 05:02:42,208 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.336
2024-10-17 05:02:42,209 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.359
2024-10-17 05:02:42,209 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.442
2024-10-17 05:02:42,209 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.445
2024-10-17 05:02:42,209 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.409
2024-10-17 05:02:42,209 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.498
2024-10-17 05:02:42,209 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.436
2024-10-17 05:02:42,209 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:42,209 - optimal_runtime.py[116] - INFO: avg ratio: 1.4196612456220232
2024-10-17 05:02:42,210 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.00269540019594358
2024-10-17 05:02:42,210 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00152442 0.00120866 0.00120788 0.00115874 0.00056145 0.00054556
 0.00112339 0.00055364 0.00115439 0.00060468 0.00068031]
2024-10-17 05:02:42,213 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:42,377 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:42,379 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.4, 0.4, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:42,397 - pure_runtime.py[42] - DEBUG: load 0th block (block-0) (sparsity 0.4) from file
2024-10-17 05:02:42,410 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.4) from file
2024-10-17 05:02:42,424 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.6) from file
2024-10-17 05:02:42,424 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 155285472.0,
  'blocks_sparsity': [0.4, 0.4, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
  'esti_latency': 0.002289962102340542,
  'esti_test_accuracy': 0.6622333327929179,
  'is_relaxed': False,
  'model_size': 5838477.0,
  'update_swap_mem_cost': 765974.0,
  'update_swap_time_cost': 0.04539847373962402}
2024-10-17 05:02:42,472 - gen_series_legodnn_models.py[28] - INFO: target model size: 5.935MB
2024-10-17 05:02:42,472 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 6223247.777777778B (5.935MB), try to adapt blocks
2024-10-17 05:02:42,475 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:42,526 - optimal_runtime.py[77] - INFO: infer time of current model: 0.024750015258789063
2024-10-17 05:02:42,526 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0014745599925518034, 0.0012301120050251485, 0.0012515840046107769, 0.001203903999179602, 0.0005109120123088359, 0.0004982720091938973, 0.0011715840101242064, 0.0004979199953377247, 0.001142080008983612, 0.0005166079998016357, 0.0006573120169341564]
2024-10-17 05:02:42,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.970
2024-10-17 05:02:42,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.301
2024-10-17 05:02:42,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.474
2024-10-17 05:02:42,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.318
2024-10-17 05:02:42,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.326
2024-10-17 05:02:42,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.352
2024-10-17 05:02:42,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.391
2024-10-17 05:02:42,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.410
2024-10-17 05:02:42,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.400
2024-10-17 05:02:42,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.426
2024-10-17 05:02:42,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.413
2024-10-17 05:02:42,527 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:42,527 - optimal_runtime.py[116] - INFO: avg ratio: 1.3707516956607435
2024-10-17 05:02:42,528 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0027915742958673443
2024-10-17 05:02:42,528 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0015033  0.00126337 0.00123512 0.00123489 0.00056566 0.00054848
 0.00116521 0.00056705 0.00116188 0.00063529 0.00069107]
2024-10-17 05:02:42,531 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:42,696 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:42,698 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.4, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:42,717 - pure_runtime.py[42] - DEBUG: load 0th block (block-0) (sparsity 0.2) from file
2024-10-17 05:02:42,730 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.2) from file
2024-10-17 05:02:42,745 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.4) from file
2024-10-17 05:02:42,758 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.6) from file
2024-10-17 05:02:42,759 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 209376864.0,
  'blocks_sparsity': [0.2, 0.2, 0.4, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],
  'esti_latency': 0.002365392109799967,
  'esti_test_accuracy': 0.6738333304723104,
  'is_relaxed': False,
  'model_size': 6206605.0,
  'update_swap_mem_cost': 1658952.0,
  'update_swap_time_cost': 0.06024813652038574}
2024-10-17 05:02:42,808 - gen_series_legodnn_models.py[28] - INFO: target model size: 6.283MB
2024-10-17 05:02:42,808 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 6588390.333333333B (6.283MB), try to adapt blocks
2024-10-17 05:02:42,811 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:42,856 - optimal_runtime.py[77] - INFO: infer time of current model: 0.023052640914916992
2024-10-17 05:02:42,856 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0014474240131676199, 0.0011381759978830812, 0.0012250560075044632, 0.0011128960028290748, 0.0004988480024039746, 0.0004822400026023388, 0.0010938240140676498, 0.0004836160093545913, 0.0011024320013821125, 0.0004890560060739518, 0.0006545279957354069]
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.964
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.383
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.604
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.384
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.358
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.397
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.489
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.452
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.450
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.506
2024-10-17 05:02:42,857 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.419
2024-10-17 05:02:42,858 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:42,858 - optimal_runtime.py[116] - INFO: avg ratio: 1.4265831470018224
2024-10-17 05:02:42,858 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0026823218875571225
2024-10-17 05:02:42,859 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00151189 0.00118782 0.00113553 0.00117644 0.00055231 0.00053083
 0.00108787 0.00055076 0.00112154 0.00060141 0.00068815]
2024-10-17 05:02:42,862 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:43,022 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:43,024 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.4, 0.6, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:43,040 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.4) from file
2024-10-17 05:02:43,051 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.6) from file
2024-10-17 05:02:43,062 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.6) from file
2024-10-17 05:02:43,063 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 190400736.0,
  'blocks_sparsity': [0.2, 0.4, 0.6, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8, 0.8],
  'esti_latency': 0.0021850337497882396,
  'esti_test_accuracy': 0.6880999803543091,
  'is_relaxed': False,
  'model_size': 6575821.0,
  'update_swap_mem_cost': 2161400.0,
  'update_swap_time_cost': 0.03899812698364258}
2024-10-17 05:02:43,099 - gen_series_legodnn_models.py[28] - INFO: target model size: 6.631MB
2024-10-17 05:02:43,099 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 6953532.888888889B (6.631MB), try to adapt blocks
2024-10-17 05:02:43,101 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:43,133 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01714995193481445
2024-10-17 05:02:43,134 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010558079928159713, 0.0008335039913654326, 0.0008887360058724881, 0.000850559998303652, 0.00036511999741196635, 0.0003533440008759499, 0.0008510080054402352, 0.00037151999771595, 0.0008694400005042555, 0.0003754879981279373, 0.00048444799706339836]
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.322
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.920
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.076
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.811
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.855
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.907
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.871
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.890
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.838
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.962
2024-10-17 05:02:43,134 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.918
2024-10-17 05:02:43,135 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:43,135 - optimal_runtime.py[116] - INFO: avg ratio: 1.8857014103723928
2024-10-17 05:02:43,135 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020292476733458197
2024-10-17 05:02:43,136 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00110283 0.00085604 0.00087705 0.00089913 0.00040425 0.00038895
 0.00086603 0.0004231  0.00088451 0.00046175 0.00050933]
2024-10-17 05:02:43,138 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:43,300 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:43,302 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8, 0.8]
2024-10-17 05:02:43,319 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:43,330 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:43,331 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 266063584.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.6, 0.8, 0.8, 0.6, 0.8, 0.8, 0.8, 0.8],
  'esti_latency': 0.001671181373659631,
  'esti_test_accuracy': 0.6933333277702332,
  'is_relaxed': False,
  'model_size': 6907405.0,
  'update_swap_mem_cost': 1061894.0,
  'update_swap_time_cost': 0.028850555419921875}
2024-10-17 05:02:43,366 - gen_series_legodnn_models.py[28] - INFO: target model size: 6.980MB
2024-10-17 05:02:43,366 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 7318675.444444445B (6.980MB), try to adapt blocks
2024-10-17 05:02:43,369 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:43,399 - optimal_runtime.py[77] - INFO: infer time of current model: 0.015949824333190917
2024-10-17 05:02:43,399 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009576960019767283, 0.0008103040084242821, 0.0008511679954826832, 0.0007818240001797675, 0.0003707839995622635, 0.0003599039986729622, 0.0007848320081830024, 0.00034291200339794155, 0.0007960000038146971, 0.00035334400832653045, 0.0004985600076615811]
2024-10-17 05:02:43,399 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.457
2024-10-17 05:02:43,399 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.028
2024-10-17 05:02:43,399 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.152
2024-10-17 05:02:43,399 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.970
2024-10-17 05:02:43,399 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.827
2024-10-17 05:02:43,400 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.872
2024-10-17 05:02:43,400 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.029
2024-10-17 05:02:43,400 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.048
2024-10-17 05:02:43,400 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.008
2024-10-17 05:02:43,400 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.085
2024-10-17 05:02:43,400 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.864
2024-10-17 05:02:43,400 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:43,400 - optimal_runtime.py[116] - INFO: avg ratio: 1.969942260539717
2024-10-17 05:02:43,400 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0019424707395102657
2024-10-17 05:02:43,401 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00100035 0.0008103  0.00084627 0.00082647 0.00041052 0.00039617
 0.00079869 0.00039052 0.0008098  0.00043452 0.00052417]
2024-10-17 05:02:43,403 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:43,559 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:43,561 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.2, 0.6, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8]
2024-10-17 05:02:43,579 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.2) from file
2024-10-17 05:02:43,585 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.6) from file
2024-10-17 05:02:43,591 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.6) from file
2024-10-17 05:02:43,592 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 253903456.0,
  'blocks_sparsity': [0.2, 0.2, 0.2, 0.6, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8],
  'esti_latency': 0.0015856696984360213,
  'esti_test_accuracy': 0.6986333330472311,
  'is_relaxed': False,
  'model_size': 7311629.0,
  'update_swap_mem_cost': 1848554.0,
  'update_swap_time_cost': 0.030857086181640625}
2024-10-17 05:02:43,630 - gen_series_legodnn_models.py[28] - INFO: target model size: 7.328MB
2024-10-17 05:02:43,630 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 7683818.0B (7.328MB), try to adapt blocks
2024-10-17 05:02:43,632 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:43,663 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01717852783203125
2024-10-17 05:02:43,663 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010445439890027046, 0.000865695994347334, 0.0008914559930562973, 0.0008352639935910701, 0.00039151999726891513, 0.00036015999689698215, 0.0008545279987156389, 0.0003879999928176403, 0.000864576004445553, 0.0003970560021698475, 0.0005195200033485889]
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.336
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.819
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.054
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.844
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.730
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.950
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.863
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.822
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.849
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.855
2024-10-17 05:02:43,664 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.788
2024-10-17 05:02:43,664 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:43,665 - optimal_runtime.py[116] - INFO: avg ratio: 1.835557955032949
2024-10-17 05:02:43,665 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020846823109730798
2024-10-17 05:02:43,665 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00109107 0.00090346 0.00088633 0.00088296 0.00043348 0.00038025
 0.00086961 0.00043908 0.00087956 0.00048827 0.0005462 ]
2024-10-17 05:02:43,667 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:43,827 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:43,829 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.4, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8]
2024-10-17 05:02:43,849 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:43,859 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.4) from file
2024-10-17 05:02:43,860 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 290748512.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.4, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8],
  'esti_latency': 0.001749715553314415,
  'esti_test_accuracy': 0.7019000053405762,
  'is_relaxed': False,
  'model_size': 7623117.0,
  'update_swap_mem_cost': 1584420.0,
  'update_swap_time_cost': 0.030744314193725586}
2024-10-17 05:02:43,896 - gen_series_legodnn_models.py[28] - INFO: target model size: 7.676MB
2024-10-17 05:02:43,896 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 8048960.555555556B (7.676MB), try to adapt blocks
2024-10-17 05:02:43,898 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:43,929 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01611302375793457
2024-10-17 05:02:43,929 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.000962239995598793, 0.0008154560029506682, 0.0008505280017852782, 0.0007908800020813942, 0.0003278400003910065, 0.0003471040055155754, 0.0007896960042417051, 0.0003554239943623543, 0.000790431994944811, 0.0003573440015316009, 0.0004608000069856644]
2024-10-17 05:02:43,929 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.450
2024-10-17 05:02:43,929 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.015
2024-10-17 05:02:43,929 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.153
2024-10-17 05:02:43,929 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.983
2024-10-17 05:02:43,929 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.066
2024-10-17 05:02:43,929 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.024
2024-10-17 05:02:43,930 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.016
2024-10-17 05:02:43,930 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.988
2024-10-17 05:02:43,930 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.022
2024-10-17 05:02:43,930 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.062
2024-10-17 05:02:43,930 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.016
2024-10-17 05:02:43,930 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:43,930 - optimal_runtime.py[116] - INFO: avg ratio: 2.0345433364527876
2024-10-17 05:02:43,930 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0018807931642757147
2024-10-17 05:02:43,931 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0010051  0.00081546 0.00084564 0.00082108 0.00036297 0.00036647
 0.00080364 0.00040221 0.00080413 0.00043944 0.00048447]
2024-10-17 05:02:43,933 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:44,100 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:44,102 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8]
2024-10-17 05:02:44,122 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:44,127 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.6) from file
2024-10-17 05:02:44,127 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 306391904.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8],
  'esti_latency': 0.0015797608882027804,
  'esti_test_accuracy': 0.7043333450953165,
  'is_relaxed': False,
  'model_size': 8045133.0,
  'update_swap_mem_cost': 1943790.0,
  'update_swap_time_cost': 0.024837732315063477}
2024-10-17 05:02:44,168 - gen_series_legodnn_models.py[28] - INFO: target model size: 8.024MB
2024-10-17 05:02:44,168 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 8414103.111111112B (8.024MB), try to adapt blocks
2024-10-17 05:02:44,170 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:44,201 - optimal_runtime.py[77] - INFO: infer time of current model: 0.0165928955078125
2024-10-17 05:02:44,202 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009669759906828404, 0.0008370880037546157, 0.0008840640112757682, 0.0008676160089671612, 0.0003536960035562515, 0.00035795200243592264, 0.0008556159883737564, 0.0003731200061738491, 0.0008549120016396047, 0.0003830400034785271, 0.00047536000609397887]
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.443
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.963
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.072
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.796
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.955
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.962
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.861
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.894
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.870
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.923
2024-10-17 05:02:44,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.954
2024-10-17 05:02:44,203 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:44,203 - optimal_runtime.py[116] - INFO: avg ratio: 1.908691384323055
2024-10-17 05:02:44,203 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020048056123961873
2024-10-17 05:02:44,203 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00101004 0.00083709 0.00087898 0.00090639 0.0003837  0.00037792
 0.00087072 0.00042224 0.00086973 0.00047104 0.00049978]
2024-10-17 05:02:44,206 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:44,388 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:44,390 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.4, 0.6, 0.8, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8]
2024-10-17 05:02:44,407 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.2) from file
2024-10-17 05:02:44,419 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.4) from file
2024-10-17 05:02:44,429 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.6) from file
2024-10-17 05:02:44,434 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.8) from file
2024-10-17 05:02:44,439 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.8) from file
2024-10-17 05:02:44,452 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.6) from file
2024-10-17 05:02:44,452 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 250751008.0,
  'blocks_sparsity': [0.2, 0.2, 0.4, 0.6, 0.8, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8],
  'esti_latency': 0.0016979764078115455,
  'esti_test_accuracy': 0.7059333523114523,
  'is_relaxed': False,
  'model_size': 8403085.0,
  'update_swap_mem_cost': 6888674.0,
  'update_swap_time_cost': 0.06193184852600098}
2024-10-17 05:02:44,490 - gen_series_legodnn_models.py[28] - INFO: target model size: 8.373MB
2024-10-17 05:02:44,490 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 8779245.666666668B (8.373MB), try to adapt blocks
2024-10-17 05:02:44,492 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:44,524 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016895999908447267
2024-10-17 05:02:44,524 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001049567997455597, 0.0008327040001749993, 0.0008907840028405189, 0.000850432001054287, 0.0003548479974269867, 0.000348191998898983, 0.0008226240016520023, 0.00036851199716329575, 0.0008834559954702854, 0.00037126400694251064, 0.0005094079934060574]
2024-10-17 05:02:44,524 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.330
2024-10-17 05:02:44,524 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.891
2024-10-17 05:02:44,524 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.205
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.811
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.909
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.935
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.935
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.918
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.839
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.984
2024-10-17 05:02:44,525 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.824
2024-10-17 05:02:44,525 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:44,525 - optimal_runtime.py[116] - INFO: avg ratio: 1.8939853303991454
2024-10-17 05:02:44,525 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020203721423843798
2024-10-17 05:02:44,526 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00109631 0.00086903 0.00082569 0.00089899 0.00039287 0.00038328
 0.00083714 0.00041702 0.00088421 0.00045656 0.00053557]
2024-10-17 05:02:44,528 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:44,685 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:44,687 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.2, 0.4, 0.8, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8]
2024-10-17 05:02:44,706 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:44,718 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.4) from file
2024-10-17 05:02:44,718 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 273133600.0,
  'blocks_sparsity': [0.2, 0.2, 0.2, 0.4, 0.8, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8],
  'esti_latency': 0.0016606585900895286,
  'esti_test_accuracy': 0.7102333704630533,
  'is_relaxed': False,
  'model_size': 8743117.0,
  'update_swap_mem_cost': 1673990.0,
  'update_swap_time_cost': 0.031629323959350586}
2024-10-17 05:02:44,754 - gen_series_legodnn_models.py[28] - INFO: target model size: 8.721MB
2024-10-17 05:02:44,754 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 9144388.222222222B (8.721MB), try to adapt blocks
2024-10-17 05:02:44,757 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:44,792 - optimal_runtime.py[77] - INFO: infer time of current model: 0.020210784912109377
2024-10-17 05:02:44,792 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010273919925093652, 0.0008280640020966528, 0.004319456048309803, 0.0008691199980676174, 0.0003496959991753101, 0.00033164799585938456, 0.0008502399921417236, 0.00038099200278520585, 0.0008525439947843552, 0.0003564159981906414, 0.00048038400337100024]
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.358
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.901
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.004 = 0.424
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.804
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.937
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.031
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.873
2024-10-17 05:02:44,792 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.855
2024-10-17 05:02:44,793 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.906
2024-10-17 05:02:44,793 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.067
2024-10-17 05:02:44,793 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.934
2024-10-17 05:02:44,793 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 0,  1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:44,793 - optimal_runtime.py[116] - INFO: avg ratio: 1.8666737077836468
2024-10-17 05:02:44,793 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002049932553111536
2024-10-17 05:02:44,794 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107315 0.00086418 0.00429461 0.0009023  0.00038717 0.00036507
 0.00086525 0.00043115 0.00085328 0.0004383  0.00050506]
2024-10-17 05:02:44,796 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:44,955 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:44,956 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8]
2024-10-17 05:02:44,973 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:44,979 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.6) from file
2024-10-17 05:02:44,984 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.6) from file
2024-10-17 05:02:44,984 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 310018336.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.4, 0.6, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8],
  'esti_latency': 0.0017738188224785366,
  'esti_test_accuracy': 0.7129666805267334,
  'is_relaxed': False,
  'model_size': 9122125.0,
  'update_swap_mem_cost': 1380970.0,
  'update_swap_time_cost': 0.02761101722717285}
2024-10-17 05:02:45,023 - gen_series_legodnn_models.py[28] - INFO: target model size: 9.069MB
2024-10-17 05:02:45,023 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 9509530.777777778B (9.069MB), try to adapt blocks
2024-10-17 05:02:45,025 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:45,057 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01665500831604004
2024-10-17 05:02:45,057 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010295040011405947, 0.0008494400046765805, 0.0008963200002908707, 0.0008339200019836427, 0.00036611199751496323, 0.00036639999598264694, 0.0008665280081331729, 0.00038051199913024907, 0.0008725759945809841, 0.00035974400490522385, 0.0004811200015246868]
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.356
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.934
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.043
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.880
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.888
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.917
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.837
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.857
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.862
2024-10-17 05:02:45,057 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.048
2024-10-17 05:02:45,058 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.931
2024-10-17 05:02:45,058 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8, 10]),)
2024-10-17 05:02:45,058 - optimal_runtime.py[116] - INFO: avg ratio: 1.9056890279073515
2024-10-17 05:02:45,058 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020079641240444523
2024-10-17 05:02:45,059 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107536 0.00084944 0.00089116 0.00086576 0.00039717 0.00038684
 0.00088182 0.0004306  0.00087332 0.00044239 0.00050583]
2024-10-17 05:02:45,061 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:45,227 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:45,228 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8]
2024-10-17 05:02:45,246 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:45,257 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:45,258 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 334664352.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8],
  'esti_latency': 0.0017000808431294368,
  'esti_test_accuracy': 0.7151000301043192,
  'is_relaxed': False,
  'model_size': 9502925.0,
  'update_swap_mem_cost': 2394822.0,
  'update_swap_time_cost': 0.029086589813232422}
2024-10-17 05:02:45,294 - gen_series_legodnn_models.py[28] - INFO: target model size: 9.417MB
2024-10-17 05:02:45,294 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 9874673.333333334B (9.417MB), try to adapt blocks
2024-10-17 05:02:45,297 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:45,430 - optimal_runtime.py[77] - INFO: infer time of current model: 0.11906463623046876
2024-10-17 05:02:45,431 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010510080046951772, 0.0009112320058047771, 0.000982015997171402, 0.0008919680006802081, 0.0003832319974899292, 0.00037331199645996096, 0.0008565759994089603, 0.00038297599554061893, 0.0009333759918808938, 0.0003816959932446479, 0.0005020799972116947]
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.328
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.803
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.854
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.747
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.804
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.881
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.859
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.845
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.741
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.930
2024-10-17 05:02:45,431 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.850
2024-10-17 05:02:45,432 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:45,432 - optimal_runtime.py[116] - INFO: avg ratio: 1.8315339255697765
2024-10-17 05:02:45,432 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002089262528092509
2024-10-17 05:02:45,432 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00109782 0.00091123 0.00098202 0.00093183 0.00041574 0.00039414
 0.0008717  0.00043339 0.00093418 0.00046939 0.00052787]
2024-10-17 05:02:45,434 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:45,599 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:45,601 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.2, 0.4, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.6]
2024-10-17 05:02:45,618 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.2) from file
2024-10-17 05:02:45,630 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:45,640 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.4) from file
2024-10-17 05:02:45,645 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.8) from file
2024-10-17 05:02:45,656 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.8) from file
2024-10-17 05:02:45,666 - pure_runtime.py[42] - DEBUG: load 10th block (block-10) (sparsity 0.6) from file
2024-10-17 05:02:45,667 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 281921888.0,
  'blocks_sparsity': [0.2, 0.2, 0.2, 0.4, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.6],
  'esti_latency': 0.0017168620880404144,
  'esti_test_accuracy': 0.7177667021751404,
  'is_relaxed': False,
  'model_size': 9866125.0,
  'update_swap_mem_cost': 12765502.0,
  'update_swap_time_cost': 0.06585931777954102}
2024-10-17 05:02:45,705 - gen_series_legodnn_models.py[28] - INFO: target model size: 9.765MB
2024-10-17 05:02:45,705 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 10239815.88888889B (9.765MB), try to adapt blocks
2024-10-17 05:02:45,707 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:45,737 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01588707160949707
2024-10-17 05:02:45,738 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009739199876785279, 0.0007796160019934178, 0.0008455360084772107, 0.000788960002362728, 0.0003387200012803078, 0.0003395839966833591, 0.0007888000085949896, 0.00035356800258159635, 0.0008145280033349991, 0.00037836800143122674, 0.0005524160005152225]
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.433
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.020
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.166
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.988
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.000
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.068
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.018
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.999
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.962
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.947
2024-10-17 05:02:45,738 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.672
2024-10-17 05:02:45,739 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:02:45,739 - optimal_runtime.py[116] - INFO: avg ratio: 2.000250257028377
2024-10-17 05:02:45,739 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0019130382241809767
2024-10-17 05:02:45,739 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0010173  0.00081362 0.00084067 0.00081908 0.00037502 0.00035853
 0.00080272 0.00040011 0.00082865 0.00046529 0.00058418]
2024-10-17 05:02:45,741 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:45,907 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:45,909 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.6]
2024-10-17 05:02:45,926 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:45,937 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:45,938 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 320091872.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.8, 0.6, 0.6, 0.6, 0.8, 0.8, 0.6],
  'esti_latency': 0.0015909593605258332,
  'esti_test_accuracy': 0.7202333807945251,
  'is_relaxed': False,
  'model_size': 10211213.0,
  'update_swap_mem_cost': 2090852.0,
  'update_swap_time_cost': 0.02852344512939453}
2024-10-17 05:02:45,976 - gen_series_legodnn_models.py[28] - INFO: target model size: 10.114MB
2024-10-17 05:02:45,976 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 10604958.444444444B (10.114MB), try to adapt blocks
2024-10-17 05:02:45,978 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:46,009 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01658956718444824
2024-10-17 05:02:46,010 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009573119878768918, 0.0008389440067112445, 0.0008712640032172203, 0.0008568960018455982, 0.0003950399979948998, 0.000360992006957531, 0.0007940159998834132, 0.0003537600003182888, 0.0008624640032649039, 0.000401663988828659, 0.0005423359982669354]
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.458
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.959
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.102
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.819
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.715
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.946
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.005
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.998
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.853
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.834
2024-10-17 05:02:46,010 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.703
2024-10-17 05:02:46,011 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:46,011 - optimal_runtime.py[116] - INFO: avg ratio: 1.8701245575832581
2024-10-17 05:02:46,011 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002046149912371679
2024-10-17 05:02:46,011 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00099995 0.00083894 0.00086625 0.00089519 0.00043737 0.00038113
 0.00080803 0.00040033 0.00087741 0.00049394 0.00057353]
2024-10-17 05:02:46,013 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:46,187 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:46,189 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.8, 0.2, 0.6, 0.6, 0.8, 0.8, 0.6]
2024-10-17 05:02:46,204 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:46,205 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 331729888.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.8, 0.2, 0.6, 0.6, 0.8, 0.8, 0.6],
  'esti_latency': 0.001729495288304303,
  'esti_test_accuracy': 0.7218667070070902,
  'is_relaxed': False,
  'model_size': 10580301.0,
  'update_swap_mem_cost': 923836.0,
  'update_swap_time_cost': 0.015228509902954102}
2024-10-17 05:02:46,243 - gen_series_legodnn_models.py[28] - INFO: target model size: 10.462MB
2024-10-17 05:02:46,243 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 10970101.0B (10.462MB), try to adapt blocks
2024-10-17 05:02:46,245 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:46,278 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01715376091003418
2024-10-17 05:02:46,279 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010364160016179084, 0.0008786880075931549, 0.0009107519946992398, 0.0008961280062794685, 0.00035353600233793263, 0.00042665600031614303, 0.0008387839943170548, 0.00037228799238801, 0.0008603840023279191, 0.0003557439967989922, 0.0005137600004673005]
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.347
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.870
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.011
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.739
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.916
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.741
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.898
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.898
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.858
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.071
2024-10-17 05:02:46,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.798
2024-10-17 05:02:46,280 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8, 10]),)
2024-10-17 05:02:46,280 - optimal_runtime.py[116] - INFO: avg ratio: 1.8587767134954265
2024-10-17 05:02:46,280 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020586416710737018
2024-10-17 05:02:46,280 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108258 0.00087869 0.00090551 0.00093618 0.00039142 0.00042598
 0.00085359 0.0004213  0.0008753  0.00043747 0.00054331]
2024-10-17 05:02:46,283 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:46,452 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:46,453 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.4, 0.4, 0.8, 0.8, 0.6, 0.6, 0.6, 0.8, 0.6]
2024-10-17 05:02:46,470 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.2) from file
2024-10-17 05:02:46,481 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.4) from file
2024-10-17 05:02:46,492 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.4) from file
2024-10-17 05:02:46,497 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.8) from file
2024-10-17 05:02:46,510 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.6) from file
2024-10-17 05:02:46,511 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 278769440.0,
  'blocks_sparsity': [0.2, 0.2, 0.4, 0.4, 0.8, 0.8, 0.6, 0.6, 0.6, 0.8, 0.6],
  'esti_latency': 0.0017599995838103177,
  'esti_test_accuracy': 0.7250667214393616,
  'is_relaxed': False,
  'model_size': 10957581.0,
  'update_swap_mem_cost': 7078630.0,
  'update_swap_time_cost': 0.05726289749145508}
2024-10-17 05:02:46,549 - gen_series_legodnn_models.py[28] - INFO: target model size: 10.810MB
2024-10-17 05:02:46,549 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 11335243.555555556B (10.810MB), try to adapt blocks
2024-10-17 05:02:46,551 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:46,584 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01860966491699219
2024-10-17 05:02:46,585 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010371840000152587, 0.0008336959928274156, 0.0008947839960455893, 0.00245894406363368, 0.00035276799649000166, 0.0003523199930787086, 0.0008327679969370365, 0.00037280000373721123, 0.0009128959961235524, 0.00037759999930858613, 0.0005620160028338432]
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.346
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.889
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.196
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.002 = 0.638
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.920
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.912
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.912
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.896
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.780
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.951
2024-10-17 05:02:46,585 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.644
2024-10-17 05:02:46,586 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 0,  1,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:46,586 - optimal_runtime.py[116] - INFO: avg ratio: 1.8053697049206217
2024-10-17 05:02:46,586 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002119541049788112
2024-10-17 05:02:46,586 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108338 0.00087006 0.00082939 0.00255283 0.00039057 0.00038782
 0.00084747 0.00042188 0.00091368 0.00046435 0.00059434]
2024-10-17 05:02:46,588 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:46,744 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:46,745 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.4, 0.8, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6]
2024-10-17 05:02:46,762 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:46,773 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:46,778 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.6) from file
2024-10-17 05:02:46,779 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 323718304.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.4, 0.8, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6],
  'esti_latency': 0.0017423371818348164,
  'esti_test_accuracy': 0.728866716225942,
  'is_relaxed': False,
  'model_size': 11288205.0,
  'update_swap_mem_cost': 1617602.0,
  'update_swap_time_cost': 0.033204078674316406}
2024-10-17 05:02:46,818 - gen_series_legodnn_models.py[28] - INFO: target model size: 11.158MB
2024-10-17 05:02:46,818 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 11700386.111111112B (11.158MB), try to adapt blocks
2024-10-17 05:02:46,820 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:46,853 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017258495330810548
2024-10-17 05:02:46,853 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010231040008366109, 0.0008660160079598428, 0.000917151991277933, 0.0008583999946713448, 0.00036089599877595904, 0.0003685440048575401, 0.0008551360033452512, 0.0003807359896600246, 0.0009095359928905964, 0.000393695991486311, 0.0005484799928963184]
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.364
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.897
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.997
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.827
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.877
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.906
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.862
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.856
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.786
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.871
2024-10-17 05:02:46,853 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.684
2024-10-17 05:02:46,854 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:46,854 - optimal_runtime.py[116] - INFO: avg ratio: 1.840737760076179
2024-10-17 05:02:46,854 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020788160500737197
2024-10-17 05:02:46,855 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00106867 0.00086602 0.00091188 0.00089117 0.00039957 0.00038911
 0.00087023 0.00043086 0.00091032 0.00048414 0.00058002]
2024-10-17 05:02:46,857 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:47,038 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:47,039 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.8, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6]
2024-10-17 05:02:47,059 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:47,070 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:47,070 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 348364320.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.8, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6],
  'esti_latency': 0.0017479035311904012,
  'esti_test_accuracy': 0.7310000658035278,
  'is_relaxed': False,
  'model_size': 11669005.0,
  'update_swap_mem_cost': 2394822.0,
  'update_swap_time_cost': 0.030733108520507812}
2024-10-17 05:02:47,110 - gen_series_legodnn_models.py[28] - INFO: target model size: 11.507MB
2024-10-17 05:02:47,110 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 12065528.666666666B (11.507MB), try to adapt blocks
2024-10-17 05:02:47,112 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:47,143 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01642585563659668
2024-10-17 05:02:47,143 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009488640017807484, 0.0008083520047366618, 0.0008621760085225107, 0.0008264960013329982, 0.0003428480066359043, 0.00037344000115990643, 0.0008352319896221159, 0.0003767040073871612, 0.0008627520054578782, 0.0003680320009589196, 0.0005311999991536141]
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.471
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.033
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.112
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.885
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.976
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.881
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.906
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.876
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.883
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.002
2024-10-17 05:02:47,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.739
2024-10-17 05:02:47,144 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:47,145 - optimal_runtime.py[116] - INFO: avg ratio: 1.908985406842962
2024-10-17 05:02:47,145 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020044968316187298
2024-10-17 05:02:47,145 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00099112 0.00080835 0.00086218 0.00086343 0.00037959 0.00039428
 0.00084998 0.00042629 0.00086349 0.00045258 0.00056175]
2024-10-17 05:02:47,147 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:47,316 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:47,318 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.4, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:47,337 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:47,348 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.4) from file
2024-10-17 05:02:47,353 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.8) from file
2024-10-17 05:02:47,360 - pure_runtime.py[42] - DEBUG: load 9th block (block-9) (sparsity 0.6) from file
2024-10-17 05:02:47,360 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 326141376.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.4, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0016936390593182645,
  'esti_test_accuracy': 0.7328667442003886,
  'is_relaxed': False,
  'model_size': 12042701.0,
  'update_swap_mem_cost': 4887678.0,
  'update_swap_time_cost': 0.04253029823303223}
2024-10-17 05:02:47,401 - gen_series_legodnn_models.py[28] - INFO: target model size: 11.855MB
2024-10-17 05:02:47,401 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 12430671.222222222B (11.855MB), try to adapt blocks
2024-10-17 05:02:47,403 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:47,436 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01767363166809082
2024-10-17 05:02:47,437 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011021119840443133, 0.0008803200051188468, 0.0009213759936392309, 0.000864256002008915, 0.0003654079996049405, 0.0003590399920940399, 0.0008722559958696365, 0.00037967999652028085, 0.0009127359949052333, 0.0004093439914286137, 0.0005867199972271919]
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.266
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.867
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.988
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.814
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.854
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.876
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.825
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.861
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.780
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.854
2024-10-17 05:02:47,437 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.574
2024-10-17 05:02:47,438 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:02:47,438 - optimal_runtime.py[116] - INFO: avg ratio: 1.8415200500781497
2024-10-17 05:02:47,438 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020779329551479596
2024-10-17 05:02:47,438 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0011512  0.00088032 0.00091608 0.00089725 0.00040457 0.00039522
 0.00088765 0.00042966 0.00091352 0.00048855 0.00062046]
2024-10-17 05:02:47,440 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:47,597 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:47,599 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.2, 0.2, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:47,617 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.2) from file
2024-10-17 05:02:47,628 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:47,633 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.6) from file
2024-10-17 05:02:47,634 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 314472128.0,
  'blocks_sparsity': [0.2, 0.2, 0.2, 0.2, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0017198127448197805,
  'esti_test_accuracy': 0.7358334263165792,
  'is_relaxed': False,
  'model_size': 12389581.0,
  'update_swap_mem_cost': 2493664.0,
  'update_swap_time_cost': 0.034714460372924805}
2024-10-17 05:02:47,675 - gen_series_legodnn_models.py[28] - INFO: target model size: 12.203MB
2024-10-17 05:02:47,676 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 12795813.777777778B (12.203MB), try to adapt blocks
2024-10-17 05:02:47,678 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:47,709 - optimal_runtime.py[77] - INFO: infer time of current model: 0.0165053768157959
2024-10-17 05:02:47,709 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010169920027256012, 0.0008711359985172748, 0.000888031993061304, 0.0008194559998810292, 0.00033366400003433225, 0.0003434239998459816, 0.0007888960018754006, 0.00038406400009989745, 0.000904736001044512, 0.0004341120012104512, 0.0005566080026328564]
2024-10-17 05:02:47,709 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.372
2024-10-17 05:02:47,709 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.807
2024-10-17 05:02:47,709 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.062
2024-10-17 05:02:47,709 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.902
2024-10-17 05:02:47,709 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.030
2024-10-17 05:02:47,710 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.045
2024-10-17 05:02:47,710 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.018
2024-10-17 05:02:47,710 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.840
2024-10-17 05:02:47,710 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.796
2024-10-17 05:02:47,710 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.749
2024-10-17 05:02:47,710 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.659
2024-10-17 05:02:47,710 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:47,710 - optimal_runtime.py[116] - INFO: avg ratio: 1.8501700416913711
2024-10-17 05:02:47,710 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002068218116927774
2024-10-17 05:02:47,711 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00106229 0.00090914 0.00088292 0.00085608 0.00036942 0.00036258
 0.00080282 0.00043462 0.00090551 0.00051811 0.00058862]
2024-10-17 05:02:47,713 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:47,874 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:47,876 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:47,890 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:47,891 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 326110144.0,
  'blocks_sparsity': [0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0017369542481529723,
  'esti_test_accuracy': 0.7374667525291443,
  'is_relaxed': False,
  'model_size': 12758669.0,
  'update_swap_mem_cost': 923836.0,
  'update_swap_time_cost': 0.014330625534057617}
2024-10-17 05:02:47,932 - gen_series_legodnn_models.py[28] - INFO: target model size: 12.551MB
2024-10-17 05:02:47,932 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 13160956.333333334B (12.551MB), try to adapt blocks
2024-10-17 05:02:47,934 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:47,972 - optimal_runtime.py[77] - INFO: infer time of current model: 0.018148000717163087
2024-10-17 05:02:47,972 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011326720155775546, 0.0008599039986729621, 0.000931295994669199, 0.0009248959980905054, 0.0003656320013105869, 0.0004028480052947998, 0.000863423991948366, 0.00038169599324464797, 0.000919423993676901, 0.0004189760014414787, 0.0005551679879426956]
2024-10-17 05:02:47,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.232
2024-10-17 05:02:47,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.831
2024-10-17 05:02:47,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.967
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.685
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.853
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.844
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.844
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.852
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.767
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.812
2024-10-17 05:02:47,973 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.664
2024-10-17 05:02:47,973 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:47,973 - optimal_runtime.py[116] - INFO: avg ratio: 1.7944859847338155
2024-10-17 05:02:47,973 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021323962584141997
2024-10-17 05:02:47,974 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00118312 0.00089741 0.00092594 0.00096623 0.00040481 0.00040221
 0.00087867 0.00043194 0.00092021 0.00050005 0.00058709]
2024-10-17 05:02:47,976 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:48,136 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:48,138 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.6, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:48,156 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:48,167 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:48,173 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.6) from file
2024-10-17 05:02:48,173 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 371997504.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.6, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0018303184506936149,
  'esti_test_accuracy': 0.73880006869634,
  'is_relaxed': False,
  'model_size': 13096525.0,
  'update_swap_mem_cost': 1844738.0,
  'update_swap_time_cost': 0.03516101837158203}
2024-10-17 05:02:48,213 - gen_series_legodnn_models.py[28] - INFO: target model size: 12.899MB
2024-10-17 05:02:48,213 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 13526098.88888889B (12.899MB), try to adapt blocks
2024-10-17 05:02:48,216 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:48,247 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016783103942871094
2024-10-17 05:02:48,249 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009761600047349932, 0.0008357439897954466, 0.0008660480007529257, 0.0009168639965355396, 0.0003808639980852604, 0.00037273600324988363, 0.0008628799952566624, 0.0003877119980752468, 0.0008943680003285408, 0.00040790399536490436, 0.0005331199988722801]
2024-10-17 05:02:48,249 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.430
2024-10-17 05:02:48,249 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.966
2024-10-17 05:02:48,249 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.103
2024-10-17 05:02:48,249 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.700
2024-10-17 05:02:48,249 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.815
2024-10-17 05:02:48,249 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.993
2024-10-17 05:02:48,250 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.845
2024-10-17 05:02:48,250 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.823
2024-10-17 05:02:48,250 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.817
2024-10-17 05:02:48,250 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.861
2024-10-17 05:02:48,250 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.733
2024-10-17 05:02:48,250 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:48,250 - optimal_runtime.py[116] - INFO: avg ratio: 1.8390705231071505
2024-10-17 05:02:48,250 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002080700631946435
2024-10-17 05:02:48,251 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00101964 0.00083574 0.00086605 0.00095784 0.00041317 0.00037215
 0.00087811 0.00043875 0.00089514 0.00048683 0.00056378]
2024-10-17 05:02:48,253 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:48,429 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:48,431 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.2, 0.2, 0.2, 0.0, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:48,448 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.2) from file
2024-10-17 05:02:48,459 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:48,465 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.0) from file
2024-10-17 05:02:48,465 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 349317312.0,
  'blocks_sparsity': [0.2, 0.2, 0.2, 0.2, 0.0, 0.2, 0.6, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0017902875665898954,
  'esti_test_accuracy': 0.7398667534192404,
  'is_relaxed': False,
  'model_size': 13494861.0,
  'update_swap_mem_cost': 2580930.0,
  'update_swap_time_cost': 0.03422117233276367}
2024-10-17 05:02:48,506 - gen_series_legodnn_models.py[28] - INFO: target model size: 13.248MB
2024-10-17 05:02:48,506 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 13891241.444444444B (13.248MB), try to adapt blocks
2024-10-17 05:02:48,509 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:48,540 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017335391998291014
2024-10-17 05:02:48,541 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001060800015926361, 0.0008476160019636154, 0.0009101119972765446, 0.0008860799856483935, 0.00042783999815583225, 0.0003941119946539402, 0.000834623996168375, 0.0003726720027625561, 0.0008905280083417892, 0.00043100799620151523, 0.0005665920078754426]
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.316
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.858
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.012
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.759
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.753
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.885
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.908
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.896
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.824
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.761
2024-10-17 05:02:48,541 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.630
2024-10-17 05:02:48,542 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:48,542 - optimal_runtime.py[116] - INFO: avg ratio: 1.8081744139236275
2024-10-17 05:02:48,542 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021162533714431443
2024-10-17 05:02:48,542 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00110805 0.00088459 0.00090488 0.00092568 0.00042784 0.00039349
 0.00084936 0.00042173 0.00089129 0.00051441 0.00059918]
2024-10-17 05:02:48,544 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:48,708 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:48,710 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.8, 0.4, 0.2, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:48,728 - pure_runtime.py[42] - DEBUG: load 1th block (block-1) (sparsity 0.0) from file
2024-10-17 05:02:48,734 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.8) from file
2024-10-17 05:02:48,739 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.4) from file
2024-10-17 05:02:48,752 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.2) from file
2024-10-17 05:02:48,752 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 386248256.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.8, 0.4, 0.2, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0018096917488297024,
  'esti_test_accuracy': 0.7423000733057658,
  'is_relaxed': False,
  'model_size': 13869773.0,
  'update_swap_mem_cost': 5691612.0,
  'update_swap_time_cost': 0.04173636436462402}
2024-10-17 05:02:48,792 - gen_series_legodnn_models.py[28] - INFO: target model size: 13.596MB
2024-10-17 05:02:48,793 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 14256384.0B (13.596MB), try to adapt blocks
2024-10-17 05:02:48,795 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:48,827 - optimal_runtime.py[77] - INFO: infer time of current model: 0.0169268798828125
2024-10-17 05:02:48,828 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010037120021879674, 0.0008618559986352921, 0.0008851200044155121, 0.0008787199892103672, 0.00035008000209927557, 0.0003808320052921772, 0.0009483519978821278, 0.00041033599898219103, 0.0009120959825813771, 0.0004100160077214241, 0.000514816004782915]
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.390
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.907
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.069
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.773
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.935
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.887
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.697
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.722
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.781
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.851
2024-10-17 05:02:48,828 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.794
2024-10-17 05:02:48,829 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:48,829 - optimal_runtime.py[116] - INFO: avg ratio: 1.8164751697718526
2024-10-17 05:02:48,829 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021065827176177253
2024-10-17 05:02:48,829 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00104841 0.00086186 0.00088003 0.00091799 0.0003876  0.000393
 0.00095459 0.00046435 0.00091288 0.00048935 0.00054442]
2024-10-17 05:02:48,831 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:48,988 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:48,990 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:49,002 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.6) from file
2024-10-17 05:02:49,009 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:49,009 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 396922176.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0018280308924725575,
  'esti_test_accuracy': 0.743766725063324,
  'is_relaxed': False,
  'model_size': 14208653.0,
  'update_swap_mem_cost': 1521592.0,
  'update_swap_time_cost': 0.019129037857055664}
2024-10-17 05:02:49,050 - gen_series_legodnn_models.py[28] - INFO: target model size: 13.944MB
2024-10-17 05:02:49,050 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 14621526.555555556B (13.944MB), try to adapt blocks
2024-10-17 05:02:49,053 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:49,085 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01733964729309082
2024-10-17 05:02:49,085 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010298879891633987, 0.0008968959972262383, 0.0009348160028457642, 0.0008626560010015964, 0.00034073600172996525, 0.00038198399916291237, 0.0009362559951841831, 0.0003591999970376491, 0.0009042560122907162, 0.00041936000436544423, 0.000544416006654501]
2024-10-17 05:02:49,085 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.355
2024-10-17 05:02:49,085 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.832
2024-10-17 05:02:49,085 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.959
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.806
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.029
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.944
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.719
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.968
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.797
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.810
2024-10-17 05:02:49,086 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.697
2024-10-17 05:02:49,086 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:49,086 - optimal_runtime.py[116] - INFO: avg ratio: 1.8369297862826757
2024-10-17 05:02:49,086 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020831254565079277
2024-10-17 05:02:49,087 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107576 0.0008969  0.00092944 0.00090121 0.00036964 0.00038138
 0.00094241 0.00040649 0.00090503 0.00050051 0.00057572]
2024-10-17 05:02:49,089 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:49,255 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:49,257 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:49,270 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.2) from file
2024-10-17 05:02:49,271 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 408525760.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.001825764807760305,
  'esti_test_accuracy': 0.7448333899180094,
  'is_relaxed': False,
  'model_size': 14576653.0,
  'update_swap_mem_cost': 935548.0,
  'update_swap_time_cost': 0.013025999069213867}
2024-10-17 05:02:49,314 - gen_series_legodnn_models.py[28] - INFO: target model size: 14.292MB
2024-10-17 05:02:49,314 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 14986669.111111112B (14.292MB), try to adapt blocks
2024-10-17 05:02:49,317 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:49,348 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01744144058227539
2024-10-17 05:02:49,348 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010717440135776999, 0.0008630719929933547, 0.00092281598970294, 0.0009187839962542058, 0.0004031360000371933, 0.0004008639939129353, 0.0008981120027601719, 0.00035801600292325014, 0.0009133119992911816, 0.0004159680046141147, 0.0005390719920396806]
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.302
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.904
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.985
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.696
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.799
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.853
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.792
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.974
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.779
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.825
2024-10-17 05:02:49,349 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.713
2024-10-17 05:02:49,349 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  8,  9, 10]),)
2024-10-17 05:02:49,350 - optimal_runtime.py[116] - INFO: avg ratio: 1.7951159348272123
2024-10-17 05:02:49,350 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021316479484047536
2024-10-17 05:02:49,350 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00111948 0.00086307 0.00091751 0.00095985 0.00041695 0.00040023
 0.00090402 0.00040515 0.0009141  0.00049646 0.00057007]
2024-10-17 05:02:49,352 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:49,512 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:49,514 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:49,532 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:49,538 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.0) from file
2024-10-17 05:02:49,539 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 429131968.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0018781742991646924,
  'esti_test_accuracy': 0.7458667357762655,
  'is_relaxed': False,
  'model_size': 14903629.0,
  'update_swap_mem_cost': 2386320.0,
  'update_swap_time_cost': 0.0245516300201416}
2024-10-17 05:02:49,581 - gen_series_legodnn_models.py[28] - INFO: target model size: 14.641MB
2024-10-17 05:02:49,581 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 15351811.666666666B (14.641MB), try to adapt blocks
2024-10-17 05:02:49,583 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:49,616 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017625247955322266
2024-10-17 05:02:49,616 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001038847990334034, 0.0008692479990422726, 0.0009290239997208118, 0.0008836479932069778, 0.00042435199394822126, 0.00040537599846720695, 0.0009547839872539042, 0.00038447998836636544, 0.0009117120020091534, 0.0004184960052371025, 0.0005419520027935505]
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.343
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.890
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.960
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.764
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.767
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.832
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.686
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.838
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.782
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.814
2024-10-17 05:02:49,616 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.704
2024-10-17 05:02:49,617 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:49,617 - optimal_runtime.py[116] - INFO: avg ratio: 1.7864058921700083
2024-10-17 05:02:49,617 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021420413000176916
2024-10-17 05:02:49,618 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108512 0.00086925 0.00092902 0.00092314 0.00042435 0.00040474
 0.00096106 0.00043509 0.00091249 0.00049948 0.00057312]
2024-10-17 05:02:49,620 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:49,786 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:49,788 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.4, 0.2, 0.0, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:49,802 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.4) from file
2024-10-17 05:02:49,815 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.0) from file
2024-10-17 05:02:49,815 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 441881408.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.4, 0.2, 0.0, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0018550218260933846,
  'esti_test_accuracy': 0.7467000683148702,
  'is_relaxed': False,
  'model_size': 15305101.0,
  'update_swap_mem_cost': 6383534.0,
  'update_swap_time_cost': 0.027406692504882812}
2024-10-17 05:02:49,856 - gen_series_legodnn_models.py[28] - INFO: target model size: 14.989MB
2024-10-17 05:02:49,856 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 15716954.222222222B (14.989MB), try to adapt blocks
2024-10-17 05:02:49,859 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:49,893 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016835391998291014
2024-10-17 05:02:49,893 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009930879920721054, 0.0008073279969394207, 0.0009155519902706145, 0.0008261440061032771, 0.0003625599965453148, 0.00037507200613617897, 0.001001504000276327, 0.0003626560010015964, 0.0008641279935836791, 0.00046160000190138817, 0.000552863996475935]
2024-10-17 05:02:49,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.405
2024-10-17 05:02:49,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.035
2024-10-17 05:02:49,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.989
2024-10-17 05:02:49,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.886
2024-10-17 05:02:49,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.882
2024-10-17 05:02:49,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.980
2024-10-17 05:02:49,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.618
2024-10-17 05:02:49,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.949
2024-10-17 05:02:49,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.880
2024-10-17 05:02:49,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.644
2024-10-17 05:02:49,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.671
2024-10-17 05:02:49,894 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 2,  3,  4,  5,  7,  8,  9, 10]),)
2024-10-17 05:02:49,894 - optimal_runtime.py[116] - INFO: avg ratio: 1.860132472552809
2024-10-17 05:02:49,894 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020571412284263926
2024-10-17 05:02:49,895 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00103732 0.00080733 0.00091555 0.00086307 0.00039859 0.00037448
 0.0010015  0.0004104  0.00086487 0.00055092 0.00058466]
2024-10-17 05:02:49,897 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:50,050 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:50,050 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.6, 0.6, 0.6, 0.6]
2024-10-17 05:02:50,064 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.0) from file
2024-10-17 05:02:50,065 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 454586816.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.6, 0.6, 0.6, 0.6],
  'esti_latency': 0.0018069841233179108,
  'esti_test_accuracy': 0.7480000853538513,
  'is_relaxed': False,
  'model_size': 15708173.0,
  'update_swap_mem_cost': 1332860.0,
  'update_swap_time_cost': 0.014223575592041016}
2024-10-17 05:02:50,109 - gen_series_legodnn_models.py[28] - INFO: target model size: 15.337MB
2024-10-17 05:02:50,109 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 16082096.777777778B (15.337MB), try to adapt blocks
2024-10-17 05:02:50,111 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:50,143 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016908224105834962
2024-10-17 05:02:50,143 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010225280076265335, 0.0008535680063068867, 0.0008788800090551376, 0.0008546240031719209, 0.0003977279998362064, 0.00037772800400853154, 0.0009247039929032325, 0.00037091200053691864, 0.0008903039917349814, 0.0004276159964501858, 0.000560672003775835]
2024-10-17 05:02:50,143 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.365
2024-10-17 05:02:50,143 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.925
2024-10-17 05:02:50,143 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.072
2024-10-17 05:02:50,143 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.823
2024-10-17 05:02:50,143 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.886
2024-10-17 05:02:50,143 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.966
2024-10-17 05:02:50,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.752
2024-10-17 05:02:50,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.905
2024-10-17 05:02:50,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.825
2024-10-17 05:02:50,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.775
2024-10-17 05:02:50,144 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.647
2024-10-17 05:02:50,144 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:50,144 - optimal_runtime.py[116] - INFO: avg ratio: 1.8339412943840179
2024-10-17 05:02:50,144 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020865200054881626
2024-10-17 05:02:50,145 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00106807 0.00085357 0.00087888 0.00089282 0.00039773 0.00037713
 0.0009247  0.00041974 0.00089107 0.00051036 0.00059292]
2024-10-17 05:02:50,147 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:50,307 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:50,309 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6]
2024-10-17 05:02:50,328 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:50,334 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.2) from file
2024-10-17 05:02:50,346 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.2) from file
2024-10-17 05:02:50,359 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.4) from file
2024-10-17 05:02:50,360 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 424393856.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6],
  'esti_latency': 0.0017876129411276572,
  'esti_test_accuracy': 0.7486667037010193,
  'is_relaxed': False,
  'model_size': 16022541.0,
  'update_swap_mem_cost': 13793588.0,
  'update_swap_time_cost': 0.05069756507873535}
2024-10-17 05:02:50,401 - gen_series_legodnn_models.py[28] - INFO: target model size: 15.685MB
2024-10-17 05:02:50,401 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 16447239.333333334B (15.685MB), try to adapt blocks
2024-10-17 05:02:50,403 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:50,437 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017580032348632812
2024-10-17 05:02:50,437 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010374399907886983, 0.0008654399998486043, 0.0009119680002331736, 0.0008782080002129078, 0.00038921599835157393, 0.0003963199965655804, 0.0009781440012156962, 0.0003746560029685497, 0.0009871039986610413, 0.00044742400199174887, 0.0005468159988522529]
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.345
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.899
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.008
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.774
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.863
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.874
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.646
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.886
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.577
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.697
2024-10-17 05:02:50,438 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.689
2024-10-17 05:02:50,438 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:50,439 - optimal_runtime.py[116] - INFO: avg ratio: 1.7672640412523333
2024-10-17 05:02:50,439 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021652424936522236
2024-10-17 05:02:50,439 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108365 0.00086544 0.00090672 0.00091746 0.00040255 0.00039569
 0.00098458 0.00042398 0.00103086 0.000534   0.00057826]
2024-10-17 05:02:50,441 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:50,602 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:50,604 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6]
2024-10-17 05:02:50,723 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:50,729 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.0) from file
2024-10-17 05:02:50,730 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 445000064.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6],
  'esti_latency': 0.0018628834476989394,
  'esti_test_accuracy': 0.7497000495592753,
  'is_relaxed': False,
  'model_size': 16349517.0,
  'update_swap_mem_cost': 2386320.0,
  'update_swap_time_cost': 0.12552857398986816}
2024-10-17 05:02:50,770 - gen_series_legodnn_models.py[28] - INFO: target model size: 16.034MB
2024-10-17 05:02:50,770 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 16812381.888888888B (16.034MB), try to adapt blocks
2024-10-17 05:02:50,772 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:50,805 - optimal_runtime.py[77] - INFO: infer time of current model: 0.0168503360748291
2024-10-17 05:02:50,806 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010630400031805037, 0.0008117119930684567, 0.0008956159986555574, 0.0008228159956634044, 0.00039484800025820724, 0.00042896000295877454, 0.000890303999185562, 0.0003557120040059089, 0.0009614080078899862, 0.00042096000537276274, 0.0005385599955916405]
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.313
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.024
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.033
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.894
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.899
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.731
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.808
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.987
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.620
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.803
2024-10-17 05:02:50,806 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.715
2024-10-17 05:02:50,807 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:50,807 - optimal_runtime.py[116] - INFO: avg ratio: 1.8071759827163252
2024-10-17 05:02:50,807 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002117422562174327
2024-10-17 05:02:50,807 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00111039 0.00081171 0.00089562 0.00085959 0.00039485 0.00042828
 0.00089616 0.00040254 0.00100402 0.00050242 0.00056953]
2024-10-17 05:02:50,809 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:50,972 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:50,974 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.4, 0.2, 0.0, 0.6, 0.4, 0.6, 0.6]
2024-10-17 05:02:50,986 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.4) from file
2024-10-17 05:02:51,000 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.0) from file
2024-10-17 05:02:51,001 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 457749504.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.4, 0.2, 0.0, 0.6, 0.4, 0.6, 0.6],
  'esti_latency': 0.0017964207172557556,
  'esti_test_accuracy': 0.75053338209788,
  'is_relaxed': False,
  'model_size': 16750989.0,
  'update_swap_mem_cost': 6383534.0,
  'update_swap_time_cost': 0.026598453521728516}
2024-10-17 05:02:51,043 - gen_series_legodnn_models.py[28] - INFO: target model size: 16.382MB
2024-10-17 05:02:51,043 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 17177524.444444444B (16.382MB), try to adapt blocks
2024-10-17 05:02:51,046 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:51,077 - optimal_runtime.py[77] - INFO: infer time of current model: 0.0170283203125
2024-10-17 05:02:51,077 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009671039953827858, 0.0008367359936237337, 0.0009019520021975039, 0.0008913599997758865, 0.0003834239989519119, 0.00039820799976587293, 0.0009870399944484232, 0.00035600000247359276, 0.0009437120109796526, 0.0004181760065257549, 0.0005455360040068627]
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.443
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.964
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.019
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.748
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.779
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.865
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.642
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.985
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.650
2024-10-17 05:02:51,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.815
2024-10-17 05:02:51,078 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.693
2024-10-17 05:02:51,078 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  8,  9, 10]),)
2024-10-17 05:02:51,078 - optimal_runtime.py[116] - INFO: avg ratio: 1.7417678508403789
2024-10-17 05:02:51,078 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002196937552715104
2024-10-17 05:02:51,079 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00101018 0.00083674 0.00090195 0.0009312  0.00042153 0.00039758
 0.00098704 0.00040286 0.00098554 0.00049909 0.00057691]
2024-10-17 05:02:51,081 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:51,231 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:51,231 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.6, 0.4, 0.6, 0.6]
2024-10-17 05:02:51,245 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.0) from file
2024-10-17 05:02:51,246 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 470454912.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.6, 0.4, 0.6, 0.6],
  'esti_latency': 0.0019136703218848982,
  'esti_test_accuracy': 0.7518333991368612,
  'is_relaxed': False,
  'model_size': 17154061.0,
  'update_swap_mem_cost': 1332860.0,
  'update_swap_time_cost': 0.014091253280639648}
2024-10-17 05:02:51,289 - gen_series_legodnn_models.py[28] - INFO: target model size: 16.730MB
2024-10-17 05:02:51,289 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 17542667.0B (16.730MB), try to adapt blocks
2024-10-17 05:02:51,291 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:51,323 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01714860725402832
2024-10-17 05:02:51,323 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010363840013742445, 0.0008133440092206004, 0.0009012159965932367, 0.0008464640080928802, 0.0003985279947519302, 0.000374975997954607, 0.0009301439970731736, 0.0003632320016622543, 0.0009241599924862386, 0.000392351996153593, 0.0005160959959030152]
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.347
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.020
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.021
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.841
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.882
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.981
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.742
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.946
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.685
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.935
2024-10-17 05:02:51,324 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.790
2024-10-17 05:02:51,324 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:51,325 - optimal_runtime.py[116] - INFO: avg ratio: 1.8841308240543095
2024-10-17 05:02:51,325 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020309392271334174
2024-10-17 05:02:51,325 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108254 0.00081334 0.00090122 0.00088429 0.00039853 0.00037438
 0.00093014 0.00041105 0.00096512 0.00046827 0.00054578]
2024-10-17 05:02:51,327 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:51,499 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:51,501 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.6, 0.4, 0.6, 0.6]
2024-10-17 05:02:51,524 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.0) from file
2024-10-17 05:02:51,524 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 482671488.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.6, 0.4, 0.6, 0.6],
  'esti_latency': 0.00179099165332087,
  'esti_test_accuracy': 0.7524667183558146,
  'is_relaxed': False,
  'model_size': 17457037.0,
  'update_swap_mem_cost': 2101234.0,
  'update_swap_time_cost': 0.02327132225036621}
2024-10-17 05:02:51,568 - gen_series_legodnn_models.py[28] - INFO: target model size: 17.078MB
2024-10-17 05:02:51,568 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 17907809.555555556B (17.078MB), try to adapt blocks
2024-10-17 05:02:51,571 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:51,605 - optimal_runtime.py[77] - INFO: infer time of current model: 0.018935808181762694
2024-10-17 05:02:51,605 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009891520142555237, 0.000861375994980335, 0.0009042559973895549, 0.0009347199909389019, 0.0003914560005068779, 0.00039798399806022645, 0.001002752009779215, 0.0003816639967262745, 0.0009664640054106713, 0.000432608000934124, 0.0005471359975636004]
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.411
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.908
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.014
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.742
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.916
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.866
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.616
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.852
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.611
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.755
2024-10-17 05:02:51,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.688
2024-10-17 05:02:51,606 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:51,606 - optimal_runtime.py[116] - INFO: avg ratio: 1.772540103063854
2024-10-17 05:02:51,606 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.00215879753186338
2024-10-17 05:02:51,607 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00103321 0.00086138 0.00090426 0.00093472 0.00039146 0.00039736
 0.00100275 0.00043191 0.0010093  0.00051632 0.0005786 ]
2024-10-17 05:02:51,609 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:51,763 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:51,765 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.4, 0.6, 0.6]
2024-10-17 05:02:51,778 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.0) from file
2024-10-17 05:02:51,778 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 489661184.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.4, 0.6, 0.6],
  'esti_latency': 0.0019064859095658951,
  'esti_test_accuracy': 0.7527667284011841,
  'is_relaxed': False,
  'model_size': 17678541.0,
  'update_swap_mem_cost': 1514428.0,
  'update_swap_time_cost': 0.013440132141113281}
2024-10-17 05:02:51,823 - gen_series_legodnn_models.py[28] - INFO: target model size: 17.426MB
2024-10-17 05:02:51,823 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 18272952.111111112B (17.426MB), try to adapt blocks
2024-10-17 05:02:51,826 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:51,858 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01670345687866211
2024-10-17 05:02:51,858 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009893120154738427, 0.0008239360041916371, 0.0008632959946990013, 0.0008506560064852238, 0.00039478399604558944, 0.0003969599977135658, 0.0009334399998188019, 0.0003558719977736473, 0.0009275200031697751, 0.0003947519920766354, 0.0005147840008139611]
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.411
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.994
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.109
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.914
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.900
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.868
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.736
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.986
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.679
2024-10-17 05:02:51,858 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.923
2024-10-17 05:02:51,859 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.794
2024-10-17 05:02:51,859 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:51,859 - optimal_runtime.py[116] - INFO: avg ratio: 1.8659523082411853
2024-10-17 05:02:51,859 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.00205072508162331
2024-10-17 05:02:51,860 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00103337 0.00082394 0.0008633  0.00085066 0.00039478 0.00039696
 0.00093344 0.00040272 0.00096863 0.00047114 0.00054439]
2024-10-17 05:02:51,862 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:52,037 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:52,039 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.6, 0.4, 0.4, 0.6]
2024-10-17 05:02:52,059 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:52,070 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:52,078 - pure_runtime.py[42] - DEBUG: load 9th block (block-9) (sparsity 0.4) from file
2024-10-17 05:02:52,078 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 471659104.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.6, 0.4, 0.4, 0.6],
  'esti_latency': 0.0018237116339404201,
  'esti_test_accuracy': 0.7532334129015604,
  'is_relaxed': False,
  'model_size': 18271309.0,
  'update_swap_mem_cost': 6970818.0,
  'update_swap_time_cost': 0.03896903991699219}
2024-10-17 05:02:52,124 - gen_series_legodnn_models.py[28] - INFO: target model size: 17.775MB
2024-10-17 05:02:52,124 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 18638094.666666668B (17.775MB), try to adapt blocks
2024-10-17 05:02:52,127 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:52,158 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016994239807128907
2024-10-17 05:02:52,158 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001035423994064331, 0.0008960960023105145, 0.0008474239967763423, 0.0008196160085499287, 0.00040208000317215923, 0.00039686400815844536, 0.0009288320094347, 0.00035564799234271045, 0.0009252480007708073, 0.00043273599818348895, 0.0005167999975383283]
2024-10-17 05:02:52,158 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.348
2024-10-17 05:02:52,158 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.834
2024-10-17 05:02:52,158 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.161
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.901
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.865
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.869
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.744
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.987
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.683
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.942
2024-10-17 05:02:52,159 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.787
2024-10-17 05:02:52,159 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:52,159 - optimal_runtime.py[116] - INFO: avg ratio: 1.8458938641195681
2024-10-17 05:02:52,159 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020730093284362545
2024-10-17 05:02:52,160 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108154 0.0008961  0.00084255 0.00085625 0.00040208 0.00039686
 0.00092883 0.00040247 0.00096626 0.00046638 0.00054652]
2024-10-17 05:02:52,162 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:52,327 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:52,329 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.0, 0.6, 0.2, 0.6, 0.6]
2024-10-17 05:02:52,340 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.2) from file
2024-10-17 05:02:52,348 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:52,364 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.2) from file
2024-10-17 05:02:52,371 - pure_runtime.py[42] - DEBUG: load 9th block (block-9) (sparsity 0.6) from file
2024-10-17 05:02:52,372 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 468231488.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.2, 0.2, 0.0, 0.6, 0.2, 0.6, 0.6],
  'esti_latency': 0.0018428105518003067,
  'esti_test_accuracy': 0.7541000644365946,
  'is_relaxed': False,
  'model_size': 18561101.0,
  'update_swap_mem_cost': 16573670.0,
  'update_swap_time_cost': 0.04256415367126465}
2024-10-17 05:02:52,413 - gen_series_legodnn_models.py[28] - INFO: target model size: 18.123MB
2024-10-17 05:02:52,414 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 19003237.222222224B (18.123MB), try to adapt blocks
2024-10-17 05:02:52,416 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:52,448 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01746099281311035
2024-10-17 05:02:52,448 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001091359995305538, 0.0008373760022222997, 0.0009034880027174951, 0.0008639359995722769, 0.0003771839998662472, 0.0003789440020918846, 0.0009491199962794781, 0.0003822719976305962, 0.0010435199998319147, 0.0004150719977915287, 0.000550207994878292]
2024-10-17 05:02:52,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.279
2024-10-17 05:02:52,448 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.962
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.027
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.804
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.922
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.960
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.707
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.849
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.578
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.829
2024-10-17 05:02:52,449 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.679
2024-10-17 05:02:52,449 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:52,450 - optimal_runtime.py[116] - INFO: avg ratio: 1.8100489951468266
2024-10-17 05:02:52,450 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021140616689841085
2024-10-17 05:02:52,450 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00113997 0.00083738 0.00089829 0.00090255 0.00039011 0.00037835
 0.00094912 0.00043259 0.00103017 0.00049539 0.00058185]
2024-10-17 05:02:52,452 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:52,609 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:52,611 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.6, 0.2, 0.6, 0.6]
2024-10-17 05:02:52,624 - pure_runtime.py[42] - DEBUG: load 4th block (block-4) (sparsity 0.0) from file
2024-10-17 05:02:52,630 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.0) from file
2024-10-17 05:02:52,630 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 482038720.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.6, 0.2, 0.6, 0.6],
  'esti_latency': 0.0018831041427389292,
  'esti_test_accuracy': 0.7552000880241394,
  'is_relaxed': False,
  'model_size': 18998797.0,
  'update_swap_mem_cost': 3034168.0,
  'update_swap_time_cost': 0.01923990249633789}
2024-10-17 05:02:52,677 - gen_series_legodnn_models.py[28] - INFO: target model size: 18.471MB
2024-10-17 05:02:52,678 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 19368379.77777778B (18.471MB), try to adapt blocks
2024-10-17 05:02:52,680 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:52,711 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01709491157531738
2024-10-17 05:02:52,712 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001064959991723299, 0.000827743999660015, 0.0008501440063118937, 0.00082345599681139, 0.00039664000272750856, 0.0004076799973845482, 0.000944704007357359, 0.0004097919948399067, 0.0010156479999423027, 0.00039052800461649893, 0.0005128319971263409]
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.311
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.985
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.154
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.892
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.891
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.819
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.715
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.725
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.622
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.944
2024-10-17 05:02:52,712 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.801
2024-10-17 05:02:52,713 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:52,713 - optimal_runtime.py[116] - INFO: avg ratio: 1.8215191635184764
2024-10-17 05:02:52,713 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021007493504661633
2024-10-17 05:02:52,713 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00111239 0.00082774 0.00084525 0.00086026 0.00039664 0.00040768
 0.0009447  0.00046374 0.00100266 0.0004661  0.00054232]
2024-10-17 05:02:52,715 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:52,873 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:52,875 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.6, 0.2, 0.6, 0.6]
2024-10-17 05:02:52,893 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.0) from file
2024-10-17 05:02:52,893 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 494255296.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.6, 0.2, 0.6, 0.6],
  'esti_latency': 0.001912191879627066,
  'esti_test_accuracy': 0.7558334072430929,
  'is_relaxed': False,
  'model_size': 19301773.0,
  'update_swap_mem_cost': 2101234.0,
  'update_swap_time_cost': 0.018299579620361328}
2024-10-17 05:02:52,936 - gen_series_legodnn_models.py[28] - INFO: target model size: 18.819MB
2024-10-17 05:02:52,936 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 19733522.333333332B (18.819MB), try to adapt blocks
2024-10-17 05:02:52,938 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:52,971 - optimal_runtime.py[77] - INFO: infer time of current model: 0.018462751388549806
2024-10-17 05:02:52,971 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009609279930591584, 0.0008080000095069408, 0.0008690240010619163, 0.00090243199467659, 0.0004035839922726155, 0.0020783359967172146, 0.0009409280009567738, 0.00036723199859261515, 0.0010012800060212614, 0.000393664002418518, 0.0005321600027382373]
2024-10-17 05:02:52,971 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.452
2024-10-17 05:02:52,971 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.034
2024-10-17 05:02:52,971 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.108
2024-10-17 05:02:52,971 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.804
2024-10-17 05:02:52,971 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.858
2024-10-17 05:02:52,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.002 = 0.357
2024-10-17 05:02:52,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.722
2024-10-17 05:02:52,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.925
2024-10-17 05:02:52,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.645
2024-10-17 05:02:52,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.928
2024-10-17 05:02:52,972 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.736
2024-10-17 05:02:52,972 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 0,  1,  2,  3,  4,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:52,972 - optimal_runtime.py[116] - INFO: avg ratio: 1.8211282539934257
2024-10-17 05:02:52,972 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021012002813267663
2024-10-17 05:02:52,973 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00100373 0.000808   0.00086403 0.00090243 0.00040358 0.00207834
 0.00094093 0.00041557 0.00098847 0.00046984 0.00056276]
2024-10-17 05:02:52,975 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:53,126 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:53,128 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.6, 0.4, 0.2, 0.6]
2024-10-17 05:02:53,142 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:53,220 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.4) from file
2024-10-17 05:02:53,249 - pure_runtime.py[42] - DEBUG: load 9th block (block-9) (sparsity 0.2) from file
2024-10-17 05:02:53,250 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 487647424.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.6, 0.4, 0.2, 0.6],
  'esti_latency': 0.0019309979596420592,
  'esti_test_accuracy': 0.7567667166392008,
  'is_relaxed': False,
  'model_size': 19706637.0,
  'update_swap_mem_cost': 16407786.0,
  'update_swap_time_cost': 0.1216731071472168}
2024-10-17 05:02:53,295 - gen_series_legodnn_models.py[28] - INFO: target model size: 19.168MB
2024-10-17 05:02:53,295 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 20098664.888888888B (19.168MB), try to adapt blocks
2024-10-17 05:02:53,298 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:53,604 - optimal_runtime.py[77] - INFO: infer time of current model: 0.2915209350585938
2024-10-17 05:02:53,605 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.002479039912577719, 0.00018505599861964582, 0.0001637439993210137, 0.00021174400392919783, 0.00012166399834677578, 0.00010467200167477131, 0.0003016959996894002, 9.465599968098103e-05, 0.0002923519958276302, 0.0004444159902632236, 0.0001616639995481819]
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.002 = 0.563
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.000 = 8.880
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.000 = 11.185
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.000 = 7.689
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 6.164
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 7.096
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.000 = 5.370
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 7.467
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.000 = 5.326
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.974
2024-10-17 05:02:53,605 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 5.714
2024-10-17 05:02:53,606 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8, 10]),)
2024-10-17 05:02:53,606 - optimal_runtime.py[116] - INFO: avg ratio: 6.713067377638923
2024-10-17 05:02:53,606 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0005700159084309623
2024-10-17 05:02:53,607 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00258945 0.00018506 0.0001628  0.00021174 0.00012166 0.00010451
 0.0003017  0.00010712 0.00030531 0.00045886 0.00017096]
2024-10-17 05:02:53,609 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:53,785 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:53,787 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.4, 0.2, 0.6]
2024-10-17 05:02:53,883 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:53,915 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.0) from file
2024-10-17 05:02:53,915 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 508425792.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.4, 0.2, 0.6],
  'esti_latency': 0.00041044988083769,
  'esti_test_accuracy': 0.757300059000651,
  'is_relaxed': False,
  'model_size': 20038925.0,
  'update_swap_mem_cost': 2381008.0,
  'update_swap_time_cost': 0.12829279899597168}
2024-10-17 05:02:53,960 - gen_series_legodnn_models.py[28] - INFO: target model size: 19.516MB
2024-10-17 05:02:53,961 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 20463807.444444444B (19.516MB), try to adapt blocks
2024-10-17 05:02:53,963 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:54,050 - optimal_runtime.py[77] - INFO: infer time of current model: 0.07046486663818359
2024-10-17 05:02:54,050 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.00012559999991208315, 0.0004423359972424805, 0.0008933760076761246, 0.0008296000137925148, 0.0003845760002732277, 0.0003736639991402626, 0.0009272640049457549, 0.0003466560021042824, 0.000885152004659176, 0.0004719679951667786, 0.0005189120024442672]
2024-10-17 05:02:54,050 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 11.112
2024-10-17 05:02:54,050 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.000 = 3.715
2024-10-17 05:02:54,050 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.038
2024-10-17 05:02:54,050 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.962
2024-10-17 05:02:54,051 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.950
2024-10-17 05:02:54,051 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.985
2024-10-17 05:02:54,051 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.747
2024-10-17 05:02:54,051 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.039
2024-10-17 05:02:54,051 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.759
2024-10-17 05:02:54,051 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.859
2024-10-17 05:02:54,051 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.780
2024-10-17 05:02:54,051 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:54,051 - optimal_runtime.py[116] - INFO: avg ratio: 2.0834475515142463
2024-10-17 05:02:54,051 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.001836645802214687
2024-10-17 05:02:54,052 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00013119 0.00044234 0.00089338 0.0008296  0.00038458 0.00037366
 0.00092726 0.00039229 0.00092439 0.0004873  0.00054875]
2024-10-17 05:02:54,054 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:54,214 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:54,218 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.6]
2024-10-17 05:02:54,235 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:54,241 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:54,253 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.2) from file
2024-10-17 05:02:54,267 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.2) from file
2024-10-17 05:02:54,268 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 482147456.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.6],
  'esti_latency': 0.0017112587995722131,
  'esti_test_accuracy': 0.7575333913167318,
  'is_relaxed': False,
  'model_size': 20443917.0,
  'update_swap_mem_cost': 18202834.0,
  'update_swap_time_cost': 0.04947972297668457}
2024-10-17 05:02:54,311 - gen_series_legodnn_models.py[28] - INFO: target model size: 19.864MB
2024-10-17 05:02:54,311 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 20828950.0B (19.864MB), try to adapt blocks
2024-10-17 05:02:54,313 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:54,345 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016717824935913086
2024-10-17 05:02:54,346 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010574720017611982, 0.0008432960025966166, 0.0009064959958195687, 0.0008303359970450403, 0.0003763199932873249, 0.0003598399944603443, 0.0008389120064675806, 0.00035382400453090666, 0.0010056000091135501, 0.0004371520131826401, 0.0004763199985027313]
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.320
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.949
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.009
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.877
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.993
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.064
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.919
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.997
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.638
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.007
2024-10-17 05:02:54,346 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.939
2024-10-17 05:02:54,347 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  9, 10]),)
2024-10-17 05:02:54,347 - optimal_runtime.py[116] - INFO: avg ratio: 1.9726248286162595
2024-10-17 05:02:54,347 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0019398291779118124
2024-10-17 05:02:54,347 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00110457 0.0008433  0.0009065  0.00086745 0.00037632 0.00035927
 0.00084443 0.0004004  0.00099274 0.00045136 0.00050371]
2024-10-17 05:02:54,349 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:54,507 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:54,509 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.6]
2024-10-17 05:02:54,529 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.0) from file
2024-10-17 05:02:54,530 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 494364032.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.6],
  'esti_latency': 0.0018124704444250537,
  'esti_test_accuracy': 0.7581667105356852,
  'is_relaxed': False,
  'model_size': 20746893.0,
  'update_swap_mem_cost': 2101234.0,
  'update_swap_time_cost': 0.02100348472595215}
2024-10-17 05:02:54,574 - gen_series_legodnn_models.py[28] - INFO: target model size: 20.212MB
2024-10-17 05:02:54,575 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 21194092.555555556B (20.212MB), try to adapt blocks
2024-10-17 05:02:54,577 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:54,609 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017122943878173827
2024-10-17 05:02:54,609 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010076160058379174, 0.0007894719950854779, 0.000927520003169775, 0.0008729280084371566, 0.00040988800302147865, 0.0003903039917349815, 0.0009111679941415786, 0.00037574400380253794, 0.0009728320054709911, 0.0004512959942221642, 0.0005158720016479493]
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.385
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.081
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.963
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.865
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.830
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.903
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.767
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.881
2024-10-17 05:02:54,609 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.693
2024-10-17 05:02:54,610 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.944
2024-10-17 05:02:54,610 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.791
2024-10-17 05:02:54,610 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:54,610 - optimal_runtime.py[116] - INFO: avg ratio: 1.8484756309158397
2024-10-17 05:02:54,610 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020701139553174503
2024-10-17 05:02:54,611 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00105249 0.00078947 0.00092752 0.00087293 0.00040989 0.00038969
 0.00091716 0.00042521 0.00096039 0.00046596 0.00054554]
2024-10-17 05:02:54,613 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:54,778 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:54,780 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.6]
2024-10-17 05:02:54,803 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:54,818 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:54,835 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.0) from file
2024-10-17 05:02:54,835 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 493813632.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.6],
  'esti_latency': 0.0019125223794887803,
  'esti_test_accuracy': 0.7594334085782369,
  'is_relaxed': False,
  'model_size': 21137677.0,
  'update_swap_mem_cost': 8018488.0,
  'update_swap_time_cost': 0.05486869812011719}
2024-10-17 05:02:54,892 - gen_series_legodnn_models.py[28] - INFO: target model size: 20.560MB
2024-10-17 05:02:54,892 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 21559235.111111112B (20.560MB), try to adapt blocks
2024-10-17 05:02:54,896 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:54,939 - optimal_runtime.py[77] - INFO: infer time of current model: 0.02304800033569336
2024-10-17 05:02:54,940 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0013469760119915006, 0.0011236480101943017, 0.0011731520034372808, 0.0011052480041980747, 0.0005110080018639564, 0.0005084799937903881, 0.0011828160025179387, 0.0004777279943227768, 0.0012525439970195293, 0.0005612799935042858, 0.0006504639983177185]
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.036
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.462
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.561
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.410
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.468
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.461
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.370
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.479
2024-10-17 05:02:54,940 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.315
2024-10-17 05:02:54,941 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.563
2024-10-17 05:02:54,941 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.420
2024-10-17 05:02:54,941 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8, 10]),)
2024-10-17 05:02:54,941 - optimal_runtime.py[116] - INFO: avg ratio: 1.4231154934963106
2024-10-17 05:02:54,941 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002688857803256731
2024-10-17 05:02:54,942 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00140697 0.00112365 0.0011664  0.00115464 0.00051101 0.00050768
 0.00118282 0.00054062 0.00123653 0.00057952 0.00068787]
2024-10-17 05:02:54,945 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:55,097 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:55,099 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.6]
2024-10-17 05:02:55,117 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:55,128 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.0) from file
2024-10-17 05:02:55,129 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 519818880.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.6],
  'esti_latency': 0.0025271553954480186,
  'esti_test_accuracy': 0.7603000601132711,
  'is_relaxed': False,
  'model_size': 21551437.0,
  'update_swap_mem_cost': 2967814.0,
  'update_swap_time_cost': 0.029882192611694336}
2024-10-17 05:02:55,174 - gen_series_legodnn_models.py[28] - INFO: target model size: 20.909MB
2024-10-17 05:02:55,174 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 21924377.666666668B (20.909MB), try to adapt blocks
2024-10-17 05:02:55,176 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:55,209 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017817535400390626
2024-10-17 05:02:55,210 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010489280000329018, 0.0008483519963920117, 0.0008940800055861473, 0.0008972799964249133, 0.0003985599987208843, 0.00039843199774622915, 0.0009317759983241558, 0.00037119999900460247, 0.0009815359897911548, 0.0005077440068125725, 0.0005330879949033261]
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.331
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.937
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.037
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.814
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.882
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.864
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.739
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.904
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.678
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.728
2024-10-17 05:02:55,210 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.733
2024-10-17 05:02:55,211 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:55,211 - optimal_runtime.py[116] - INFO: avg ratio: 1.80876287081999
2024-10-17 05:02:55,211 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002115564876610037
2024-10-17 05:02:55,211 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00109564 0.00084835 0.00089408 0.00089728 0.00039856 0.0003978
 0.00093178 0.00042007 0.00096898 0.00052424 0.00056375]
2024-10-17 05:02:55,213 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:55,365 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:55,365 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.2, 0.2, 0.6]
2024-10-17 05:02:55,379 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.0) from file
2024-10-17 05:02:55,380 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 526808576.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.2, 0.2, 0.6],
  'esti_latency': 0.0019853819181130275,
  'esti_test_accuracy': 0.7606000701586405,
  'is_relaxed': False,
  'model_size': 21772941.0,
  'update_swap_mem_cost': 1514428.0,
  'update_swap_time_cost': 0.01344609260559082}
2024-10-17 05:02:55,433 - gen_series_legodnn_models.py[28] - INFO: target model size: 21.257MB
2024-10-17 05:02:55,434 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 22289520.222222224B (21.257MB), try to adapt blocks
2024-10-17 05:02:55,436 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:55,468 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017722911834716797
2024-10-17 05:02:55,469 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010509760081768035, 0.0008331199958920478, 0.0009063040018081665, 0.0008732160031795501, 0.0003888639993965626, 0.00039721599593758584, 0.0009366720020771029, 0.0003559040017426014, 0.0009555200040340424, 0.00045395199209451675, 0.0005276799984276295]
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.328
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.972
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.009
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.864
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.929
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.867
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.730
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.986
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.724
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.933
2024-10-17 05:02:55,469 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.750
2024-10-17 05:02:55,470 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:55,470 - optimal_runtime.py[116] - INFO: avg ratio: 1.8764126188006713
2024-10-17 05:02:55,470 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020392930431628047
2024-10-17 05:02:55,471 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00109778 0.00083312 0.0009063  0.00087322 0.00038886 0.00039722
 0.00093667 0.00040276 0.0009433  0.0004687  0.00055803]
2024-10-17 05:02:55,473 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:55,629 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:55,632 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2, 0.2, 0.6]
2024-10-17 05:02:55,647 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.4) from file
2024-10-17 05:02:55,647 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 538343808.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.2, 0.2, 0.6],
  'esti_latency': 0.001925007034249539,
  'esti_test_accuracy': 0.7607667247454325,
  'is_relaxed': False,
  'model_size': 22136973.0,
  'update_swap_mem_cost': 1574524.0,
  'update_swap_time_cost': 0.015388250350952148}
2024-10-17 05:02:55,694 - gen_series_legodnn_models.py[28] - INFO: target model size: 21.605MB
2024-10-17 05:02:55,694 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 22654662.77777778B (21.605MB), try to adapt blocks
2024-10-17 05:02:55,697 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:55,728 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017146976470947265
2024-10-17 05:02:55,729 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011161280088126657, 0.0008287039920687676, 0.0009354240112006663, 0.0008479680009186269, 0.0003710720054805279, 0.0003720000050961971, 0.0008978240080177783, 0.0003757439963519573, 0.0009154880009591579, 0.00043535999953746795, 0.0005092799961566926]
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.250
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.983
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.947
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.920
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.021
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.993
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.805
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.946
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.799
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.015
2024-10-17 05:02:55,729 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.814
2024-10-17 05:02:55,730 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:55,730 - optimal_runtime.py[116] - INFO: avg ratio: 1.9242507245661051
2024-10-17 05:02:55,730 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.001988594911656302
2024-10-17 05:02:55,731 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116584 0.0008287  0.00093542 0.00084797 0.00037107 0.000372
 0.00089782 0.00041106 0.00090378 0.0004495  0.00053857]
2024-10-17 05:02:55,733 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:55,887 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:55,889 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.6]
2024-10-17 05:02:55,904 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.2) from file
2024-10-17 05:02:55,905 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 551962496.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.6],
  'esti_latency': 0.0018937112478641077,
  'esti_test_accuracy': 0.7613000671068827,
  'is_relaxed': False,
  'model_size': 22566989.0,
  'update_swap_mem_cost': 2368572.0,
  'update_swap_time_cost': 0.01630234718322754}
2024-10-17 05:02:55,951 - gen_series_legodnn_models.py[28] - INFO: target model size: 21.953MB
2024-10-17 05:02:55,951 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 23019805.333333332B (21.953MB), try to adapt blocks
2024-10-17 05:02:55,953 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:55,986 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017854400634765625
2024-10-17 05:02:55,986 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010104959905147552, 0.0008982079997658729, 0.000946016002446413, 0.0009325119927525521, 0.0004195519872009755, 0.00043254399672150613, 0.0009551999941468239, 0.00041219200566411016, 0.0010479679964482782, 0.0005161600038409233, 0.0005487679950892925]
2024-10-17 05:02:55,986 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.381
2024-10-17 05:02:55,986 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.829
2024-10-17 05:02:55,986 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.925
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.746
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.788
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.714
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.696
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.877
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.572
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.700
2024-10-17 05:02:55,987 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.683
2024-10-17 05:02:55,987 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  9, 10]),)
2024-10-17 05:02:55,987 - optimal_runtime.py[116] - INFO: avg ratio: 1.7366475657952702
2024-10-17 05:02:55,987 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022034149443965036
2024-10-17 05:02:55,988 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0010555  0.00089821 0.00094602 0.00093251 0.00041955 0.00043254
 0.0009552  0.00042613 0.00103457 0.00053293 0.00058033]
2024-10-17 05:02:55,990 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:56,148 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:56,150 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6]
2024-10-17 05:02:56,170 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:56,177 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.0) from file
2024-10-17 05:02:56,177 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 553316992.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6],
  'esti_latency': 0.0021289558746951697,
  'esti_test_accuracy': 0.7613334059715271,
  'is_relaxed': False,
  'model_size': 22934029.0,
  'update_swap_mem_cost': 4142992.0,
  'update_swap_time_cost': 0.027373313903808594}
2024-10-17 05:02:56,224 - gen_series_legodnn_models.py[28] - INFO: target model size: 22.302MB
2024-10-17 05:02:56,224 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 23384947.888888888B (22.302MB), try to adapt blocks
2024-10-17 05:02:56,226 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:56,257 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016753631591796873
2024-10-17 05:02:56,257 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009517439976334572, 0.000802751999348402, 0.0008526719957590102, 0.0008479040078818798, 0.0003910400010645389, 0.000393471997231245, 0.0009560639970004559, 0.000465952005237341, 0.0009893120080232622, 0.0004905920103192329, 0.0005137920007109642]
2024-10-17 05:02:56,257 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.466
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.047
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.148
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.920
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.918
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.885
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.695
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.716
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.665
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.789
2024-10-17 05:02:56,258 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.798
2024-10-17 05:02:56,258 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:56,258 - optimal_runtime.py[116] - INFO: avg ratio: 1.7981135418620775
2024-10-17 05:02:56,258 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002128094311363915
2024-10-17 05:02:56,259 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00099413 0.00080275 0.00084777 0.0008479  0.00039104 0.00039347
 0.00095606 0.00046595 0.00097666 0.00050653 0.00054334]
2024-10-17 05:02:56,261 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:56,421 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:56,423 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6]
2024-10-17 05:02:56,442 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:56,443 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 567105664.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6],
  'esti_latency': 0.0020528712247698906,
  'esti_test_accuracy': 0.7615667382876078,
  'is_relaxed': False,
  'model_size': 23044813.0,
  'update_swap_mem_cost': 866580.0,
  'update_swap_time_cost': 0.020008325576782227}
2024-10-17 05:02:56,490 - gen_series_legodnn_models.py[28] - INFO: target model size: 22.650MB
2024-10-17 05:02:56,490 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 23750090.444444444B (22.650MB), try to adapt blocks
2024-10-17 05:02:56,492 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:56,526 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019184064865112305
2024-10-17 05:02:56,526 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009549120068550112, 0.0008178560025990007, 0.0008965759985148907, 0.0009023680016398428, 0.00043222400173544885, 0.0004257280044257641, 0.00095974400639534, 0.0004679039940237999, 0.0010574399940669536, 0.0004988799989223481, 0.0005090880058705806]
2024-10-17 05:02:56,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.462
2024-10-17 05:02:56,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.009
2024-10-17 05:02:56,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.031
2024-10-17 05:02:56,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.804
2024-10-17 05:02:56,526 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.735
2024-10-17 05:02:56,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.742
2024-10-17 05:02:56,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.688
2024-10-17 05:02:56,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.709
2024-10-17 05:02:56,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.558
2024-10-17 05:02:56,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.759
2024-10-17 05:02:56,527 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.814
2024-10-17 05:02:56,527 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  7,  9, 10]),)
2024-10-17 05:02:56,527 - optimal_runtime.py[116] - INFO: avg ratio: 1.7502592910485073
2024-10-17 05:02:56,527 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021862790383079633
2024-10-17 05:02:56,528 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00099744 0.00081786 0.00089658 0.00090237 0.00043222 0.00042573
 0.00095974 0.0004679  0.00104392 0.00051509 0.00053836]
2024-10-17 05:02:56,530 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:56,692 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:56,694 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.4]
2024-10-17 05:02:56,707 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:56,719 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.2) from file
2024-10-17 05:02:56,725 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.6) from file
2024-10-17 05:02:56,736 - pure_runtime.py[42] - DEBUG: load 10th block (block-10) (sparsity 0.4) from file
2024-10-17 05:02:56,736 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 517696928.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.6, 0.2, 0.2, 0.4],
  'esti_latency': 0.0020435121304735863,
  'esti_test_accuracy': 0.7620333631833395,
  'is_relaxed': False,
  'model_size': 23671629.0,
  'update_swap_mem_cost': 20119106.0,
  'update_swap_time_cost': 0.04193568229675293}
2024-10-17 05:02:56,780 - gen_series_legodnn_models.py[28] - INFO: target model size: 22.998MB
2024-10-17 05:02:56,780 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 24115233.0B (22.998MB), try to adapt blocks
2024-10-17 05:02:56,782 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:56,813 - optimal_runtime.py[77] - INFO: infer time of current model: 0.016732671737670898
2024-10-17 05:02:56,814 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009462079890072345, 0.0008568320050835608, 0.0009232320003211497, 0.0008453119955956936, 0.0003930879943072796, 0.000399616003036499, 0.0009140799865126609, 0.00035225600004196167, 0.0010138880014419556, 0.0005077120028436184, 0.0005788479931652546]
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.475
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.918
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.972
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.926
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.908
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.859
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.761
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 2.006
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.625
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.728
2024-10-17 05:02:56,814 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.570
2024-10-17 05:02:56,815 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 9]),)
2024-10-17 05:02:56,815 - optimal_runtime.py[116] - INFO: avg ratio: 1.849886920214199
2024-10-17 05:02:56,815 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020685346535560295
2024-10-17 05:02:56,815 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00098835 0.00085683 0.00092323 0.00084531 0.00039309 0.00039899
 0.00092009 0.00039863 0.00100092 0.00052421 0.00062222]
2024-10-17 05:02:56,820 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:56,980 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:56,982 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.4]
2024-10-17 05:02:57,000 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:57,011 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.2) from file
2024-10-17 05:02:57,024 - pure_runtime.py[42] - DEBUG: load 6th block (block-6) (sparsity 0.0) from file
2024-10-17 05:02:57,024 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 517146528.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.4],
  'esti_latency': 0.0019029305745745287,
  'esti_test_accuracy': 0.7633000612258911,
  'is_relaxed': False,
  'model_size': 24062413.0,
  'update_swap_mem_cost': 8018488.0,
  'update_swap_time_cost': 0.04197287559509277}
2024-10-17 05:02:57,069 - gen_series_legodnn_models.py[28] - INFO: target model size: 23.346MB
2024-10-17 05:02:57,069 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 24480375.555555556B (23.346MB), try to adapt blocks
2024-10-17 05:02:57,071 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:57,104 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017290111541748046
2024-10-17 05:02:57,104 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010711040049791337, 0.0008310719989240168, 0.0008379840068519116, 0.0008669440001249314, 0.0003986879959702492, 0.0003754240013659001, 0.0009783679954707624, 0.0003803519979119301, 0.0010314240008592605, 0.0004883839972317219, 0.0005892160087823869]
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.303
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.977
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.186
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.798
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.881
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.978
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.656
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.858
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.597
2024-10-17 05:02:57,104 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.797
2024-10-17 05:02:57,105 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.542
2024-10-17 05:02:57,105 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:02:57,105 - optimal_runtime.py[116] - INFO: avg ratio: 1.8177422785909203
2024-10-17 05:02:57,105 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021051142643771165
2024-10-17 05:02:57,106 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00111881 0.00083107 0.00083316 0.00090569 0.00039869 0.00037483
 0.00097837 0.00043042 0.00101823 0.00050425 0.00063336]
2024-10-17 05:02:57,108 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:57,265 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:57,267 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.4]
2024-10-17 05:02:57,383 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:57,394 - pure_runtime.py[42] - DEBUG: load 3th block (block-3) (sparsity 0.0) from file
2024-10-17 05:02:57,395 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 543151776.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.6, 0.2, 0.2, 0.4],
  'esti_latency': 0.0019611083893980837,
  'esti_test_accuracy': 0.7641667127609253,
  'is_relaxed': False,
  'model_size': 24476173.0,
  'update_swap_mem_cost': 2967814.0,
  'update_swap_time_cost': 0.1279911994934082}
2024-10-17 05:02:57,443 - gen_series_legodnn_models.py[28] - INFO: target model size: 23.695MB
2024-10-17 05:02:57,443 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 24845518.111111112B (23.695MB), try to adapt blocks
2024-10-17 05:02:57,446 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:57,477 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017190784454345704
2024-10-17 05:02:57,478 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009502719976007938, 0.0008061759956181049, 0.0008653120025992394, 0.000887872003018856, 0.0004196159988641739, 0.00043030400946736337, 0.0009834879897534846, 0.0003712959997355938, 0.0010179840028285982, 0.0005046720020473003, 0.0006145599968731404]
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.469
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.038
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.104
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.834
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.787
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.726
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.647
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.903
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.618
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.739
2024-10-17 05:02:57,478 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.479
2024-10-17 05:02:57,479 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:02:57,479 - optimal_runtime.py[116] - INFO: avg ratio: 1.7506368142620874
2024-10-17 05:02:57,479 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002185807569250761
2024-10-17 05:02:57,479 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00099259 0.00080618 0.00086531 0.00088787 0.00041962 0.00042963
 0.00098349 0.00042017 0.00100497 0.00052107 0.00066061]
2024-10-17 05:02:57,481 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:57,629 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:57,630 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.2, 0.2, 0.4]
2024-10-17 05:02:57,644 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.0) from file
2024-10-17 05:02:57,645 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 550141472.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.2, 0.2, 0.4],
  'esti_latency': 0.0020451833049374615,
  'esti_test_accuracy': 0.7644667228062948,
  'is_relaxed': False,
  'model_size': 24697677.0,
  'update_swap_mem_cost': 1514428.0,
  'update_swap_time_cost': 0.014058351516723633}
2024-10-17 05:02:57,693 - gen_series_legodnn_models.py[28] - INFO: target model size: 24.043MB
2024-10-17 05:02:57,693 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 25210660.666666668B (24.043MB), try to adapt blocks
2024-10-17 05:02:57,696 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:57,730 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019316768646240233
2024-10-17 05:02:57,730 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001020224004983902, 0.0008651199974119663, 0.0009197119995951652, 0.0008828799985349178, 0.00042694399505853654, 0.0004119359999895096, 0.002606624025851488, 0.00037238399311900137, 0.0010549759902060032, 0.0005007359944283962, 0.000596223995089531]
2024-10-17 05:02:57,730 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.368
2024-10-17 05:02:57,730 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.899
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.980
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.844
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.757
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.800
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.003 = 0.622
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.898
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.561
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.752
2024-10-17 05:02:57,731 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.524
2024-10-17 05:02:57,731 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 10]),)
2024-10-17 05:02:57,731 - optimal_runtime.py[116] - INFO: avg ratio: 1.7383620555857136
2024-10-17 05:02:57,731 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022012417881117474
2024-10-17 05:02:57,732 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00106566 0.00086512 0.00091971 0.00088288 0.00042694 0.00041194
 0.00260662 0.0004214  0.00104148 0.000517   0.0006409 ]
2024-10-17 05:02:57,734 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:57,889 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:57,891 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.2, 0.4]
2024-10-17 05:02:57,910 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:57,916 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.2) from file
2024-10-17 05:02:57,922 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.2) from file
2024-10-17 05:02:57,923 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 554517024.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2, 0.2, 0.4],
  'esti_latency': 0.002100538230244807,
  'esti_test_accuracy': 0.7646333773930868,
  'is_relaxed': False,
  'model_size': 25159437.0,
  'update_swap_mem_cost': 4385548.0,
  'update_swap_time_cost': 0.03204512596130371}
2024-10-17 05:02:57,969 - gen_series_legodnn_models.py[28] - INFO: target model size: 24.391MB
2024-10-17 05:02:57,969 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 25575803.222222224B (24.391MB), try to adapt blocks
2024-10-17 05:02:57,972 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:58,005 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017726303100585938
2024-10-17 05:02:58,005 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010257919803261756, 0.0008288000002503396, 0.0008690240085124968, 0.0008985600024461745, 0.00041225599870085714, 0.0003914880007505417, 0.0009933119975030422, 0.00040566399320960046, 0.0009900479987263679, 0.0005353279970586299, 0.0006252480000257492]
2024-10-17 05:02:58,005 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.361
2024-10-17 05:02:58,005 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.983
2024-10-17 05:02:58,005 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.108
2024-10-17 05:02:58,005 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.812
2024-10-17 05:02:58,006 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.819
2024-10-17 05:02:58,006 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.897
2024-10-17 05:02:58,006 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.631
2024-10-17 05:02:58,006 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.907
2024-10-17 05:02:58,006 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.664
2024-10-17 05:02:58,006 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.639
2024-10-17 05:02:58,006 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.453
2024-10-17 05:02:58,006 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:02:58,006 - optimal_runtime.py[116] - INFO: avg ratio: 1.767010124543651
2024-10-17 05:02:58,006 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021655536357559676
2024-10-17 05:02:58,007 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107148 0.0008288  0.00086403 0.00089856 0.00041226 0.00039087
 0.00099331 0.00041939 0.00097739 0.00055272 0.00067209]
2024-10-17 05:02:58,009 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:58,172 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:58,172 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.4]
2024-10-17 05:02:58,191 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:58,197 - pure_runtime.py[42] - DEBUG: load 5th block (block-5) (sparsity 0.0) from file
2024-10-17 05:02:58,198 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 575295392.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.4],
  'esti_latency': 0.00205456805141503,
  'esti_test_accuracy': 0.765166719754537,
  'is_relaxed': False,
  'model_size': 25491725.0,
  'update_swap_mem_cost': 2381008.0,
  'update_swap_time_cost': 0.0247805118560791}
2024-10-17 05:02:58,244 - gen_series_legodnn_models.py[28] - INFO: target model size: 24.739MB
2024-10-17 05:02:58,244 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 25940945.77777778B (24.739MB), try to adapt blocks
2024-10-17 05:02:58,247 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:58,278 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017290847778320313
2024-10-17 05:02:58,278 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.00104262400791049, 0.000807199988514185, 0.0009108159951865673, 0.0008762559927999973, 0.00042508799582719804, 0.00043459200114011765, 0.0009776960015296938, 0.00040556799620389944, 0.0010184320136904718, 0.0004998079910874366, 0.000576992005109787]
2024-10-17 05:02:58,278 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.339
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.036
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.999
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.858
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.764
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.706
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.657
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.908
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.617
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.756
2024-10-17 05:02:58,279 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.575
2024-10-17 05:02:58,279 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:02:58,279 - optimal_runtime.py[116] - INFO: avg ratio: 1.7301194422845583
2024-10-17 05:02:58,279 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022117289165713813
2024-10-17 05:02:58,280 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108906 0.0008072  0.00091082 0.00087626 0.00042509 0.00043459
 0.0009777  0.00041929 0.00100541 0.00051605 0.00062022]
2024-10-17 05:02:58,282 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:58,435 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:58,437 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.4]
2024-10-17 05:02:58,456 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:58,463 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.0) from file
2024-10-17 05:02:58,463 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 576649888.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.4],
  'esti_latency': 0.002124116850175743,
  'esti_test_accuracy': 0.7652000586191813,
  'is_relaxed': False,
  'model_size': 25858765.0,
  'update_swap_mem_cost': 4142992.0,
  'update_swap_time_cost': 0.02585911750793457}
2024-10-17 05:02:58,511 - gen_series_legodnn_models.py[28] - INFO: target model size: 25.087MB
2024-10-17 05:02:58,511 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 26306088.333333332B (25.087MB), try to adapt blocks
2024-10-17 05:02:58,513 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:58,545 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017344127655029296
2024-10-17 05:02:58,545 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.000963744007050991, 0.0007960640043020246, 0.00088051201030612, 0.0008472320027649404, 0.00038966400548815725, 0.00039155200123786927, 0.0010760959833860397, 0.0004482559934258461, 0.0009924479834735394, 0.0004889600053429603, 0.000575263999402523]
2024-10-17 05:02:58,545 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.448
2024-10-17 05:02:58,545 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.064
2024-10-17 05:02:58,545 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.080
2024-10-17 05:02:58,545 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.922
2024-10-17 05:02:58,545 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.925
2024-10-17 05:02:58,546 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.894
2024-10-17 05:02:58,546 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.506
2024-10-17 05:02:58,546 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.784
2024-10-17 05:02:58,546 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.660
2024-10-17 05:02:58,546 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.794
2024-10-17 05:02:58,546 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.580
2024-10-17 05:02:58,546 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 7, 8, 9]),)
2024-10-17 05:02:58,546 - optimal_runtime.py[116] - INFO: avg ratio: 1.829739133440089
2024-10-17 05:02:58,546 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.00209131188686379
2024-10-17 05:02:58,547 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00100667 0.00079606 0.00087545 0.00084723 0.00038966 0.00039155
 0.0010761  0.00044826 0.00097976 0.00050485 0.00061837]
2024-10-17 05:02:58,549 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:58,704 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:58,706 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.4]
2024-10-17 05:02:58,724 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:02:58,725 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 590438560.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.4],
  'esti_latency': 0.0020020935195345824,
  'esti_test_accuracy': 0.7654333909352621,
  'is_relaxed': False,
  'model_size': 25969549.0,
  'update_swap_mem_cost': 866580.0,
  'update_swap_time_cost': 0.018668174743652344}
2024-10-17 05:02:58,772 - gen_series_legodnn_models.py[28] - INFO: target model size: 25.436MB
2024-10-17 05:02:58,772 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 26671230.888888888B (25.436MB), try to adapt blocks
2024-10-17 05:02:58,774 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:58,807 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017760608673095705
2024-10-17 05:02:58,807 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010018880143761635, 0.0008618880137801171, 0.000870975997298956, 0.0008830400072038174, 0.0004127040021121502, 0.00041152000054717067, 0.0009469439983367921, 0.0004364799968898296, 0.0010531200133264064, 0.00048774400725960733, 0.0006320960074663163]
2024-10-17 05:02:58,807 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.393
2024-10-17 05:02:58,807 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.907
2024-10-17 05:02:58,807 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.091
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.844
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.817
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.802
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.711
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.832
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.564
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.799
2024-10-17 05:02:58,808 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.438
2024-10-17 05:02:58,808 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:02:58,808 - optimal_runtime.py[116] - INFO: avg ratio: 1.7844641761410882
2024-10-17 05:02:58,808 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002144372103842427
2024-10-17 05:02:58,809 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00104651 0.00086189 0.00087098 0.00088304 0.0004127  0.00041152
 0.00094694 0.00043648 0.00103965 0.00050359 0.00067946]
2024-10-17 05:02:58,811 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:58,962 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:58,963 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.4]
2024-10-17 05:02:58,964 - gen_series_legodnn_models.py[28] - INFO: target model size: 25.784MB
2024-10-17 05:02:58,964 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 27036373.444444444B (25.784MB), try to adapt blocks
2024-10-17 05:02:58,970 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:59,008 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01913632011413574
2024-10-17 05:02:59,009 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011192959994077683, 0.0008535039946436882, 0.0009443199895322323, 0.0009240639954805373, 0.00040380799397826195, 0.0004093440063297749, 0.0010066880024969577, 0.0004776960015296936, 0.001030719995498657, 0.0005027839876711369, 0.0006904640048742294]
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.247
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.925
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.928
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.762
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.857
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.812
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.609
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.674
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.598
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.745
2024-10-17 05:02:59,009 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.316
2024-10-17 05:02:59,010 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:02:59,010 - optimal_runtime.py[116] - INFO: avg ratio: 1.722498194219876
2024-10-17 05:02:59,010 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002221514781532857
2024-10-17 05:02:59,011 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00116915 0.0008535  0.00094432 0.00092406 0.00040381 0.00040934
 0.00100669 0.0004777  0.00101754 0.00051912 0.0007422 ]
2024-10-17 05:02:59,013 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:59,170 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:59,174 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2, 0.4]
2024-10-17 05:02:59,185 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.4) from file
2024-10-17 05:02:59,201 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.0) from file
2024-10-17 05:02:59,202 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 581369568.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2, 0.4],
  'esti_latency': 0.0020625542771086033,
  'esti_test_accuracy': 0.765500028928121,
  'is_relaxed': False,
  'model_size': 26958477.0,
  'update_swap_mem_cost': 16013678.0,
  'update_swap_time_cost': 0.027771949768066406}
2024-10-17 05:02:59,250 - gen_series_legodnn_models.py[28] - INFO: target model size: 26.132MB
2024-10-17 05:02:59,250 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 27401516.0B (26.132MB), try to adapt blocks
2024-10-17 05:02:59,253 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:59,285 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017589216232299806
2024-10-17 05:02:59,286 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001060576006770134, 0.0009270399995148183, 0.0008858879953622818, 0.0008538880012929437, 0.00039334400370717054, 0.00041200000047683714, 0.0009522239975631238, 0.00038121599704027176, 0.0010956159979104998, 0.0005035839974880218, 0.0006000639908015729]
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.316
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.773
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.056
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.907
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.907
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.800
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.702
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.918
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.484
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.742
2024-10-17 05:02:59,286 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.514
2024-10-17 05:02:59,287 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:02:59,287 - optimal_runtime.py[116] - INFO: avg ratio: 1.8210401825234896
2024-10-17 05:02:59,287 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002101301902257036
2024-10-17 05:02:59,287 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00110781 0.00092704 0.00088589 0.00085389 0.00039334 0.000412
 0.00095222 0.00041705 0.00109562 0.00051995 0.00064502]
2024-10-17 05:02:59,289 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:59,443 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:59,445 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.4]
2024-10-17 05:02:59,459 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.2) from file
2024-10-17 05:02:59,460 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 594988256.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.4],
  'esti_latency': 0.001979100341630591,
  'esti_test_accuracy': 0.7660333712895712,
  'is_relaxed': False,
  'model_size': 27388493.0,
  'update_swap_mem_cost': 2368572.0,
  'update_swap_time_cost': 0.014496564865112305}
2024-10-17 05:02:59,510 - gen_series_legodnn_models.py[28] - INFO: target model size: 26.480MB
2024-10-17 05:02:59,510 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 27766658.555555556B (26.480MB), try to adapt blocks
2024-10-17 05:02:59,513 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:59,546 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017952096939086913
2024-10-17 05:02:59,546 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010268159955739975, 0.0008083520084619522, 0.0009146880097687245, 0.0008860479928553106, 0.0004355520009994507, 0.00042582400515675544, 0.0009550079964101315, 0.0005071680024266243, 0.0010984959937632081, 0.0005065280050039291, 0.0005857599973678588]
2024-10-17 05:02:59,547 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.359
2024-10-17 05:02:59,547 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.033
2024-10-17 05:02:59,547 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.991
2024-10-17 05:02:59,547 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.837
2024-10-17 05:02:59,548 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.722
2024-10-17 05:02:59,548 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.741
2024-10-17 05:02:59,548 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.697
2024-10-17 05:02:59,548 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.525
2024-10-17 05:02:59,548 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.480
2024-10-17 05:02:59,548 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.732
2024-10-17 05:02:59,549 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.551
2024-10-17 05:02:59,550 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  7,  9, 10]),)
2024-10-17 05:02:59,550 - optimal_runtime.py[116] - INFO: avg ratio: 1.6866061312843699
2024-10-17 05:02:59,550 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022687900444836773
2024-10-17 05:02:59,552 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00107255 0.00080835 0.00091469 0.00088605 0.00043555 0.00042582
 0.00095501 0.00052432 0.0010985  0.00052299 0.00062965]
2024-10-17 05:02:59,558 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:02:59,748 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:02:59,750 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.4]
2024-10-17 05:02:59,770 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:02:59,779 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.0) from file
2024-10-17 05:02:59,780 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 596342752.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.4],
  'esti_latency': 0.0021680045970321722,
  'esti_test_accuracy': 0.7660667101542155,
  'is_relaxed': False,
  'model_size': 27755533.0,
  'update_swap_mem_cost': 4142992.0,
  'update_swap_time_cost': 0.028786897659301758}
2024-10-17 05:02:59,842 - gen_series_legodnn_models.py[28] - INFO: target model size: 26.829MB
2024-10-17 05:02:59,842 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 28131801.111111112B (26.829MB), try to adapt blocks
2024-10-17 05:02:59,845 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:02:59,895 - optimal_runtime.py[77] - INFO: infer time of current model: 0.028382816314697266
2024-10-17 05:02:59,896 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0013227839991450312, 0.0011895039901137349, 0.001190911997109652, 0.001191039990633726, 0.000530016005039215, 0.0005284160077571869, 0.0012710400037467481, 0.0005667199939489364, 0.005090591918677091, 0.0006909119933843613, 0.0008532480113208294]
2024-10-17 05:02:59,896 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.055
2024-10-17 05:02:59,896 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.381
2024-10-17 05:02:59,896 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.538
2024-10-17 05:02:59,896 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.367
2024-10-17 05:02:59,896 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.415
2024-10-17 05:02:59,896 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.403
2024-10-17 05:02:59,896 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.275
2024-10-17 05:02:59,897 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.411
2024-10-17 05:02:59,897 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.005 = 0.319
2024-10-17 05:02:59,897 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.270
2024-10-17 05:02:59,897 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.065
2024-10-17 05:02:59,897 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10]),)
2024-10-17 05:02:59,897 - optimal_runtime.py[116] - INFO: avg ratio: 1.318057147293357
2024-10-17 05:02:59,897 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002903178521114184
2024-10-17 05:02:59,898 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0013817  0.0011895  0.00118406 0.00119104 0.00053002 0.00052842
 0.00127104 0.00056672 0.00509059 0.00071336 0.00091718]
2024-10-17 05:02:59,901 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:00,058 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:00,060 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.4]
2024-10-17 05:03:00,084 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:03:00,085 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 610131424.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.4],
  'esti_latency': 0.002757887840974041,
  'esti_test_accuracy': 0.7663000424702963,
  'is_relaxed': False,
  'model_size': 27866317.0,
  'update_swap_mem_cost': 866580.0,
  'update_swap_time_cost': 0.025143861770629883}
2024-10-17 05:03:00,158 - gen_series_legodnn_models.py[28] - INFO: target model size: 27.177MB
2024-10-17 05:03:00,158 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 28496943.666666668B (27.177MB), try to adapt blocks
2024-10-17 05:03:00,161 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:00,206 - optimal_runtime.py[77] - INFO: infer time of current model: 0.02415078353881836
2024-10-17 05:03:00,207 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0014630080088973048, 0.0011855040043592452, 0.0012168000005185603, 0.0011742719933390618, 0.0005334080010652542, 0.0005295040011405945, 0.0012654400020837785, 0.0005681599974632263, 0.001386976018548012, 0.0006143039986491203, 0.0007461440116167069]
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 0.954
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.386
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.497
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.386
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.406
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.400
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.280
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.408
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.172
2024-10-17 05:03:00,207 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.428
2024-10-17 05:03:00,208 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.218
2024-10-17 05:03:00,208 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  9, 10]),)
2024-10-17 05:03:00,208 - optimal_runtime.py[116] - INFO: avg ratio: 1.3641515889096467
2024-10-17 05:03:00,208 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002805080630871557
2024-10-17 05:03:00,209 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00152817 0.0011855  0.0012168  0.00117427 0.00053341 0.0005295
 0.00126544 0.00056816 0.00138698 0.00063426 0.00080205]
2024-10-17 05:03:00,212 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:00,357 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:00,358 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.4]
2024-10-17 05:03:00,359 - gen_series_legodnn_models.py[28] - INFO: target model size: 27.525MB
2024-10-17 05:03:00,359 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 28862086.222222224B (27.525MB), try to adapt blocks
2024-10-17 05:03:00,364 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:00,404 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01908252716064453
2024-10-17 05:03:00,404 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011758399978280068, 0.0009368640035390855, 0.0009507200047373772, 0.0009077119976282119, 0.00044697600230574606, 0.0004274880029261112, 0.0009887040071189404, 0.000468032006174326, 0.0011107199862599372, 0.0005012799985706807, 0.000597120001912117]
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.187
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.754
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.915
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.794
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.678
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.735
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.639
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.709
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.464
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.750
2024-10-17 05:03:00,405 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.522
2024-10-17 05:03:00,405 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:03:00,406 - optimal_runtime.py[116] - INFO: avg ratio: 1.6715305972757095
2024-10-17 05:03:00,406 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.00228925226128657
2024-10-17 05:03:00,406 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00122821 0.00093686 0.00095072 0.00090771 0.00044698 0.00042749
 0.0009887  0.00046803 0.00111072 0.00051757 0.00064186]
2024-10-17 05:03:00,408 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:00,562 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:00,563 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.4]
2024-10-17 05:03:00,564 - gen_series_legodnn_models.py[28] - INFO: target model size: 27.873MB
2024-10-17 05:03:00,565 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 29227228.77777778B (27.873MB), try to adapt blocks
2024-10-17 05:03:00,570 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:00,610 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019433759689331056
2024-10-17 05:03:00,610 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011385920010507106, 0.000882208000868559, 0.000959231998771429, 0.0009169280007481575, 0.00043283199891448027, 0.00043551999703049656, 0.001022272005677223, 0.0004720000065863132, 0.0011697919853031636, 0.0005186560042202474, 0.0006144960075616837]
2024-10-17 05:03:00,610 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.226
2024-10-17 05:03:00,610 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.863
2024-10-17 05:03:00,610 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.898
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.775
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.733
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.703
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.585
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.694
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.390
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.692
2024-10-17 05:03:00,611 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.479
2024-10-17 05:03:00,611 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  7,  9, 10]),)
2024-10-17 05:03:00,611 - optimal_runtime.py[116] - INFO: avg ratio: 1.6658319970200322
2024-10-17 05:03:00,611 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022970835032994586
2024-10-17 05:03:00,612 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0011893  0.00088221 0.00095923 0.00091693 0.00043283 0.00043552
 0.00102227 0.000472   0.00116979 0.00053551 0.00066054]
2024-10-17 05:03:00,614 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:00,769 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:00,770 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2]
2024-10-17 05:03:00,788 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:03:00,804 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.2) from file
2024-10-17 05:03:00,817 - pure_runtime.py[42] - DEBUG: load 10th block (block-10) (sparsity 0.2) from file
2024-10-17 05:03:00,818 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 602909600.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2],
  'esti_latency': 0.002230622807532609,
  'esti_test_accuracy': 0.7663333813349406,
  'is_relaxed': False,
  'model_size': 29149325.0,
  'update_swap_mem_cost': 31320798.0,
  'update_swap_time_cost': 0.047423601150512695}
2024-10-17 05:03:00,865 - gen_series_legodnn_models.py[28] - INFO: target model size: 28.221MB
2024-10-17 05:03:00,865 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 29592371.333333332B (28.221MB), try to adapt blocks
2024-10-17 05:03:00,867 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:00,900 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017955135345458985
2024-10-17 05:03:00,901 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001062176011502743, 0.0008638079911470412, 0.0008973759934306145, 0.0009541120007634164, 0.000455231998115778, 0.0004184319972991944, 0.0009795839935541153, 0.00045049599558115005, 0.0010338880009949207, 0.0005088960006833077, 0.0006759039871394634]
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.314
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.902
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.041
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.706
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.647
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.772
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.654
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.775
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.593
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.724
2024-10-17 05:03:00,901 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.402
2024-10-17 05:03:00,902 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:00,902 - optimal_runtime.py[116] - INFO: avg ratio: 1.6960896447589022
2024-10-17 05:03:00,902 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022561043347252147
2024-10-17 05:03:00,902 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00110948 0.00086381 0.00089221 0.00095411 0.00045523 0.00041843
 0.00097958 0.0004505  0.00102067 0.00052543 0.00069658]
2024-10-17 05:03:00,904 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:01,064 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:01,066 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2]
2024-10-17 05:03:01,086 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:03:01,086 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 616698272.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2],
  'esti_latency': 0.0021848136144594246,
  'esti_test_accuracy': 0.7665667136510214,
  'is_relaxed': False,
  'model_size': 29260109.0,
  'update_swap_mem_cost': 866580.0,
  'update_swap_time_cost': 0.020027875900268555}
2024-10-17 05:03:01,137 - gen_series_legodnn_models.py[28] - INFO: target model size: 28.570MB
2024-10-17 05:03:01,137 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 29957513.888888888B (28.570MB), try to adapt blocks
2024-10-17 05:03:01,139 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:01,171 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017391584396362306
2024-10-17 05:03:01,172 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010072640068829063, 0.0008971520029008387, 0.0009194879904389382, 0.0009167360104620456, 0.00041219200566411016, 0.00039315200597047804, 0.000924575999379158, 0.0004504319988191128, 0.0009874559976160526, 0.0004882880114018917, 0.0006337599977850914]
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.386
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.832
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.980
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.776
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.819
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.886
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.752
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.776
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.668
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.797
2024-10-17 05:03:01,172 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.496
2024-10-17 05:03:01,173 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:01,173 - optimal_runtime.py[116] - INFO: avg ratio: 1.7882617058573838
2024-10-17 05:03:01,173 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021398183426337273
2024-10-17 05:03:01,173 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00105213 0.00089715 0.00091949 0.00091674 0.00041219 0.00039315
 0.00092458 0.00045043 0.00097483 0.00050415 0.00065314]
2024-10-17 05:03:01,175 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:01,332 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:01,334 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2]
2024-10-17 05:03:01,335 - gen_series_legodnn_models.py[28] - INFO: target model size: 28.918MB
2024-10-17 05:03:01,335 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 30322656.444444444B (28.918MB), try to adapt blocks
2024-10-17 05:03:01,341 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:01,381 - optimal_runtime.py[77] - INFO: infer time of current model: 0.02177782440185547
2024-10-17 05:03:01,382 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010733119957149029, 0.0009007680006325246, 0.0009323519989848138, 0.0008822399899363517, 0.00041347200050950055, 0.0004121599942445755, 0.0010229440145194532, 0.00047619199752807614, 0.0010395200029015542, 0.0004957120083272457, 0.0006406079865992069]
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.300
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.824
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.953
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.845
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.814
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.799
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.584
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.680
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.584
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.770
2024-10-17 05:03:01,382 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.480
2024-10-17 05:03:01,383 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:01,383 - optimal_runtime.py[116] - INFO: avg ratio: 1.7375668851015864
2024-10-17 05:03:01,383 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022022491522099823
2024-10-17 05:03:01,384 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00112111 0.00090077 0.00093235 0.00088224 0.00041347 0.00041216
 0.00102294 0.00047619 0.00102623 0.00051182 0.0006602 ]
2024-10-17 05:03:01,386 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:01,544 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:01,546 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2, 0.2]
2024-10-17 05:03:01,560 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.4) from file
2024-10-17 05:03:01,576 - pure_runtime.py[42] - DEBUG: load 8th block (block-8) (sparsity 0.0) from file
2024-10-17 05:03:01,577 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 607629280.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.2, 0.2],
  'esti_latency': 0.002077835804223196,
  'esti_test_accuracy': 0.7666333516438802,
  'is_relaxed': False,
  'model_size': 30249037.0,
  'update_swap_mem_cost': 16013678.0,
  'update_swap_time_cost': 0.031065702438354492}
2024-10-17 05:03:01,627 - gen_series_legodnn_models.py[28] - INFO: target model size: 29.266MB
2024-10-17 05:03:01,627 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 30687799.0B (29.266MB), try to adapt blocks
2024-10-17 05:03:01,630 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:01,662 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01737740707397461
2024-10-17 05:03:01,662 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.000971168003976345, 0.0008172480128705501, 0.0009033920094370843, 0.0008488320037722586, 0.00039248000830411907, 0.00040355200693011284, 0.0009526079967617988, 0.0004074240066111088, 0.0010989440083503722, 0.0004945920035243033, 0.0007080959975719451]
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.437
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.011
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.016
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.918
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.911
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.838
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.701
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.794
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.480
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.774
2024-10-17 05:03:01,662 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.339
2024-10-17 05:03:01,663 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:01,663 - optimal_runtime.py[116] - INFO: avg ratio: 1.8226014746069328
2024-10-17 05:03:01,663 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0020995018674877094
2024-10-17 05:03:01,664 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00101442 0.00081725 0.00090339 0.00084883 0.00039248 0.00040355
 0.00095261 0.00044572 0.00109894 0.00051066 0.00072975]
2024-10-17 05:03:01,666 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:01,824 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:01,826 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2]
2024-10-17 05:03:01,840 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.2) from file
2024-10-17 05:03:01,840 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 621247968.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.2],
  'esti_latency': 0.0020039392865017535,
  'esti_test_accuracy': 0.7671666940053304,
  'is_relaxed': False,
  'model_size': 30679053.0,
  'update_swap_mem_cost': 2368572.0,
  'update_swap_time_cost': 0.014307498931884766}
2024-10-17 05:03:01,892 - gen_series_legodnn_models.py[28] - INFO: target model size: 29.614MB
2024-10-17 05:03:01,892 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 31052941.555555556B (29.614MB), try to adapt blocks
2024-10-17 05:03:01,895 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:01,928 - optimal_runtime.py[77] - INFO: infer time of current model: 0.018067455291748045
2024-10-17 05:03:01,928 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010367039889097212, 0.0008561280034482479, 0.000925087995827198, 0.0009056640006601811, 0.00043040000274777407, 0.0004128639958798885, 0.000967423990368843, 0.00043843199312686924, 0.0011246719919145106, 0.0005144000016152858, 0.0006656959876418112]
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.346
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.919
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.968
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.798
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.742
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.796
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.675
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.765
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.446
2024-10-17 05:03:01,928 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.706
2024-10-17 05:03:01,929 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.424
2024-10-17 05:03:01,929 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:01,929 - optimal_runtime.py[116] - INFO: avg ratio: 1.7468721199513688
2024-10-17 05:03:01,929 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002190518215912471
2024-10-17 05:03:01,930 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108288 0.00085613 0.00092509 0.00090566 0.0004304  0.00041286
 0.00096742 0.00045326 0.00112467 0.00053111 0.00068606]
2024-10-17 05:03:01,932 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:02,089 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:02,091 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:02,109 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.2) from file
2024-10-17 05:03:02,117 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.0) from file
2024-10-17 05:03:02,118 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 622602464.0,
  'blocks_sparsity': [0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2],
  'esti_latency': 0.0021126247235025513,
  'esti_test_accuracy': 0.7672000328699747,
  'is_relaxed': False,
  'model_size': 31046093.0,
  'update_swap_mem_cost': 4142992.0,
  'update_swap_time_cost': 0.026621103286743164}
2024-10-17 05:03:02,166 - gen_series_legodnn_models.py[28] - INFO: target model size: 29.963MB
2024-10-17 05:03:02,167 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 31418084.111111112B (29.963MB), try to adapt blocks
2024-10-17 05:03:02,169 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:02,201 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017579904556274416
2024-10-17 05:03:02,201 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001039872005581856, 0.000821280013769865, 0.000877600010484457, 0.0009240319952368736, 0.0004299839995801449, 0.0004118079990148544, 0.0009984319992363455, 0.0004630399905145168, 0.0010863999985158443, 0.0004897600039839745, 0.0006576639860868454]
2024-10-17 05:03:02,201 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.342
2024-10-17 05:03:02,201 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.001
2024-10-17 05:03:02,201 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.087
2024-10-17 05:03:02,201 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.762
2024-10-17 05:03:02,201 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.744
2024-10-17 05:03:02,201 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.801
2024-10-17 05:03:02,201 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.623
2024-10-17 05:03:02,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.727
2024-10-17 05:03:02,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.497
2024-10-17 05:03:02,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.792
2024-10-17 05:03:02,202 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.441
2024-10-17 05:03:02,202 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:02,202 - optimal_runtime.py[116] - INFO: avg ratio: 1.7064381161182063
2024-10-17 05:03:02,202 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002242422484284241
2024-10-17 05:03:02,203 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108619 0.00082128 0.00087255 0.00092403 0.00042998 0.00041181
 0.00099843 0.00046304 0.0010864  0.00050567 0.00067778]
2024-10-17 05:03:02,205 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:02,359 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:02,361 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:02,381 - pure_runtime.py[42] - DEBUG: load 2th block (block-2) (sparsity 0.0) from file
2024-10-17 05:03:02,382 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 636391136.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2],
  'esti_latency': 0.002160082495020013,
  'esti_test_accuracy': 0.7674333651860555,
  'is_relaxed': False,
  'model_size': 31156877.0,
  'update_swap_mem_cost': 866580.0,
  'update_swap_time_cost': 0.020556211471557617}
2024-10-17 05:03:02,431 - gen_series_legodnn_models.py[28] - INFO: target model size: 30.311MB
2024-10-17 05:03:02,432 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 31783226.666666668B (30.311MB), try to adapt blocks
2024-10-17 05:03:02,434 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:02,467 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017771711349487306
2024-10-17 05:03:02,467 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009858880043029786, 0.0008926079943776133, 0.0008670080043375493, 0.0008831039927899838, 0.00042012799158692364, 0.0004158400036394596, 0.000989311996847391, 0.0004697919972240925, 0.0011474559977650643, 0.0004917760044336319, 0.0006449919901788235]
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.416
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.841
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.100
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.843
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.785
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.783
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.638
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.702
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.417
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.784
2024-10-17 05:03:02,468 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.470
2024-10-17 05:03:02,468 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:02,469 - optimal_runtime.py[116] - INFO: avg ratio: 1.7681633408181827
2024-10-17 05:03:02,469 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021641412370038424
2024-10-17 05:03:02,469 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0010298  0.00089261 0.00086701 0.0008831  0.00042013 0.00041584
 0.00098931 0.00046979 0.00114746 0.00050775 0.00066472]
2024-10-17 05:03:02,471 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:02,622 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:02,624 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:02,625 - gen_series_legodnn_models.py[28] - INFO: target model size: 30.659MB
2024-10-17 05:03:02,625 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 32148369.222222224B (30.659MB), try to adapt blocks
2024-10-17 05:03:02,632 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:02,668 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019667104721069337
2024-10-17 05:03:02,668 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011666880063712596, 0.0009315839931368829, 0.0009608960002660752, 0.000969888012856245, 0.0004411520026624203, 0.00043935999274253846, 0.0010127999894320966, 0.0005050240010023117, 0.0011624960005283357, 0.0005272639952600002, 0.0006731840036809445]
2024-10-17 05:03:02,668 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.196
2024-10-17 05:03:02,668 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.764
2024-10-17 05:03:02,668 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.895
2024-10-17 05:03:02,668 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.679
2024-10-17 05:03:02,668 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.700
2024-10-17 05:03:02,668 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.688
2024-10-17 05:03:02,668 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.600
2024-10-17 05:03:02,669 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.584
2024-10-17 05:03:02,669 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.399
2024-10-17 05:03:02,669 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.664
2024-10-17 05:03:02,669 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.408
2024-10-17 05:03:02,669 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:02,669 - optimal_runtime.py[116] - INFO: avg ratio: 1.668257014537782
2024-10-17 05:03:02,669 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022937444088513654
2024-10-17 05:03:02,670 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00121865 0.00093158 0.0009609  0.00096989 0.00044115 0.00043936
 0.0010128  0.00050502 0.0011625  0.00054439 0.00069377]
2024-10-17 05:03:02,672 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:02,826 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:02,828 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:02,829 - gen_series_legodnn_models.py[28] - INFO: target model size: 31.007MB
2024-10-17 05:03:02,829 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 32513511.77777778B (31.007MB), try to adapt blocks
2024-10-17 05:03:02,834 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:02,873 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019501951217651366
2024-10-17 05:03:02,874 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011619840078055858, 0.0009099839925765991, 0.001029568001627922, 0.0009441279955208303, 0.00044147200137376784, 0.0004323199987411499, 0.001015231989324093, 0.0004612479992210865, 0.0011642240025103094, 0.0005334400050342083, 0.0006679679900407792]
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.201
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.806
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.769
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.724
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.699
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.715
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.596
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.734
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.397
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.645
2024-10-17 05:03:02,874 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.419
2024-10-17 05:03:02,875 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([2, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:02,875 - optimal_runtime.py[116] - INFO: avg ratio: 1.6974034863660008
2024-10-17 05:03:02,875 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022543580417732287
2024-10-17 05:03:02,875 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00121374 0.00090998 0.00102957 0.00094413 0.00044147 0.00043232
 0.00101523 0.00046125 0.00116422 0.00055077 0.0006884 ]
2024-10-17 05:03:02,878 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:03,029 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:03,031 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:03,032 - gen_series_legodnn_models.py[28] - INFO: target model size: 31.356MB
2024-10-17 05:03:03,032 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 32878654.333333332B (31.356MB), try to adapt blocks
2024-10-17 05:03:03,037 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:03,076 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019708511352539062
2024-10-17 05:03:03,076 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011257599890232089, 0.0009397439919412137, 0.000978656005114317, 0.000966847989708185, 0.0004348480105400085, 0.00046083199605345725, 0.001030591994524002, 0.00047030399739742283, 0.0011489279866218568, 0.0005352319888770581, 0.0006981440111994743]
2024-10-17 05:03:03,076 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.240
2024-10-17 05:03:03,076 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.749
2024-10-17 05:03:03,076 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.861
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.684
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.725
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.609
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.572
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.701
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.415
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.639
2024-10-17 05:03:03,077 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.358
2024-10-17 05:03:03,077 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:03,077 - optimal_runtime.py[116] - INFO: avg ratio: 1.6683265213741127
2024-10-17 05:03:03,077 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002293648845473832
2024-10-17 05:03:03,078 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0011759  0.00093974 0.00097866 0.00096685 0.00043485 0.00046083
 0.00103059 0.0004703  0.00114893 0.00055262 0.0007195 ]
2024-10-17 05:03:03,080 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:03,235 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:03,237 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:03,238 - gen_series_legodnn_models.py[28] - INFO: target model size: 31.704MB
2024-10-17 05:03:03,238 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 33243796.888888888B (31.704MB), try to adapt blocks
2024-10-17 05:03:03,244 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:03,279 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019205919265747072
2024-10-17 05:03:03,279 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011247039921581743, 0.0009425920061767101, 0.0009685759954154491, 0.000919967994093895, 0.0004308479949831963, 0.0004206079952418804, 0.0009678399935364722, 0.0004466560035943985, 0.0011841599904000761, 0.0005223360061645507, 0.0006803519874811173]
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.241
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.743
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.880
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.770
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.741
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.763
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.674
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.791
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.373
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.680
2024-10-17 05:03:03,280 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.393
2024-10-17 05:03:03,280 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:03,280 - optimal_runtime.py[116] - INFO: avg ratio: 1.7373075682125358
2024-10-17 05:03:03,281 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022025778679823156
2024-10-17 05:03:03,281 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0011748  0.00094259 0.00096858 0.00091997 0.00043085 0.00042061
 0.00096784 0.00044666 0.00118416 0.00053931 0.00070116]
2024-10-17 05:03:03,283 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:03,436 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:03,438 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:03,439 - gen_series_legodnn_models.py[28] - INFO: target model size: 32.052MB
2024-10-17 05:03:03,439 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 33608939.44444445B (32.052MB), try to adapt blocks
2024-10-17 05:03:03,445 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:03,483 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01868185615539551
2024-10-17 05:03:03,483 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011248959973454474, 0.0008593599982559681, 0.0009062720090150833, 0.0009069119878113268, 0.0004304319992661476, 0.0004195199981331825, 0.0009558720067143441, 0.0004440319985151291, 0.0011585599966347216, 0.0005421439968049526, 0.0006426240019500256]
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.241
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.912
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.009
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.795
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.742
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.768
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.695
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.801
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.404
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.618
2024-10-17 05:03:03,484 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.475
2024-10-17 05:03:03,484 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 3,  4,  5,  6,  7,  9, 10]),)
2024-10-17 05:03:03,485 - optimal_runtime.py[116] - INFO: avg ratio: 1.6992398303386598
2024-10-17 05:03:03,485 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022519217895572005
2024-10-17 05:03:03,485 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.001175   0.00085936 0.00090627 0.00090691 0.00043043 0.00041952
 0.00095587 0.00044403 0.00115856 0.00055976 0.00066228]
2024-10-17 05:03:03,487 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:03,641 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:03,643 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:03,644 - gen_series_legodnn_models.py[28] - INFO: target model size: 32.400MB
2024-10-17 05:03:03,644 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 33974082.0B (32.400MB), try to adapt blocks
2024-10-17 05:03:03,649 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:03,688 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019280351638793946
2024-10-17 05:03:03,688 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011400639973580837, 0.000894527990370989, 0.0009473279975354671, 0.0009385279975831508, 0.00042732800170779226, 0.0004514880031347275, 0.001027840007096529, 0.0004612159989774227, 0.0011225919909775258, 0.0005299519971013069, 0.000666176002472639]
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.224
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.837
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.922
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.735
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.755
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.642
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.576
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.734
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.448
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.656
2024-10-17 05:03:03,689 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.423
2024-10-17 05:03:03,689 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:03,689 - optimal_runtime.py[116] - INFO: avg ratio: 1.6495295785156505
2024-10-17 05:03:03,690 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0023197857434399454
2024-10-17 05:03:03,690 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00119084 0.00089453 0.00094733 0.00093853 0.00042733 0.00045149
 0.00102784 0.00046122 0.00112259 0.00054717 0.00068655]
2024-10-17 05:03:03,692 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:03,846 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:03,848 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:03,849 - gen_series_legodnn_models.py[28] - INFO: target model size: 32.748MB
2024-10-17 05:03:03,849 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 34339224.55555555B (32.748MB), try to adapt blocks
2024-10-17 05:03:03,855 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:03,893 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019646047592163086
2024-10-17 05:03:03,893 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0013513600043952466, 0.0009485119991004468, 0.0009850879944860937, 0.0009519360102713109, 0.00043702400475740437, 0.0004384000040590763, 0.001041599988937378, 0.0004758080020546913, 0.0011338240019977094, 0.0005439679995179177, 0.0006837439946830272]
2024-10-17 05:03:03,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.033
2024-10-17 05:03:03,893 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.732
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.849
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.710
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.716
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.692
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.556
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.681
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.434
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.613
2024-10-17 05:03:03,894 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.386
2024-10-17 05:03:03,894 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([ 1,  3,  4,  5,  6,  7,  8,  9, 10]),)
2024-10-17 05:03:03,894 - optimal_runtime.py[116] - INFO: avg ratio: 1.6133314782610215
2024-10-17 05:03:03,894 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002371834462529472
2024-10-17 05:03:03,895 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00141155 0.00094851 0.00098509 0.00095194 0.00043702 0.0004384
 0.0010416  0.00047581 0.00113382 0.00056164 0.00070466]
2024-10-17 05:03:03,897 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:04,049 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:04,051 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2]
2024-10-17 05:03:04,052 - gen_series_legodnn_models.py[28] - INFO: target model size: 33.097MB
2024-10-17 05:03:04,052 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 34704367.11111111B (33.097MB), try to adapt blocks
2024-10-17 05:03:04,058 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:04,097 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019610336303710937
2024-10-17 05:03:04,097 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0011763200089335442, 0.0009246079996228219, 0.0009730880074203015, 0.0009592640027403832, 0.00043398399651050566, 0.0004241599999368191, 0.0010267520025372507, 0.0004668480008840561, 0.0011796479858458041, 0.0005439360029995442, 0.0006912639960646629]
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.186
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.777
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.871
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.697
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.728
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.748
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.578
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.713
2024-10-17 05:03:04,097 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.378
2024-10-17 05:03:04,098 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.613
2024-10-17 05:03:04,098 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.371
2024-10-17 05:03:04,098 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:04,098 - optimal_runtime.py[116] - INFO: avg ratio: 1.6935740368920533
2024-10-17 05:03:04,098 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022594555161257524
2024-10-17 05:03:04,099 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00122871 0.00092461 0.00097309 0.00095926 0.00043398 0.00042416
 0.00102675 0.00046685 0.00117965 0.00056161 0.00071241]
2024-10-17 05:03:04,101 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:04,257 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:04,258 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0]
2024-10-17 05:03:04,272 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.2) from file
2024-10-17 05:03:04,289 - pure_runtime.py[42] - DEBUG: load 10th block (block-10) (sparsity 0.0) from file
2024-10-17 05:03:04,289 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 651666080.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0],
  'esti_latency': 0.0021741181318040387,
  'esti_test_accuracy': 0.767466684182485,
  'is_relaxed': False,
  'model_size': 34489165.0,
  'update_swap_mem_cost': 27664020.0,
  'update_swap_time_cost': 0.030593395233154297}
2024-10-17 05:03:04,341 - gen_series_legodnn_models.py[28] - INFO: target model size: 33.445MB
2024-10-17 05:03:04,341 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 35069509.666666664B (33.445MB), try to adapt blocks
2024-10-17 05:03:04,344 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:04,376 - optimal_runtime.py[77] - INFO: infer time of current model: 0.017350656509399414
2024-10-17 05:03:04,376 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0009725119993090631, 0.0008042879961431027, 0.0009424000009894373, 0.0009393279999494553, 0.0004273279942572117, 0.00039683199673891064, 0.0009307839982211591, 0.0004120319932699203, 0.0010764480009675026, 0.0004894400015473365, 0.0006992320008575917]
2024-10-17 05:03:04,376 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.435
2024-10-17 05:03:04,376 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 2.043
2024-10-17 05:03:04,376 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.932
2024-10-17 05:03:04,376 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.733
2024-10-17 05:03:04,376 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.755
2024-10-17 05:03:04,377 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.869
2024-10-17 05:03:04,377 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.741
2024-10-17 05:03:04,377 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.878
2024-10-17 05:03:04,377 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.511
2024-10-17 05:03:04,377 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.793
2024-10-17 05:03:04,377 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.397
2024-10-17 05:03:04,377 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([2, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:04,377 - optimal_runtime.py[116] - INFO: avg ratio: 1.814303844732352
2024-10-17 05:03:04,377 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021091038365669153
2024-10-17 05:03:04,378 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00101583 0.00080429 0.0009424  0.00093933 0.00042733 0.00039683
 0.00093078 0.00042597 0.00107645 0.00050534 0.00069923]
2024-10-17 05:03:04,380 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:04,527 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:04,528 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0]
2024-10-17 05:03:04,543 - pure_runtime.py[42] - DEBUG: load 7th block (block-7) (sparsity 0.0) from file
2024-10-17 05:03:04,544 - gen_series_legodnn_models.py[33] - INFO: update info: 
{ 'FLOPs': 666809248.0,
  'blocks_sparsity': [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0],
  'esti_latency': 0.002049888563232604,
  'esti_test_accuracy': 0.7677333553632101,
  'is_relaxed': False,
  'model_size': 34966989.0,
  'update_swap_mem_cost': 3276412.0,
  'update_swap_time_cost': 0.014832019805908203}
2024-10-17 05:03:04,596 - gen_series_legodnn_models.py[28] - INFO: target model size: 33.793MB
2024-10-17 05:03:04,596 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 35434652.222222224B (33.793MB), try to adapt blocks
2024-10-17 05:03:04,599 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:04,634 - optimal_runtime.py[77] - INFO: infer time of current model: 0.018358528137207033
2024-10-17 05:03:04,634 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0010413759909570217, 0.0008755839988589286, 0.0009414719939231873, 0.0009075520038604736, 0.0004206080101430416, 0.00042982400208711627, 0.0010092159807682038, 0.0004612159952521324, 0.0011168959960341453, 0.0005151359960436821, 0.000738783996552229]
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.340
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.877
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.934
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.794
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.783
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.725
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.605
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.734
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.456
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.703
2024-10-17 05:03:04,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.322
2024-10-17 05:03:04,635 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:04,635 - optimal_runtime.py[116] - INFO: avg ratio: 1.724160814591407
2024-10-17 05:03:04,635 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002219372559241192
2024-10-17 05:03:04,636 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00108776 0.00087558 0.00094147 0.00090755 0.00042061 0.00042982
 0.00100922 0.00046122 0.0011169  0.00053187 0.00073878]
2024-10-17 05:03:04,638 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:04,786 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:04,788 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0]
2024-10-17 05:03:04,789 - gen_series_legodnn_models.py[28] - INFO: target model size: 34.141MB
2024-10-17 05:03:04,789 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 35799794.777777776B (34.141MB), try to adapt blocks
2024-10-17 05:03:04,794 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:04,833 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019392351150512695
2024-10-17 05:03:04,834 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001157375995069742, 0.000906432006508112, 0.0009647359922528267, 0.0009146559983491898, 0.0004275839999318123, 0.0004270080029964447, 0.0010438399985432626, 0.0004737920016050338, 0.0011383999958634377, 0.0005085759945213795, 0.000723295982927084]
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.206
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.813
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.888
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.780
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.754
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.737
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.552
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.688
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.428
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.725
2024-10-17 05:03:04,834 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.350
2024-10-17 05:03:04,834 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:04,835 - optimal_runtime.py[116] - INFO: avg ratio: 1.684651262296435
2024-10-17 05:03:04,835 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002271422747997668
2024-10-17 05:03:04,835 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00120892 0.00090643 0.00096474 0.00091466 0.00042758 0.00042701
 0.00104384 0.00047379 0.0011384  0.0005251  0.0007233 ]
2024-10-17 05:03:04,837 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:04,987 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:04,989 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0]
2024-10-17 05:03:04,990 - gen_series_legodnn_models.py[28] - INFO: target model size: 34.490MB
2024-10-17 05:03:04,990 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 36164937.333333336B (34.490MB), try to adapt blocks
2024-10-17 05:03:04,995 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:05,037 - optimal_runtime.py[77] - INFO: infer time of current model: 0.019723136901855468
2024-10-17 05:03:05,037 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001173920001834631, 0.0008829119987785817, 0.0009475839845836163, 0.0009960640072822573, 0.0004422079920768738, 0.00043763199821114537, 0.0010376320071518424, 0.00046902400627732275, 0.0011879359930753708, 0.0005178559981286525, 0.0007488639988005163]
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.189
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.861
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.922
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.634
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.696
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.694
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.561
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.705
2024-10-17 05:03:05,037 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.369
2024-10-17 05:03:05,038 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.694
2024-10-17 05:03:05,038 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.304
2024-10-17 05:03:05,038 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:05,038 - optimal_runtime.py[116] - INFO: avg ratio: 1.6643176941088396
2024-10-17 05:03:05,038 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0022991735371004638
2024-10-17 05:03:05,039 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.0012262  0.00088291 0.00094758 0.00099606 0.00044221 0.00043763
 0.00103763 0.00046902 0.00118794 0.00053468 0.00074886]
2024-10-17 05:03:05,041 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:05,189 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:05,191 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0]
2024-10-17 05:03:05,192 - gen_series_legodnn_models.py[28] - INFO: target model size: 34.838MB
2024-10-17 05:03:05,192 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 36530079.88888889B (34.838MB), try to adapt blocks
2024-10-17 05:03:05,197 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:05,236 - optimal_runtime.py[77] - INFO: infer time of current model: 0.020066400527954102
2024-10-17 05:03:05,237 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001187295999377966, 0.0009224640093743803, 0.0009674879983067513, 0.0010186879932880403, 0.0004594879895448684, 0.00042886399850249296, 0.0010366719998419285, 0.00048729600757360463, 0.0011700800061225892, 0.0005164160057902335, 0.0007471680082380772]
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.175
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.781
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.882
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.598
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.632
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.729
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.563
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.641
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.390
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.699
2024-10-17 05:03:05,237 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.307
2024-10-17 05:03:05,238 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:05,238 - optimal_runtime.py[116] - INFO: avg ratio: 1.629215307044826
2024-10-17 05:03:05,238 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.002348710562119599
2024-10-17 05:03:05,239 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00124018 0.00092246 0.00096749 0.00101869 0.00045949 0.00042886
 0.00103667 0.0004873  0.00117008 0.00053319 0.00074717]
2024-10-17 05:03:05,241 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:05,387 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:05,389 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0]
2024-10-17 05:03:05,390 - gen_series_legodnn_models.py[28] - INFO: target model size: 35.186MB
2024-10-17 05:03:05,390 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 36895222.44444445B (35.186MB), try to adapt blocks
2024-10-17 05:03:05,395 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:05,435 - optimal_runtime.py[77] - INFO: infer time of current model: 0.020349023818969726
2024-10-17 05:03:05,435 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.001258560009300709, 0.0009749759957194328, 0.001003007996827364, 0.0009738559946417809, 0.00046086400002241134, 0.00045651200786232945, 0.0010066560097038745, 0.00045475199446082113, 0.0011701760105788709, 0.0004958079941570759, 0.0007790080048143864]
2024-10-17 05:03:05,435 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.109
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.685
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.815
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.672
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.627
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.624
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.610
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.759
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.390
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.770
2024-10-17 05:03:05,436 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.254
2024-10-17 05:03:05,436 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 8, 9]),)
2024-10-17 05:03:05,436 - optimal_runtime.py[116] - INFO: avg ratio: 1.6420404479344337
2024-10-17 05:03:05,437 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0023303659812013975
2024-10-17 05:03:05,437 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00131461 0.00097498 0.00100301 0.00097386 0.00046086 0.00045651
 0.00100666 0.00045475 0.00117018 0.00051192 0.00077901]
2024-10-17 05:03:05,439 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:05,587 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:05,589 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0]
2024-10-17 05:03:05,590 - gen_series_legodnn_models.py[28] - INFO: target model size: 35.534MB
2024-10-17 05:03:05,590 - optimal_runtime.py[268] - INFO: cur max inference time: 100.000000s, cur available max memory: 37260365.0B (35.534MB), try to adapt blocks
2024-10-17 05:03:05,595 - optimal_runtime.py[243] - INFO: no blocks infer time info, profile it through an inference
2024-10-17 05:03:05,633 - optimal_runtime.py[77] - INFO: infer time of current model: 0.01927382469177246
2024-10-17 05:03:05,634 - optimal_runtime.py[85] - INFO: infer time of current blocks: [0.0012168320044875142, 0.0009010239988565445, 0.0009770879931747913, 0.0009219199940562249, 0.0004073600023984909, 0.0004111680015921593, 0.0010166719928383827, 0.00047068799659609787, 0.0011663999818265437, 0.0005180159993469715, 0.000740320011973381]
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.147
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.824
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.864
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.766
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.841
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.804
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.594
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.000 = 1.699
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.002 / 0.001 = 1.394
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.694
2024-10-17 05:03:05,634 - optimal_runtime.py[106] - INFO: ratio: cal infer time / real infer time of biggest block of cur sparsity: 0.001 / 0.001 = 1.319
2024-10-17 05:03:05,635 - optimal_runtime.py[114] - INFO: legal avg ratio indexes: (array([1, 3, 4, 5, 6, 7, 9]),)
2024-10-17 05:03:05,635 - optimal_runtime.py[116] - INFO: avg ratio: 1.745835801173636
2024-10-17 05:03:05,635 - optimal_runtime.py[119] - INFO: cur cal infer time of original model: 0.0021918184957890724
2024-10-17 05:03:05,636 - optimal_runtime.py[254] - INFO: cur original blocks pred infer time: [0.00127103 0.00090102 0.00097709 0.00092192 0.00040736 0.00041117
 0.00101667 0.00047069 0.0011664  0.00053485 0.00074032]
2024-10-17 05:03:05,638 - optimal_runtime.py[226] - INFO: solving...
2024-10-17 05:03:05,786 - optimal_runtime.py[228] - INFO: solving finished
2024-10-17 05:03:05,788 - pure_runtime.py[26] - INFO: load blocks with sparsity [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0]
